{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SM_rudraksh.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2bb8d6ae1ecf488c99f4fe0365b9e2b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ce9609155b154c8084a7236b7c02de5d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_78bd75164f7a4b1b80bbb996475d9401","IPY_MODEL_687658b8d9324cd0a0cb2d29beb1e826"]}},"ce9609155b154c8084a7236b7c02de5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"78bd75164f7a4b1b80bbb996475d9401":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ed7a536c74dc4b978286fbc201c663e8","_dom_classes":[],"description":" 71%","_model_name":"FloatProgressModel","bar_style":"danger","max":60000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42635,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1edaa744b2124ccf88bbc52281d84371"}},"687658b8d9324cd0a0cb2d29beb1e826":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_31cf920ffbe841d8bb5b5c73867f2ecb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 42635/60000 [6:11:15&lt;2:22:48,  2.03it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f872ff15f5774585883902c52f7d7a11"}},"ed7a536c74dc4b978286fbc201c663e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1edaa744b2124ccf88bbc52281d84371":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"31cf920ffbe841d8bb5b5c73867f2ecb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f872ff15f5774585883902c52f7d7a11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LqiEYIQuMxMo"},"source":["# Cue Reward Association \n","\n","In this simple meta-learning task, one of four input cues is arbitrarily chosen as a *target cue*. The agent is repeatedly shown two random cues in succession, and then a *response cue* during which the agent must respond with a 1 if the target was part of the pair, or 0 otherwise. \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2EP3PdRlM2v9"},"source":["**Google Drive Set-up:**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GH-TD0QVWvsY","outputId":"f6f48192-e2c5-4801-bb6f-42fc59c331eb","executionInfo":{"status":"ok","timestamp":1591604762214,"user_tz":-330,"elapsed":217067,"user":{"displayName":"Rudraksh Kapil","photoUrl":"","userId":"04893937089984046700"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# project folder, e.g. 'project submissions/A09_A10_A54'\n","# $$ for us - 'DSc Term Project/'\n","FOLDERNAME = 'DSc Term Project/Task 1'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# test - this notebooks name should show up:\n","# is oserror - restart runtime\n","%cd /content/drive/My\\ Drive/$FOLDERNAME\n","%ls "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/DSc Term Project/Task 1\n","'Cue Reward Association.ipynb'   NP.ipynb   \u001b[0m\u001b[01;34msaved\u001b[0m/     SM_rudraksh.ipynb\n"," NM.ipynb                        RM.ipynb   SM.ipynb   \u001b[01;34mutils\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QW-zioHpNhqF"},"source":["**Import Statements:**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lYMYbf6BNuJo","colab":{}},"source":["import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch import optim\n","from torch.optim import lr_scheduler\n","\n","import numpy as np\n","import time\n","import pickle\n","import random\n","from tqdm.autonotebook import tqdm\n","#from graphics import *\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (20.0, 16.0) # set default size of plots\n","\n","\n","\n","np.set_printoptions(precision=4)\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZXE1D9PkN-E_"},"source":["**GPU Set-up:**\n","\n","You have an option to use GPU by setting the flag to `True` below.\n","\n","The global variables `dtype` and `device` will control the data types throughout this notebook. \n","\n","You need to manually switch to a GPU device. You can do this by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. Note that you have to rerun the cells from the top since the kernel gets restarted upon switching runtimes."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CH6wFaQaOB__","outputId":"9483b155-b892-4a9b-f88e-3e329528715f","executionInfo":{"status":"ok","timestamp":1591604771975,"user_tz":-330,"elapsed":9725,"user":{"displayName":"Rudraksh Kapil","photoUrl":"","userId":"04893937089984046700"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# flag\n","USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    dtype = torch.float32\n","else:\n","    device = torch.device('cpu')\n","    dtype = torch.float32\n","\n","print('Using Device:', device)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using Device: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1hQAbcUnZZcV"},"source":["### Training\n","\n","The cells in this section define utility functions required by the `train()` function, which is given in the last cell."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gH89ae8cb5mx"},"source":["**Creating Networks:**\n","\n","We will use 4 different types of recurrent neural networks (RNNs) to show the effect of modulation and plasticity on the accumulated reward in this task. These are defined in `utils/networks.py`. In our experiments we will analyse the following RNNs:\n","1. Non-Plastic, Non-Modulated  `NP`\n","2. Plastic, Non-Modulated `NM`\n","3. Plastic, Simple Modulation `SM`\n","4. Plastic, Retro Modulation `RM`\n","\n","We define a function `get_network()` that takes a dictionary `params` (defined later) holding the parameters required for the networks as well as a variable `type` dictating which of the RNNs is to be returned. It also migrates the model to the GPU, if available.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LHo_MuBZb7E9","colab":{}},"source":["from utils.networks import *\n","\n","\n","def get_network():\n","    # determine which RNN to return\n","    net_type = params['net_type']\n","    if net_type == 'NP':\n","        RNN = NonPlastic_NonModulated_RNN(params)\n","    elif net_type == 'NM':\n","        RNN = Plastic_NonModulated_RNN(params)\n","    elif net_type == 'SM':\n","        RNN = Plastic_SimpleModulated_RNN(params)\n","    else:\n","        RNN = Plastic_RetroactiveModulated_RNN(params)\n","\n","    # move to GPU and return, if possible\n","    RNN.to(device=device, dtype=dtype)\n","    return RNN\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zQ51PjWxVr93"},"source":["**Visualizations:**\n","\n","We define a function `start_graphics()` to add visualizations so the activities performed in this task are better understood. We have used `utils/graphics.py`, a Python library developed by *Zelle*.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NQDrQLkc2Dml","colab":{}},"source":["#from utils.graphics import *\n","\n","def start_graphics():\n","    if params['visualise']:\n","        # set title and background\n","        win = GraphWin(\"Cue Visualization\", 750, 700)\n","        win.setBackground('light blue')\n","\n","        # lists to hold the cues\n","        GraphicsCues=[]\n","        MainCues=[]\n","\n","        # create the cues and store in GraphicsCues[]\n","        for j in range(NUM_CUE_BITS +1):\n","            aRectangle = Rectangle(Point(50+j*25,50), Point(75+j*25,75))\n","            aRectangle.setFill('black')\n","            aRectangle.setOutline('yellow')\n","            aRectangle.draw(win)\n","            GraphicsCues.append(aRectangle)\n","\n","        # text set up\n","        message=Text(Point(300,15),'Present Cue')\n","        message.setTextColor('red')\n","        message.setStyle('bold')\n","        message.setSize(15)\n","        message.draw(win)\n","        m=Text(Point(650,250),'Network Response')\n","        m.draw(win)\n","\n","        # \n","        Cir = Circle(Point(650,350),50)\n","        Cir.setFill('yellow')\n","        Cir.draw(win)\n","        for j in range(5):\n","            Arr=[]\n","            for k in range(NUM_CUE_BITS+1):\n","                aRectangle = Rectangle(Point(100+j*100,100+k*25), Point(100+j*100+25,100+k*25+25))\n","                aRectangle.setFill('black')\n","                aRectangle.setOutline('yellow')\n","                aRectangle.draw(win)\n","                Arr.append(aRectangle)\n","            MainCues.append(Arr)\n","            if(j!=4):\n","                msg=Text(Point(100-5+j*100,650),'  Cue no '+str(j+1))\n","            else:\n","                msg=Text(Point(100-5+j*100,650),'Target Cue')\n","            msg.setStyle('bold')\n","            msg.setTextColor('green')\n","            msg.draw(win)\n","\n","        # return\n","        return GraphicsCues, MainCues, win , Cir\n","    else:\n","        return None, None, None, None\n","\n","#start_graphics()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RaHMa6VeZzis"},"source":["**Generating Cues:**\n","\n","We define a function `generate_cues()` that generates the `NUM_CUE_BITS`-bit cues required for the experiments. Of the cues generated randomly, one particular cue in each episode is chosen as the target cue. These cues, along with some additional inputs, will be fed into the RNN at each time step.\n","\n","This function returns `Cues`, a list of all the random cues generated and `TargetCue`, a list which gives the target cue, for each batch."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F6TMIQw95OXl","colab":{}},"source":["## $$ VECTORISE\n","def generate_cues():\n","    Cues=[]\n","    TargetCue=[]\n","    for nb in range(params['batch_size']):\n","\n","      Prescue=[]\n","      Prescue.append([0]*(NUM_CUE_BITS+1))   #The first cue or ( BLANK CUE ) \n","\n","      RndNumGen = random.sample(range(1,2**NUM_CUE_BITS ),NUM_CUES)  # Generating a rndom binary number from [1,2^NO_BITS_IN_CUE]\n","      for j in range(NUM_CUES):\n","          Presnum=RndNumGen[j]\n","          \n","          temp=([int(i) for i in bin(Presnum)[2:]]) \n","          # Converting the number to binary \n","          while(len(temp) != NUM_CUE_BITS):\n","              temp.insert(0,0)  # Adding leading zeros to make it equal to 20 bits\n","              \n","          temp.append(0)    # Final bit to determine go cue\n","          \n","          # GRAPHICS\n","          if params['visualise']:\n","              for bit in range(len(temp)):\n","                 if(temp[bit]==0 and nb==0):\n","                     MainCues[j][bit].setFill('white')\n","                 elif(temp[bit]==1 and nb==0):\n","                     MainCues[j][bit].setFill('black')\n","\n","          Prescue.append(temp)\n","\n","\n","      # Generating the GOCUE\n","      GoCue=[0]*NUM_CUE_BITS   \n","      GoCue.append(1)\n","      Prescue.append(GoCue)\n","      Cues.append(Prescue)\n","\n","      # Visualizing Target Cue\n","      tgtcue=random.sample(range(1,NUM_CUES+1),1)[0]\n","      TargetCue.append(tgtcue)\n","\n","\n","      # GRAPHICS\n","      if params['visualise']:\n","          for bit in range(NUM_CUE_BITS+1):\n","             if(Cues[0][tgtcue][bit]==0 and nb==0):\n","                 MainCues[4][bit].setFill('white')\n","             elif(Cues[0][tgtcue][bit]==1 and nb==0):\n","                 MainCues[4][bit].setFill('black')\n","\n","      \n","    return np.asarray(Cues), np.asarray(TargetCue)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GYwJkDWYNt5q","colab":{}},"source":["## $$ VECTORISE\n","def f(x):\n","    t=list(np.binary_repr(x).zfill(NUM_CUE_BITS))\n","    t=np.array(t).astype('int')\n","    t[t==0] = -1\n","    t=np.append(t,0)\n","    return np.array(t).astype('int')\n","\n","# Add main cues for graphics\n","def vectorized_generate_cues(MainCues):\n","    Cues=np.zeros((params['batch_size'],NUM_CUES+2,NUM_CUE_BITS+1),dtype='int')\n","    TargetCue=np.zeros(params['batch_size'],dtype='int')\n","\n","    Cues[:,0,:] = np.zeros(NUM_CUE_BITS+1,dtype='int')\n","\n","    RndNumGen = np.random.choice( range(1,2**NUM_CUE_BITS),size=(params['batch_size'],NUM_CUES), replace=False)  # Generating a rndom binary number from [1,2^NO_BITS_IN_CUE]\n","\n","    for i in range(params['batch_size']):  \n","        for j in range(NUM_CUES):\n","            Cues[i,j+1,0:NUM_CUE_BITS+1]=f(RndNumGen[i,j])\n","              \n","\n","          \n","          # GRAPHICS\n","    if params['visualise']:\n","        for i in range(NUM_CUES):\n","            for bit in range(NUM_CUE_BITS+1):\n","                if(Cues[0][i+1][bit]== -1 or Cues[0][i+1][bit] == 0):\n","                    MainCues[i][bit].setFill('white')\n","                else:\n","                    MainCues[i][bit].setFill('black')\n","\n","\n","\n","      # Generating the GOCUE\n","    GoCue=[0]*NUM_CUE_BITS   \n","    GoCue.append(1)\n","    GoCue=np.array(GoCue,dtype='int')\n","    Cues[:,NUM_CUES+1,0:NUM_CUE_BITS+1]=GoCue\n","\n","    # Visualizing Target Cue\n","    TargetCue=np.random.choice(range(1,NUM_CUES+1),size=params['batch_size'],replace=True)\n","    # GRAPHICS\n","    if params['visualise']:\n","        for bit in range(NUM_CUE_BITS+1):\n","            if(Cues[0][TargetCue[0]][bit]==0 or Cues[0][TargetCue[0]][bit] ==-1 ):\n","                MainCues[4][bit].setFill('white')\n","            elif(Cues[0][TargetCue[0]][bit]==1 ):\n","                MainCues[4][bit].setFill('black')\n","\n","    if (params['verbose']):  \n","        print('Cue')\n","        print(list(Cues))\n","        print('Target')\n","        print(list(TargetCue))\n","    return Cues, TargetCue\n","# MC=[]\n","# params['batch_size']=2;\n","# NUM_CUE_BITS=5\n","# C,T=vectorized_generate_cues(params,MC)\n","# print(C)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Tri0VK8ees6_"},"source":["**Calculating Rewards:**\n","\n","Here we define a function `calculate_rewards()` that takes in the following parameters and updates the `rewards` list accordingly.\n","\n","1.    The list `reward` stores the reward for each batch.\n","2.    The array `numactionschosen` is a list which has the network's response for each batch. (Either 0 or 1).\n","3.  `CurrentTrialStep` holds the step number of a batch in the trial being executed. Ex : If it is 2 for a specific batch that means presently the 2nd step of the trial is being run.\n","4. `PresentTrailHasTarget` is a list which stores '0' if the target is present in a particular trial or '1' if the target isn't present.\n","5. `Cir` is a Python Graphics Object used to visualize network response.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e3lIo_gydkkW","colab":{}},"source":["## $$ VECTORISE\n","# note: if batch size is 16, a batch contains 16 'agents'\n","def calculate_rewards(numactionschosen , reward ,CurrentTrialStep , PresentTrialHasTarget, NoStepsInTrial,Cir):\n","\n","    '''\n","    # initialise rewards as 0 and subtract penalty from agents where choice 1 was made\n","    rewards = np.zeros((params['batch_size'],))\n","    chosen_ind = np.where(numactionschosen == 1)\n","    rewards[chosen_ind] -= params['penalty']\n","\n","    # get indices of the trials where we reached the last step\n","    '''\n","\n","\n","    # Calculate Reward for each batch \n","    for nb in range(params['batch_size']):\n","        numactionchosen = numactionschosen[nb]       #Gives action chosen\n","        reward[nb] = 0.0\n","\n","        # Here I calculate rewards based on output\n","        if numactionchosen == 1:\n","            reward[nb]-=params['penalty']\n","\n","        # \n","        \n","        # Rewarding when I get to the last step of the trial\n","        if CurrentTrialStep[nb]==NoStepsInTrial[nb]-1 :\n","\n","            print(f'-----{nb+1}---------\\nAction chosen: {numactionchosen}')\n","            ## GRAPHICS\n","            if PresentTrialHasTarget[nb]==1 and numactionchosen==1 :\n","                #if(nb==0):\n","                #    Cir.setFill('green')\n","                    # time.sleep(1)\n","                reward[nb]+=params['reward']\n","                print(\"CORRECT\")\n","            elif PresentTrialHasTarget[nb] ==0 and numactionchosen==0:\n","                #if(nb==0):\n","                #    Cir.setFill('green')\n","                    #time.sleep(1)\n","                reward[nb]+=params['reward']\n","                print(\"CORRECT\")\n","            else:\n","                #if(nb==0):\n","                #    Cir.setFill('red')\n","                    #time.sleep(1)\n","                reward[nb]-=params['reward']\n","                print(\"wrong!\")\n","\n","            if random.random() < params ['inversion_factor']:  #I've replicated this step as in the code IDK what\n","                reward=-reward[nb] \n","            CurrentTrialStep[nb]=0\n","            #time.sleep(5)\n","        else:\n","            CurrentTrialStep[nb]+=1\n","        \n","\n","    return reward"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s0DWTcDVozZe","colab":{}},"source":["### This is the vectorized calculate rewards function\n","\n","## $$ VECTORISE\n","# note: if batch size is 16, a batch contains 16 'agents'\n","# Add 'Cir' to list of params\n","def vectorised_calculate_rewards(numactionschosen , reward ,CurrentTrialStep , PresentTrialHasTarget, NoStepsInTrial,Cir):\n","\n","    '''\n","    # initialise rewards as 0 and subtract penalty from agents where choice 1 was made\n","    rewards = np.zeros((params['batch_size'],))\n","    chosen_ind = np.where(numactionschosen == 1)\n","    rewards[chosen_ind] -= params['penalty']\n","\n","    # get indices of the trials where we reached the last step\n","    '''\n","    # Converting arrays into np arrays for vectorization\n","\n","    # Calculate Reward for each batch \n","\n","    chosen_ind = np.where(numactionschosen == 1)\n","    reward[chosen_ind] -= params['penalty']\n","    \n","\n","    Batches_In_Last_Step=np.where(CurrentTrialStep == NoStepsInTrial -1)\n","    Batches_Not_In_Last_Step=np.where(CurrentTrialStep != NoStepsInTrial -1)\n","\n","    Batches_Which_Predicted_Correct_Response=np.where(np.bitwise_xor(PresentTrialHasTarget,numactionschosen) ==0 )\n","    Batches_Which_Predicted_Wrong_Response=np.where(np.bitwise_xor(PresentTrialHasTarget,numactionschosen) ==1 )\n","    Batches_Which_Predicted_Correct_Response = np.intersect1d(Batches_Which_Predicted_Correct_Response,Batches_In_Last_Step)\n","\n","    Batches_Which_Predicted_Wrong_Response = np.intersect1d(Batches_Which_Predicted_Wrong_Response,Batches_In_Last_Step)\n","\n","    reward[Batches_Which_Predicted_Correct_Response]+=params['reward']\n","    reward[Batches_Which_Predicted_Wrong_Response]-=params['reward']\n","\n","    some_random_indices=np.random.rand(params['batch_size'])\n","    some_random_indices=np.where(some_random_indices<params['inversion_factor'])\n","    reward[some_random_indices]*=-1\n","\n","    CurrentTrialStep[Batches_In_Last_Step]=0\n","    CurrentTrialStep[Batches_Not_In_Last_Step]+=1\n","\n","    if params['verbose']:\n","        print('Num actions chosen are : ', numactionschosen)\n","        print('Batches in last step: ', Batches_In_Last_Step)\n","        print('Right:', Batches_Which_Predicted_Correct_Response)\n","        print('Wrong: ', Batches_Which_Predicted_Wrong_Response)\n","\n","    \n","    # Running Graphics Part\n","    if params['visualise']:\n","        if CurrentTrialStep[0]==NoStepsInTrial[0]-1 :\n","\n","            ## GRAPHICS\n","            if PresentTrialHasTarget[0]==1 and numactionschosen[0]==1 :\n","                Cir.setFill('green')\n","    #         time.sleep(1)\n","            elif PresentTrialHasTarget[0]==0 and numactionschosen[0]==0 :\n","                Cir.setFill('green')\n","    #         time.sleep(1)\n","            else:\n","                Cir.setFill('red')\n","    #          time.sleep(1)\n","\n","    return reward\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SJQDFK-jDDwb"},"source":["**Other Utility Functions:**\n","\n","Here we define some additional helper functions that help us keep our code oragnised in this notebook, and reduce the LOC in the `train()` function\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zK4yrkmJDES5","colab":{}},"source":["# returns a dictionary of blank lists corresponding to the statistics\n","# over all episodes we wish to track in our experiments\n","def get_blank_statistics_dict():\n","    all_stats = {\n","        'losses' : [],\n","        'grad_norms' : [],\n","        'losses_objective' : [],\n","        'total_rewards' : [],\n","        'losses_v' : [],\n","        'mean_reward' : [],\n","        'total_rewards_batchwise' : []\n","\n","    }\n","    return all_stats\n","\n","\n","# called at the start of each episode to individually reset the values of \n","# hebb, prev, E, and plastic_weights (check architechture for more details)\n","# note: none of these require grads since we manually update them\n","def initial_zero_BH(): # used to reset prev hidden state (BxH)\n","    return Variable(torch.zeros(params['batch_size'], params['hidden_size']), \n","                    requires_grad=False).to(device, dtype)\n","\n","def initial_zero_BHH(): # used to reset E (eligibility trace), plastic_weights, and hebb\n","    return Variable(torch.zeros(params['batch_size'], params['hidden_size'], params['hidden_size']), \n","                    requires_grad=False).to(device, dtype) # (BxHxH)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_nk-br50v2TE"},"source":["**Training Function:**\n","\n","This is the main function we've used to train the network and analyze it's response over all episodes and for all batches. \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"q0P6SMCxjja7"},"source":["### Normal train"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dEdO4Rp_v194","colab":{}},"source":["def train():\n","    # print current parameters as a check - global variables\n","    print(params)\n","\n","    # set random seeds for training, so that results are comparable\n","    np.random.seed(params['rng_seed'])\n","    random.seed(params['rng_seed'])\n","    torch.manual_seed(params['rng_seed'])\n","\n","    # create the network required determined by params['net_type']\n","    net = get_network()\n","    #net = NonPlasticRNN(params)\n","\n","    # initialise optimiser - we use Adam with L2 reg\n","    optimizer = torch.optim.Adam(net.parameters(),\n","                                 lr=params['learning_rate'], \n","                                 weight_decay=params['lambd'])\n","    \n","    # create dictionary to keep track of stats over all episodes\n","    all_stats = get_blank_statistics_dict() \n","    lossbetweensaves = 0\n","    nowtime = time.time()\n","\n","\n","    # GRAPHICS\n","    # GraphicsCues, MainCues, win, Cir = Start_Graphics()\n","\n","\n","    # loop over every episode:\n","    for numiter in tqdm(range(params['num_episodes'])): \n","        # $$ \n","        # An episode consists of several trials. In each trial, half of the total no of cues are input to the network.\n","        # The network , at the last step of each trial outputs 0 or 1.\n","\n","        # We generate 4 random unique Cues with 20 bits and choose one to be TargetCue\n","        Cues, TargetCue = generate_cues()\n","        \n","        CurrentTrialStep = np.zeros(params['batch_size'], dtype='int')      # used to keep track of which step each trial is in\n","        NoStepsInTrial = np.zeros(params['batch_size'], dtype='int')        # length of each trial\n","        TrialMoves = [None]*params['batch_size']                            # The various moves to be performed in a trial \n","        PresentTrialHasTarget = np.zeros(params['batch_size'], dtype='int') # 0 if no target , 1 if target present in a trial\n","        numactionschosen = np.zeros(params['batch_size'], dtype='int')      # used to keep track of the choice in each trial\n","\n","        # we zero out these values before each episode according to their lifetime\n","        # cache holds the extra inputs and outputs needed according to different network types \n","        # initially cache is a list, then we convert it to a tuple\n","        prev = initial_zero_BH()\n","        cache = [prev]\n","        if params['net_type'] != 'NP':  # hebb is only for plastic\n","            hebb = initial_zero_BHH()\n","            cache.append(hebb)\n","        if params['net_type'] == 'RM':  # these are only for retroactive modulated\n","            E_t = initial_zero_BHH()              \n","            plastic_weights = initial_zero_BHH()\n","            cache.append(E_t)\n","            cache.append(plastic_weights)\n","        cache = tuple(cache)\n","\n","        # zero out backprop gradients as well (since pytroch accumulates them)\n","        optimizer.zero_grad()\n","        \n","        # additional variables\n","        loss = 0                                    # loss is accumulated over the episode\n","        lossv = 0                                   # needed for A2C\n","        numactionchosen = 0                         # 0 -> not present, 1 -> present\n","        reward = np.zeros(params['batch_size'])     #\n","        sumreward = np.zeros(params['batch_size'])  #\n","        rewards = []                                #\n","        vs = []                                     # \n","        logprobs = []                               # \n","        dist = 0                                    #\n","\n","\n","        # loop over the trials in each episode\n","        for numstep in range(params['episode_length']): \n","\n","            ## $$ VECTORISE\n","            # Pick the total no of moves for the trial\n","\n","\n","\n","            for nb in range(params['batch_size']):\n","                if (CurrentTrialStep[nb]==0):\n","\n","                    extrablankspaces = random.randint(1,NUM_CUES//2) # Adding some random no of extra blank spaces\n","                    NoStepsInTrial[nb] = NUM_CUES + 1 + extrablankspaces + 1\n","                    \n","                    # Half no of cues and half mandatory 0's  and 1 for Go cue and 1 extra zerotime in the beginning\n","                    shufflearray=[]\n","                    # Taking a sample of half the number of cues\n","                    # And appending it to an array\n","\n","                    tmparray=random.sample(range(1,NUM_CUES+1),NUM_CUES//2)\n","                    # print('Here at start of a trial')\n","                    shufflearray.extend(tmparray)\n","\n","                    #Interleaving blank spaces\n","\n","                    barray=[0]*(NUM_CUES//2)\n","                    random.shuffle(shufflearray)\n","                    newshufflearray = [None]*NUM_CUES\n","                    newshufflearray[::2] = shufflearray\n","                    newshufflearray[1::2] = barray\n","\n","                    newshufflearray.insert(0,0) # Adding a blank space in the beginning \n","\n","                    # Adding some more extra blank spaces in a trial at random steps\n","                    for j in range(extrablankspaces):\n","                        newshufflearray.insert(random.randint(0,len(newshufflearray)),0)\n","            \n","                    # Each trial ends with a go cue\n","\n","                    newshufflearray.append(NUM_CUES+1) # Appending the Go Cue labelled NUM_CUES +1 \n","                    TrialMoves[nb]=newshufflearray # Shuffling the array and assigning it a move\n","\n","\n","                    if TargetCue[nb] in TrialMoves[nb]:\n","                        PresentTrialHasTarget[nb]=1\n","\n","\n","            inputs = np.zeros((params['batch_size'], TOTAL_NUM_INPUTS), dtype='float32')\n","\n","            ## VECTORISE\n","            for nb in range(params['batch_size']):  \n","                # print('Input to network')\n","                # print(inputs[nb])\n","                # print(TrialMoves[nb])\n","                # print(CurrentTrialStep[nb])\n","                # print(TrialMoves[nb][CurrentTrialStep[nb]]) \n","                print('Cue being input')\n","                print(Cues[nb][TrialMoves[nb][CurrentTrialStep[nb]]]) \n","                # print('Passed this input printing phase')\n","                # I present the Cue bits as inputs to the network\n","\n","                # GRAPHICS\n","                #if nb==0:\n","                #    for bit in range(NUM_CUE_BITS + 1):\n","                #        if(Cues[nb][TrialMoves[nb][CurrentTrialStep[nb]]][bit] == 0):\n","                #            GraphicsCues[bit].setFill('white')\n","                #        else:\n","                #            GraphicsCues[bit].setFill('black')\n","                #    #time.sleep(1)\n","                 \n","\n","                # Previous chosen action\n","                inputs[nb, 0: NUM_CUE_BITS+1] = Cues[nb][TrialMoves[nb][CurrentTrialStep[nb]]]\n","                inputs[nb, NUM_CUE_BITS +1] = 1.0 # Bias neuron\n","                inputs[nb, NUM_CUE_BITS +2] = numstep / params['episode_length']\n","                inputs[nb, NUM_CUE_BITS + 3] = 1.0 * reward[nb]\n","                inputs[nb, NUM_CUE_BITS + 1 + ADDITIONAL_INPUTS + numactionschosen[nb]] = 1\n","                print('Input')\n","                print(inputs[nb])\n","                #time.sleep(3)\n","\n","\n","            # create a tensor from the inputs and pass it into the network \n","            print(\"Sending to network...\")\n","            inputsC = torch.from_numpy(inputs).to(device, dtype)\n","            inputs_tensor = Variable(inputsC, requires_grad=False)\n","            y, v, cache = net(inputs_tensor, cache)                        # y  should output raw scores, not probas\n","            #y, v, prev = net(inputs_tensor, prev) # checking with their network\n","\n","            # we apply softmax to get probabilities\n","            print(\"Y: \", y)                       \n","            y = torch.softmax(y, dim=1)      \n","            print(\"Softmax(Y): \", y)\n","\n","            # create distribution according to y and randomly choose an action accordingly\n","            distrib = torch.distributions.Categorical(y)\n","            actionschosen = distrib.sample()\n","            print('Actions chosen: ', actionschosen)\n","            logprobs.append(distrib.log_prob(actionschosen))     # store log of proba\n","\n","\n","            numactionschosen = actionschosen.data.cpu().numpy()  # We want to break gradients\n","            reward = np.zeros(params['batch_size'], dtype='float32')\n","\n","            # Function to calculate Rewards\n","            Cir = None # $$\n","            reward = calculate_rewards(numactionschosen,reward,CurrentTrialStep,PresentTrialHasTarget,NoStepsInTrial)\n","            # print('Reward: ', reward)\n","            rewards.append(reward)\n","            vs.append(v) \n","            sumreward += reward\n","\n","            # This is an \"entropy penalty\", implemented by the sum-of-squares of the probabilities \n","            # The result is to encourage diversity in chosen actions.\n","            loss += ( params['entropy_reward_coeff'] * y.pow(2).sum() / params['batch_size'] )\n","\n","\n","\n","\n","        # Episode is done, now we do the actual computations of rewards and losses for the A2C algorithm\n","        R = torch.zeros(params['batch_size']).to(device)\n","        gammaR = params['reward_discount_factor']    #Discounting factor for rewards\n","        for numstepb in reversed(range(params['episode_length'])) :         # Every step in an episode has a reward\n","            R = gammaR * R + Variable(torch.from_numpy(rewards[numstepb]), requires_grad=False)\n","            ctrR = R - vs[numstepb][0]\n","            lossv += ctrR.pow(2).sum() / params['batch_size']\n","            loss -= (logprobs[numstepb] * ctrR.detach()).sum() / params['batch_size']\n","\n","\n","\n","        loss += params['lossv_coeff'] * lossv      #Coefficient for value prediction loss\n","        loss /= params['episode_length']\n"," \n","        # // if PRINTTRACE: \n","        # //        print(\"lossv: \", float(lossv))\n","        # //        print (\"Total reward for this episode (all):\", sumreward, \"Dist:\", dist)\n","\n","\n","        # perform backprop and gradient clipping\n","        loss.backward()\n","        all_stats['grad_norms'].append(torch.nn.utils.clip_grad_norm_(net.parameters(), params['gradient_clip']))\n","        #if numiter > 100:  # Burn-in period for meanrewards\n","        optimizer.step()\n","\n","        # update stats\n","        lossnum = float(loss)\n","        lossbetweensaves += lossnum\n","        all_stats['losses_objective'].append(lossnum)\n","        all_stats['total_rewards'].append(sumreward.mean())\n","\n","\n","        if (numiter+1) % params['print_every'] == 0:\n","            print(f'\\nThis is episode {numiter}:')\n","            print(\"lossv: \", float(lossv))\n","            print (\"Total reward for this episode (all):\", sumreward, \"Dist:\", dist)\n","          \n","            print(numiter, \"====\")\n","            print(\"Mean loss: \", lossbetweensaves / params['print_every'])\n","            lossbetweensaves = 0\n","            print(\"Mean reward (across batch and last\", params['print_every'], \"eps.): \", np.sum( all_stats['total_rewards'][-params['print_every']:])/ params['print_every'])\n","            print(\"Mean reward (across batch): \", sumreward.mean())\n","            previoustime = nowtime\n","            nowtime = time.time()\n","            print(\"Time spent on last\", params['print_every'], \"iters: \", nowtime - previoustime)\n","\n","\n","\n","    return all_stats\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bxN9nlPrh-qr"},"source":["### vectorised train"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"23ImsN576dQP","colab":{}},"source":["def vectorized_train():\n","    # print current parameters as a check - global variables\n","    print(params)\n","\n","    # set random seeds for training, so that results are comparable\n","    np.random.seed(params['rng_seed'])\n","    random.seed(params['rng_seed'])\n","    torch.manual_seed(params['rng_seed'])\n","\n","    # create the network required determined by params['net_type']\n","    net = get_network()\n","\n","    # initialise optimiser - we use Adam with L2 reg\n","    optimizer = torch.optim.Adam(net.parameters(),\n","                                 lr=params['learning_rate'], \n","                                 weight_decay=params['lambd'])\n","    \n","    if params['saved_already'] == False:\n","      # create dictionary to keep track of stats over all episodes\n","      all_stats = get_blank_statistics_dict()\n","      epoch=0\n","    else:\n","      checkpoint_path = 'saved/Checkpoint' + params['net_type'] + '.pt'\n","      checkpoint = torch.load(checkpoint_path)\n","      print('Loading states 1')\n","      print(net.load_state_dict(checkpoint['state_dict']))\n","      print('Loading states 2')\n","      print(optimizer.load_state_dict(checkpoint['optimizer']))\n","      epoch = checkpoint['iters'] + 1\n","      all_stats=checkpoint['stats_dict'] \n","      print(f\"Loaded from '{checkpoint_path}' at episode {epoch}\")\n","    \n","    lossbetweensaves = 0\n","    nowtime = time.time()\n","    lossbetweensaves = 0\n","    nowtime = time.time()\n","\n","\n","    # GRAPHICS\n","    GraphicsCues, MainCues, win, Cir = start_graphics()\n","    \n","    \n","    nbtrials = [0]*params['batch_size']\n","    totalnbtrials = 0\n","    nbtrialswithcc = 0\n","\n","    # loop over every episode:\n","    for numiter in tqdm(range(params['num_episodes'])): \n","        # $$ \n","        # An episode consists of several trials. In each trial, half of the total no of cues are input to the network.\n","        # The network , at the last step of each trial outputs 0 or 1.\n","\n","        # We generate 4 random unique Cues with 20 bits and choose one to be TargetCue\n","        #Cues, TargetCue = vectorized_generate_cues(MainCues)\n","        #CurrentTrialStep = np.zeros(params['batch_size'], dtype='int')      # used to keep track of which step each trial is in\n","        #NoStepsInTrial = np.zeros(params['batch_size'], dtype='int')        # length of each trial\n","        #TrialMoves = [None]*params['batch_size']                            # The various moves to be performed in a trial \n","        #PresentTrialHasTarget = np.zeros(params['batch_size'], dtype='int') # 0 if no target , 1 if target present in a trial\n","        #numactionschosen = np.zeros(params['batch_size'], dtype='int')      # used to keep track of the choice in each trial\n","\n","        # Generate the cues. Make sure they're all different (important when using very small cues for debugging, e.g. cs=2, ni=2)\n","        cuedata=[]\n","        for nb in range(params['batch_size']):\n","            cuedata.append([])\n","            for ncue in range(NUM_CUES):\n","                assert len(cuedata[nb]) == ncue\n","                foundsame = 1\n","                cpt = 0\n","                while foundsame > 0 :\n","                    cpt += 1\n","                    if cpt > 10000:\n","                        # This should only occur with very weird parameters, e.g. cs=2, ni>4\n","                        raise ValueError(\"Could not generate a full list of different cues\")\n","                    foundsame = 0\n","                    candidate = np.random.randint(2, size=NUM_CUE_BITS) * 2 - 1\n","                    for backtrace in range(ncue):\n","                        if np.array_equal(cuedata[nb][backtrace], candidate):\n","                            foundsame = 1\n","\n","                cuedata[nb].append(candidate)\n","\n","        # we zero out these values before each episode according to their lifetime\n","        # cache holds the extra inputs and outputs needed according to different network types \n","        # initially cache is a list, then we convert it to a tuple\n","        prev = initial_zero_BH()\n","        cache = [prev]\n","        if params['net_type'] != 'NP':  # hebb is only for plastic\n","            hebb = initial_zero_BHH()\n","            cache.append(hebb)\n","        if params['net_type'] == 'RM':  # these are only for retroactive modulated\n","            E_t = initial_zero_BHH()              \n","            plastic_weights = initial_zero_BHH()\n","            cache.append(E_t)\n","            cache.append(plastic_weights)\n","        cache = tuple(cache)\n","\n","\n","        # zero out backprop gradients as well (since pytroch accumulates them)\n","        optimizer.zero_grad()\n","        \n","        \n","        # additional variables\n","        loss = 0                                    # loss is accumulated over the episode\n","        lossv = 0                                   # needed for A2C\n","        numactionchosen = 0                         # 0 -> not present, 1 -> present\n","        reward = np.zeros(params['batch_size'])     #\n","        sumreward = np.zeros(params['batch_size'])  #\n","        rewards = []                                #\n","        vs = []                                     # \n","        logprobs = []                               # \n","        dist = 0                                    #\n","        cues=[]\n","        for nb in range(params['batch_size']):\n","            cues.append([])\n","        numactionschosen = np.zeros(params['batch_size'], dtype='int32')\n","        \n","        nbtrials = np.zeros(params['batch_size'])\n","        nbrewardabletrials = np.zeros(params['batch_size'])\n","        thistrialhascorrectcue = np.zeros(params['batch_size'])\n","        triallength = np.zeros(params['batch_size'], dtype='int32')\n","        correctcue = np.random.randint(NUM_CUES, size=params['batch_size'])\n","\n","        trialstep = np.zeros(params['batch_size'], dtype='int32')\n","\n","        # loop over the trials in each episode\n","        for numstep in range(params['episode_length']): \n","\n","            ## $$ VECTORISE\n","            #ind_current_trial_step_zero = np.where(CurrentTrialStep == 0)\n","\n","            #extra_blank_spaces = np.random.randint(size=(params['batch_size'],), NUM_CUES//2)\n","\n","            #NoStepsInTrial[ind_current_trial_step_zero] = NUM_CUES + 1 + extra_blank_spaces[ind_current_trial_step_zero] + 1\n","\n","            # (nb x num_cues // 2) - choices for cue to input for each batch in this trial\n","            #choices = np.random.choice(np.arange(1, (NUM_CUES//2)+1), size=(params['batch_size'],NUM_CUES//2))\n","            \n","\n","            inputs = np.zeros((params['batch_size'], params['input_size']), dtype='float32')\n","            NBINPUTBITS=NUM_CUE_BITS+1\n","            for nb in range(params['batch_size']):\n","\n","                if trialstep[nb] == 0:\n","                    thistrialhascorrectcue[nb] = 0\n","                    # Trial length is randomly modulated for each trial; first time step always -1 (i.e. no input cue), last time step always response-cue (i.e. NBINPUTBITS-1).\n","                    #triallength = params['ni'] // 2  + 3 + np.random.randint(1 + params['ni'])  # 3 fixed-cue time steps (1st, last and next-to-last) + some random nb of no-cue time steps\n","                    triallength[nb] = NUM_CUES // 2  + 3 + np.random.randint(NUM_CUES)  # 3 fixed-cue time steps (1st, last and next-to-last) + some random nb of no-cue time steps\n","\n","\n","\n","                    # In any trial, we only show half the cues (randomly chosen), once each:\n","                    mycues = [x for x in range(NUM_CUES)]\n","                    random.shuffle(mycues); mycues = mycues[:len(mycues) // 2]\n","                    # The rest is filled with no-input time steps (i.e. cue = -1), but also with the 3 fixed-cue steps (1st, last, next-to-last)\n","                    for nc in range(triallength[nb] - 3  - len(mycues)):\n","                        mycues.append(-1)\n","                    random.shuffle(mycues)\n","                    mycues.insert(0, -1); mycues.append(NUM_CUES); mycues.append(-1)  # The first and last time step have no input (cue -1), the next-to-last has the response cue.\n","                    assert(len(mycues) == triallength[nb])\n","                    cues[nb] = mycues\n","\n","\n","                inputs[nb, :NBINPUTBITS] = 0\n","                if cues[nb][trialstep[nb]] > -1 and cues[nb][trialstep[nb]] < NUM_CUES:\n","                    #inputs[0, cues[trialstep]] = 1.0\n","                    inputs[nb, :NBINPUTBITS-1] = cuedata[nb][cues[nb][trialstep[nb]]][:]\n","                    if cues[nb][trialstep[nb]] == correctcue[nb]:\n","                        thistrialhascorrectcue[nb] = 1\n","                if cues[nb][trialstep[nb]] == NUM_CUES:\n","                    inputs[nb, NBINPUTBITS-1] = 1  # \"Go\" cue\n","\n","\n","                inputs[nb, NBINPUTBITS + 0] = 1.0 # Bias neuron, probably not necessary\n","                inputs[nb,NBINPUTBITS +  1] = numstep / params['episode_length']\n","                inputs[nb, NBINPUTBITS + 2] = 1.0 * reward[nb] # Reward from previous time step\n","                if numstep > 0:\n","                    inputs[nb, NBINPUTBITS + ADDITIONAL_INPUTS + numactionschosen[nb]] = 1  # Previously chosen action\n","\n","            \n","            # create a tensor from the inputs and pass it into the network \n","            inputsC = torch.from_numpy(inputs).to(device, dtype)\n","            inputs_tensor = Variable(inputsC, requires_grad=False)\n","            y, v, cache = net(inputs_tensor, cache)                        # y  should output raw scores, not probas\n","\n","            # we apply softmax to get probabilities\n","            y = torch.softmax(y, dim=1)             \n","\n","            # create distribution according to y and randomly choose an action accordingly\n","            distrib = torch.distributions.Categorical(y)\n","            actionschosen = distrib.sample()\n","            logprobs.append(distrib.log_prob(actionschosen))     # store log of proba\n","\n","            numactionschosen = actionschosen.data.cpu().numpy()  # We want to break gradients\n","            reward = np.zeros(params['batch_size'], dtype='float32')\n","\n","            \n","            '''\n","            # Function to calculate Rewards\n","            reward = vectorised_calculate_rewards(numactionschosen,reward,CurrentTrialStep,PresentTrialHasTarget,NoStepsInTrial,Cir)\n","            # print('Reward : ')\n","            # print(reward)\n","            all_stats['mean_reward'].append(np.median(reward))\n","            # print(np.median(reward))\n","            '''\n","            \n","            for nb in range(params['batch_size']):\n","                if numactionschosen[nb] == 1:\n","                    # Small penalty for any non-rest action taken\n","                    reward[nb]  -= params['penalty']\n","\n","\n","            ### DEBUGGING\n","            ## Easiest possible episode-dependent response (i.e. the easiest\n","            ## possible problem that actually require meta-learning, with ni=2)\n","            ## This one works pretty wel... But harder ones don't work well!\n","            #if numactionchosen == correctcue :\n","            #        reward = params['rew']\n","            #else:\n","            #        reward = -params['rew']\n","\n","\n","                trialstep[nb] += 1\n","                if trialstep[nb] == triallength[nb] - 1:\n","                    # This was the next-to-last step of the trial (and we showed the response signal, unless it was the first few steps in episode).\n","                    assert(cues[nb][trialstep[nb] - 1] == NUM_CUES or numstep < 2)\n","                    # We must deliver reward (which will be perceived by the agent at the next step), positive or negative, depending on response\n","                    if thistrialhascorrectcue[nb] and numactionschosen[nb] == 1:\n","                        reward[nb] += params['reward']\n","                    elif (not thistrialhascorrectcue[nb]) and numactionschosen[nb] == 0:\n","                        reward[nb] += params['reward']\n","                    else:\n","                        reward[nb] -= params['reward']\n","\n","                    if np.random.rand() < params['inversion_factor']:\n","                        reward[nb] = -reward[nb]\n","\n","                if trialstep[nb] == triallength[nb]:\n","                    # This was the last step of the trial (and we showed no input)\n","                    assert(cues[nb][trialstep[nb] - 1] == -1 or numstep < 2)\n","                    nbtrials[nb] += 1\n","                    totalnbtrials += 1\n","                    if thistrialhascorrectcue[nb]:\n","                        nbtrialswithcc += 1\n","                        #nbrewardabletrials += 1\n","                    # Trial is dead, long live trial\n","                    trialstep[nb] = 0\n","            \n","            \n","            rewards.append(reward)\n","            # time.sleep(3)\n","            vs.append(v) \n","            sumreward += reward\n","\n","            # This is an \"entropy penalty\", implemented by the sum-of-squares of the probabilities \n","            # The result is to encourage diversity in chosen actions.\n","            loss += ( params['entropy_reward_coeff'] * y.pow(2).sum() / params['batch_size'] )\n","\n","\n","\n","\n","        # Episode is done, now we do the actual computations of rewards and losses for the A2C algorithm\n","        R = torch.zeros(params['batch_size']).to(device)\n","        gammaR = params['reward_discount_factor']    #Discounting factor for rewards\n","        for numstepb in reversed(range(params['episode_length'])) :         # Every step in an episode has a reward\n","            R = gammaR * R + Variable(torch.from_numpy(rewards[numstepb]), requires_grad=False).to(device)\n","            ctrR = R - vs[numstepb][0]\n","            lossv += ctrR.pow(2).sum() / params['batch_size']\n","            loss -= (logprobs[numstepb] * ctrR.detach()).sum() / params['batch_size']\n","\n","\n","\n","        loss += params['lossv_coeff'] * lossv      #Coefficient for value prediction loss\n","        loss /= params['episode_length']\n","        \n","\n","        # perform backprop and gradient clipping\n","        loss.backward()\n","        all_stats['grad_norms'].append(torch.nn.utils.clip_grad_norm_(net.parameters(), params['gradient_clip']))\n","        if numiter > 100:  # Burn-in period for meanrewards\n","            optimizer.step()\n","\n","        # update stats\n","        lossnum = float(loss)\n","        lossbetweensaves += lossnum\n","        all_stats['losses_objective'].append(lossnum)\n","        all_stats['total_rewards'].append(sumreward.mean())\n","        all_stats['total_rewards_batchwise'].append(sumreward)\n","\n","\n","        if (numiter+1) % params['print_every'] == 0:\n","            print(f'\\nThis is episode {numiter}:')\n","            print(\"lossv: \", float(lossv))\n","            print (\"Total reward for this episode (all):\", sumreward, \"Dist:\", dist)\n","          \n","            print(numiter, \"====\")\n","            print(\"Mean loss: \", lossbetweensaves / params['print_every'])\n","            lossbetweensaves = 0\n","            print(\"Mean reward (across batch and last\", params['print_every'], \"eps.): \", np.sum( all_stats['total_rewards'][-params['print_every']:])/ params['print_every'])\n","            print(\"Mean reward (across batch): \", sumreward.mean())\n","            previoustime = nowtime\n","            nowtime = time.time()\n","            print(\"Time spent on last\", params['print_every'], \"iters: \", nowtime - previoustime)\n","\n","\n","\n","        if (numiter+1) % params['save_every'] ==0 :\n","          state = {\n","                'iters': numiter,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'stats_dict': all_stats\n","                }\n","          savepath='saved/Checkpoint'+params['net_type']+'.pt'\n","          torch.save(state,savepath)\n","          print(f\"Saved to '{savepath}' at episode {numiter}\")\n","\n","    return {\n","          'iters': params['num_episodes'],\n","          'state_dict': net.state_dict(),\n","          'optimizer': optimizer.state_dict(),\n","          'stats_dict': all_stats\n","          }\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tGGxhxgSdkpA"},"source":["### Experiments:"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IzrDPGeNbiSf"},"source":["**Initialising Default Parameters:**\n","\n","Here we define a global dictionary `params` containing all the parameters required during training and those required to create the recurrent neural networks. These values are changed as required before training for different experiments, by directly updating the values required. Therefore, most of the values defined in the following cell are NOT updated here itself. The exceptions to this rule are the hyperparameters: once their optimum values are determined in `experiment 2`, the default values are updated to reflect the results of this experiment.\n","\n","Thus, we also keep a copy of these default parameters in `default_params`, and define a function `reset_parameters()` that will reset all parameters. This is called before the start of each experiment, and it helps us to keep our experiments controlled easily.\n","\n","We also create a function `alter_params_for_save()` to remove functions from the dictionary so it can be pickled.\n","\n","We also initialise some other constants, for example number of additional inputs."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IAefLtr-bhin","colab":{}},"source":["# constants\n","NUM_CUE_BITS = 20\n","NUM_CUES = 4\n","ADDITIONAL_INPUTS = 4 # 1 input for the previous reward, 1 input for numstep, 1 unused,  1 \"Bias\" input\n","NUM_ACTIONS = 2   # YES OR NO\n","TOTAL_NUM_INPUTS =  NUM_CUE_BITS + 1 + ADDITIONAL_INPUTS + NUM_ACTIONS\n","\n","# these are kept constant while experimenting!\n","params = {\n","    # hyperparameters to be found out and fixed for optimum training (experiment 2)\n","    'learning_rate' : 1e-4 ,\n","    'hidden_size' : 200 ,\n","    'lambd' : 0 ,      # for l2 norm reg \n","\n","    # parameters for other experiments \n","    'net_type' : 'NP' ,\n","    'fully_modulated' : True ,\n","    'neuromod_neurons' : 1 ,\n","    \n","    # other parameters\n","    'episode_length' : 120 , \n","    'num_episodes' : 60000 ,\n","    'batch_size' : 10 ,\n","    'print_every' : 10000 ,  \n","    'rng_seed' : 0 ,\n","\n","    'penalty' :  0  , # Wall hitting penalty - not needed here\n","    'reward' : 1 ,\n","    'gradient_clip' : 2.0 ,  \n","    'activation' : torch.tanh ,\n","    'parameter_init_function' : nn.init.xavier_normal_ ,\n","    \n","    'eta_init_value' : 0.1 ,\n","    'neuromod_activation' : torch.tanh ,\n","    'inversion_factor' : 0 ,   # Probability Factor for inversion\n","    'entropy_reward_coeff' : 0.1 ,  \n","    'lossv_coeff': .1 ,        # Coefficient for value prediction loss\n","    'reward_discount_factor' : 0.9 ,\n","\n","    'input_size' : TOTAL_NUM_INPUTS ,\n","    'output_size' : NUM_ACTIONS ,\n","\n","    'verbose' : False ,\n","    'visualise': False ,\n","    'save_every' : 1000,\n","    'saved_already' : False\n","}\n","\n","\n","default_params = params\n","def reset_parameters():\n","    global params\n","    params = default_params\n","\n","# make sure to call reset after this\n","def alter_params_for_save():\n","    params['activation'] = None\n","    params['neuromod_activation'] = None\n","    params['parameter_init_function'] = None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YSUFnyZi44uK"},"source":["**Experiment 1: Different Types of Networks**\n","\n","Here we see the effect of the type of recurrent neural network on the reward accumulated and the total loss.\n","We choose some reasonable values for the hyperparameters now, and then set them to the optimum values later in `experiment 3` and repeat this study to validate our results.\n","\n","Only the parameter `net_type` is changed here.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Is75ImKqGpmC","outputId":"1bf75ff1-fece-44f9-c88c-7f4b7f5c0b3d","scrolled":true,"executionInfo":{"status":"error","timestamp":1591627405857,"user_tz":-330,"elapsed":22279400,"user":{"displayName":"Rudraksh Kapil","photoUrl":"","userId":"04893937089984046700"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2bb8d6ae1ecf488c99f4fe0365b9e2b4","ce9609155b154c8084a7236b7c02de5d","78bd75164f7a4b1b80bbb996475d9401","687658b8d9324cd0a0cb2d29beb1e826","ed7a536c74dc4b978286fbc201c663e8","1edaa744b2124ccf88bbc52281d84371","31cf920ffbe841d8bb5b5c73867f2ecb","f872ff15f5774585883902c52f7d7a11"]}},"source":["reset_parameters()\n","\n","# Plastic, Retroactive Modulation\n","params['net_type'] = 'SM'\n","SM_save = vectorized_train()\n","\n","savepath='saved/Final'+params['net_type']+'.pt'\n","torch.save(SM_save,savepath)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["{'learning_rate': 0.0001, 'hidden_size': 200, 'lambd': 0, 'net_type': 'SM', 'fully_modulated': True, 'neuromod_neurons': 1, 'episode_length': 120, 'num_episodes': 60000, 'batch_size': 10, 'print_every': 10000, 'rng_seed': 0, 'penalty': 0, 'reward': 1, 'gradient_clip': 2.0, 'activation': <built-in method tanh of type object at 0x7f6408afa300>, 'parameter_init_function': <function xavier_normal_ at 0x7f63c0cf69d8>, 'eta_init_value': 0.1, 'neuromod_activation': <built-in method tanh of type object at 0x7f6408afa300>, 'inversion_factor': 0, 'entropy_reward_coeff': 0.1, 'lossv_coeff': 0.1, 'reward_discount_factor': 0.9, 'input_size': 27, 'output_size': 2, 'verbose': False, 'visualise': False, 'save_every': 1000, 'saved_already': False}\n","final pls\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bb8d6ae1ecf488c99f4fe0365b9e2b4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=60000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Saved to 'saved/CheckpointSM.pt' at episode 999\n","Saved to 'saved/CheckpointSM.pt' at episode 1999\n","Saved to 'saved/CheckpointSM.pt' at episode 2999\n","Saved to 'saved/CheckpointSM.pt' at episode 3999\n","Saved to 'saved/CheckpointSM.pt' at episode 4999\n","Saved to 'saved/CheckpointSM.pt' at episode 5999\n","Saved to 'saved/CheckpointSM.pt' at episode 6999\n","Saved to 'saved/CheckpointSM.pt' at episode 7999\n","Saved to 'saved/CheckpointSM.pt' at episode 8999\n","\n","This is episode 9999:\n","lossv:  92.19738006591797\n","Total reward for this episode (all): [  5.   2.   6.   2.  -4. -10.  -3.   3.   3.   3.] Dist: 0\n","9999 ====\n","Mean loss:  0.1291422588162196\n","Mean reward (across batch and last 10000 eps.):  -0.008110000000000004\n","Mean reward (across batch):  0.7\n","Time spent on last 10000 iters:  5144.126960992813\n","Saved to 'saved/CheckpointSM.pt' at episode 9999\n","Saved to 'saved/CheckpointSM.pt' at episode 10999\n","Saved to 'saved/CheckpointSM.pt' at episode 11999\n","Saved to 'saved/CheckpointSM.pt' at episode 12999\n","Saved to 'saved/CheckpointSM.pt' at episode 13999\n","Saved to 'saved/CheckpointSM.pt' at episode 14999\n","Saved to 'saved/CheckpointSM.pt' at episode 15999\n","Saved to 'saved/CheckpointSM.pt' at episode 16999\n","Saved to 'saved/CheckpointSM.pt' at episode 17999\n","Saved to 'saved/CheckpointSM.pt' at episode 18999\n","\n","This is episode 19999:\n","lossv:  108.2276840209961\n","Total reward for this episode (all): [ 5.  1.  4.  8.  9.  2.  9. -5.  0.  2.] Dist: 0\n","19999 ====\n","Mean loss:  0.1284538439730126\n","Mean reward (across batch and last 10000 eps.):  0.014120000000000002\n","Mean reward (across batch):  3.5\n","Time spent on last 10000 iters:  5211.585728883743\n","Saved to 'saved/CheckpointSM.pt' at episode 19999\n","Saved to 'saved/CheckpointSM.pt' at episode 20999\n","Saved to 'saved/CheckpointSM.pt' at episode 21999\n","Saved to 'saved/CheckpointSM.pt' at episode 22999\n","Saved to 'saved/CheckpointSM.pt' at episode 23999\n","Saved to 'saved/CheckpointSM.pt' at episode 24999\n","Saved to 'saved/CheckpointSM.pt' at episode 25999\n","Saved to 'saved/CheckpointSM.pt' at episode 26999\n","Saved to 'saved/CheckpointSM.pt' at episode 27999\n","Saved to 'saved/CheckpointSM.pt' at episode 28999\n","\n","This is episode 29999:\n","lossv:  88.62374877929688\n","Total reward for this episode (all): [-6.  5.  0. -3.  4.  1.  1. -7.  6.  2.] Dist: 0\n","29999 ====\n","Mean loss:  0.1281651305425462\n","Mean reward (across batch and last 10000 eps.):  -0.0034599999999999995\n","Mean reward (across batch):  0.3\n","Time spent on last 10000 iters:  5289.960185050964\n","Saved to 'saved/CheckpointSM.pt' at episode 29999\n","Saved to 'saved/CheckpointSM.pt' at episode 30999\n","Saved to 'saved/CheckpointSM.pt' at episode 31999\n","Saved to 'saved/CheckpointSM.pt' at episode 32999\n","Saved to 'saved/CheckpointSM.pt' at episode 33999\n","Saved to 'saved/CheckpointSM.pt' at episode 34999\n","Saved to 'saved/CheckpointSM.pt' at episode 35999\n","Saved to 'saved/CheckpointSM.pt' at episode 36999\n","Saved to 'saved/CheckpointSM.pt' at episode 37999\n","Saved to 'saved/CheckpointSM.pt' at episode 38999\n","\n","This is episode 39999:\n","lossv:  93.5090560913086\n","Total reward for this episode (all): [-4.  4.  4. -6. -2.  0.  4.  2. -4. -2.] Dist: 0\n","39999 ====\n","Mean loss:  0.1282322766207928\n","Mean reward (across batch and last 10000 eps.):  0.009770000000000001\n","Mean reward (across batch):  -0.4\n","Time spent on last 10000 iters:  5248.597163677216\n","Saved to 'saved/CheckpointSM.pt' at episode 39999\n","Saved to 'saved/CheckpointSM.pt' at episode 40999\n","Saved to 'saved/CheckpointSM.pt' at episode 41999\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-59f832491e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plastic, Retroactive Modulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'SM'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mSM_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorized_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msavepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saved/Final'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-5f758b92db38>\u001b[0m in \u001b[0;36mvectorized_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# This is an \"entropy penalty\", implemented by the sum-of-squares of the probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;31m# The result is to encourage diversity in chosen actions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entropy_reward_coeff'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rNKVjtxKitfI","outputId":"49e97f93-817b-43a5-8953-3b755011ff23","executionInfo":{"status":"error","timestamp":1591594578659,"user_tz":-330,"elapsed":3764,"user":{"displayName":"Rudraksh Kapil","photoUrl":"","userId":"04893937089984046700"}},"colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["# plot loss over time\n","plt.subplot(2,1,1)\n","x = np.arange(params['num_episodes'])\n","# y_NP = NP_stats['losses_objective']\n","# y_NM = NM_stats['losses_objective']\n","y_SM = SM_stats['losses_objective']\n","#y_RM = RM_stats['losses_objective']\n","# plt.plot(x,y_NP)\n","# plt.plot(x,y_NM)\n","plt.plot(x,y_SM)\n","#plt.plot(x,y_RM)\n","\n","# plot rewards over time\n","plt.subplot(2,1,2)\n","# y_NP = NP_stats['total_rewards']\n","#y_NM = NM_stats['mean_reward']\n","y_SM = SM_stats['total_rewards']\n","#y_RM = RM_stats['total_rewards']\n","# plt.plot(x,y_NP)\n","#plt.plot(x,y_NM)\n","plt.plot(x,y_SM)\n","#plt.plot(x,y_RM)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e137fd92aa0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot loss over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_episodes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y_NP = NP_stats['losses_objective']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# y_NM = NM_stats['losses_objective']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"glHhSETU4Vc5","outputId":"519c466b-f49a-437b-b9f9-7dd54faa8522","executionInfo":{"status":"ok","timestamp":1591543884879,"user_tz":-330,"elapsed":7215,"user":{"displayName":"Rudraksh Kapil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoFYYIzudjawgt9sydxKemiYUjrtPu0UKhIh9F=s64","userId":"06594730916062239985"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# save location\n","SAVE_FOLDER = \"saved/experiment_1/\"\n","\n","# save\n","a_file = open(SAVE_FOLDER + params['net_type'] + \"_stats.pkl\", \"wb\")\n","pickle.dump(SM_stats, a_file) ## CHANGE THIS ACCORDINGLY\n","a_file.close()\n","\n","alter_params_for_save()\n","a_file = open(SAVE_FOLDER + params['net_type'] + \"_params.pkl\", \"wb\")\n","pickle.dump(params, a_file) \n","a_file.close()\n","\n","torch.save(SM_model.state_dict(), SAVE_FOLDER + params['net_type'] + \"_model.pt\") ## THIS AS WELL"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NDh4xtF2lwVa"},"source":["**Experiment 2: Hyperparameter Tuning**\n","\n","Here we try to tune the most important hyperparameters so we achieve good performance on the best network, the retro modulated RNN (as shown by `experiment 1`).\n","\n","We tune the following:\n","1. \n","2. \n","3.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ei6GvHgMlwmm","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-bJJa9cymU0V"},"source":["**Experiment 3: Different Types of Networks (repeated)**\n","\n","We must repeat `experiment 1` now with hyperparameters set to the best values found in `experiment 2` to confirm the findings of the first experiment. \n","\n","Again, only the parameter `net_type` is changed here.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F6jAYnCJmUs9","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VtcCYzV9IyNF"},"source":["**Experiment 4: Different Degrees Of Neuromodulation**\n","\n","Here we observe the effect of full-modulation versus half modulation with respect to reward accumulated by a Retroactive Modulated RNN and a Simple Modulated RNN.\n","\n","Only the parameter `"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"og9Uw9dQJZUL","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m-PLAsvSpMxS"},"source":["**Experiment 5: Different Number Of Neuromodulation Neurons** "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b5UWN6ZjzTXl","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pDFSqPuAzT7S"},"source":["**Experiment 6: Different Activation Functions for Neuromodulation Neurons** "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JktLCYHxpMk_","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dL1sYePOG7qQ"},"source":["### Results\n","\n","We can see..."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"40EpmOtOJX4I"},"source":[""]}]}