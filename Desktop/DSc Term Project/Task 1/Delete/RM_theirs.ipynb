{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RM_theirs.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8eba4feefc2e4192aad1b8c79b614b34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8745ece04d1e4cf9befc321f6b3eef35","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_43d95ba3b5064e85b0824d11d12d6c25","IPY_MODEL_e17c8ad828994b31a1d84ba4323188e5"]}},"8745ece04d1e4cf9befc321f6b3eef35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43d95ba3b5064e85b0824d11d12d6c25":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f43527935a0f4da5b39c2c73d5c96e0a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":28000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_50c67878fa7e4377acdf74641a055b04"}},"e17c8ad828994b31a1d84ba4323188e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0fb4dee7ff09494a8e584f3cb16f0af9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28000/28000 [3:14:16&lt;00:00,  2.40it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4089b967d3342f1a5e62897b75ecbe4"}},"f43527935a0f4da5b39c2c73d5c96e0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"50c67878fa7e4377acdf74641a055b04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fb4dee7ff09494a8e584f3cb16f0af9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4089b967d3342f1a5e62897b75ecbe4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LqiEYIQuMxMo"},"source":["# Cue Reward Association \n","\n","In this simple meta-learning task, one of four input cues is arbitrarily chosen as a *target cue*. The agent is repeatedly shown two random cues in succession, and then a *response cue* during which the agent must respond with a 1 if the target was part of the pair, or 0 otherwise. \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2EP3PdRlM2v9"},"source":["**Google Drive Set-up:**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GH-TD0QVWvsY","outputId":"23c277ca-cd60-46fa-f158-ea74e08475b8","executionInfo":{"status":"ok","timestamp":1591707668803,"user_tz":-330,"elapsed":27945,"user":{"displayName":"Rudraksha Kapil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1j4DjyQvmk6MNPd4_CWJ8BrD5-M2vQUJ9n6l5=s64","userId":"17469685325712467361"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# project folder, e.g. 'project submissions/A09_A10_A54'\n","# $$ for us - 'DSc Term Project/'\n","FOLDERNAME = 'DSc Term Project/Task 1'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# test - this notebooks name should show up:\n","# is oserror - restart runtime\n","%cd /content/drive/My\\ Drive/$FOLDERNAME\n","%ls "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1khPuOHY9jnXOARox2UYYUJ3TRirri2cS/DSc Term Project/Task 1\n","'Cue Reward Association.ipynb'   RM.ipynb          SM_rudraksh.ipynb\n"," NM.ipynb                        RM_theirs.ipynb   SM_theirs.ipynb\n"," NP.ipynb                        \u001b[0m\u001b[01;34msaved\u001b[0m/            \u001b[01;34mutils\u001b[0m/\n","'Plot Test.ipynb'                SM.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QW-zioHpNhqF"},"source":["**Import Statements:**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lYMYbf6BNuJo","colab":{}},"source":["import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch import optim\n","from torch.optim import lr_scheduler\n","\n","import numpy as np\n","import time\n","import pickle\n","import random\n","from tqdm.autonotebook import tqdm\n","#from graphics import *\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (20.0, 16.0) # set default size of plots\n","\n","\n","\n","np.set_printoptions(precision=4)\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZXE1D9PkN-E_"},"source":["**GPU Set-up:**\n","\n","You have an option to use GPU by setting the flag to `True` below.\n","\n","The global variables `dtype` and `device` will control the data types throughout this notebook. \n","\n","You need to manually switch to a GPU device. You can do this by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. Note that you have to rerun the cells from the top since the kernel gets restarted upon switching runtimes."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CH6wFaQaOB__","outputId":"c149345c-fa67-462e-e79d-b4234f402370","executionInfo":{"status":"ok","timestamp":1591707674944,"user_tz":-330,"elapsed":4002,"user":{"displayName":"Rudraksha Kapil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1j4DjyQvmk6MNPd4_CWJ8BrD5-M2vQUJ9n6l5=s64","userId":"17469685325712467361"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# flag\n","USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    dtype = torch.float32\n","else:\n","    device = torch.device('cpu')\n","    dtype = torch.float32\n","\n","print('Using Device:', device)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using Device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2pwUH5WHZKhd","colab_type":"code","colab":{}},"source":["import pdb\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np\n","import torch.nn.functional as F\n","\n","\n","\n","\n","##ttype = torch.FloatTensor;\n","#ttype = torch.cuda.FloatTensor;\n","\n","#ttype = torch.FloatTensor;\n","#ttype = torch.cuda.FloatTensor;\n","\n","\n","\n","class NonPlasticRNN(nn.Module):\n","    def __init__(self, params):\n","        super(NonPlasticRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","\n","    def forward(self, inputs, hidden): #, hebb):\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","\n","        # Here, the *rows* of w and hebb are the inputs weights to a single neuron\n","        # hidden = x, hactiv = y\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul(self.w,\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        #hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, hebb)),\n","        #                hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","\n","        hidden = hactiv\n","\n","        return activout, valueout, hidden #, hebb\n","\n","\n","    def initialZeroState(self):\n","        BATCHSIZE = self.params['bs']\n","        return Variable(torch.zeros(BATCHSIZE, self.params['hs']), requires_grad=False )\n","\n","\n","\n","\n","\n","class PlasticRNN(nn.Module):\n","    def __init__(self, params):\n","        super(PlasticRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.alpha =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.eta = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same eta\n","        #self.h2DA = torch.nn.Linear(params['hs'], NBDA)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","    def forward(self, inputs, hidden, hebb):\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","\n","        # Here, the *rows* of w and hebb are the inputs weights to a single neuron\n","        # hidden = x, hactiv = y\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, hebb)),\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","\n","        # Now computing the Hebbian updates...\n","\n","        # deltahebb has shape BS x HS x HS\n","        # Each row of hebb contain the input weights to a neuron\n","        deltahebb =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product...should it be other way round?\n","        hebb = torch.clamp(hebb + self.eta * deltahebb, min=-1.0, max=1.0)\n","\n","        hidden = hactiv\n","\n","        return activout, valueout, hidden, hebb\n","\n","    def initialZeroHebb(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","\n","    def initialZeroState(self):\n","        BATCHSIZE = self.params['bs']\n","        return Variable(torch.zeros(BATCHSIZE, self.params['hs']), requires_grad=False )\n","\n","\n","\n","\n","class SimpleModulRNN(nn.Module):\n","    def __init__(self, params):\n","        super(SimpleModulRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.alpha =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.eta = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same eta (only for the non-modulated part, if any!)\n","        self.h2DA = torch.nn.Linear(params['hs'], NBDA)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","    def forward_test(self, inputs, hidden, hebb):\n","        NBDA = 1\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul(self.w,\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","        return activout, valueout, 0, hidden, hebb\n","\n","    def forward(self, inputs, hidden, hebb):\n","        NBDA = 1\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","\n","        # Here, the *rows* of w and hebb are the inputs weights to a single neuron\n","        # hidden = x, hactiv = y\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, hebb)),\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","\n","        # Now computing the Hebbian updates...\n","\n","        # With batching, DAout is a matrix of size BS x 1 (Really BS x NBDA, but we assume NBDA=1 for now in the deltahebb multiplication below)\n","        if self.params['da'] == 'tanh':\n","            DAout = F.tanh(self.h2DA(hactiv))\n","        elif self.params['da'] == 'sig':\n","            DAout = F.sigmoid(self.h2DA(hactiv))\n","        elif self.params['da'] == 'lin':\n","            DAout =  self.h2DA(hactiv)\n","        else:\n","            raise ValueError(\"Which transformation for DAout ?\")\n","\n","        # deltahebb has shape BS x HS x HS\n","        # Each row of hebb contain the input weights to a neuron\n","        deltahebb =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product...should it be other way round?\n","\n","\n","        hebb1 = torch.clamp(hebb + DAout.view(BATCHSIZE, 1, 1) * deltahebb, min=-1.0, max=1.0)\n","        if self.params['fm'] == 0:\n","            # Non-modulated part\n","            hebb2 = torch.clamp(hebb + self.eta * deltahebb, min=-1.0, max=1.0)\n","        # Soft Clamp (note that it's different from just putting a tanh on top of a freely varying value):\n","        #hebb1 = torch.clamp( hebb +  torch.clamp(DAout.view(BATCHSIZE, 1, 1) * deltahebb, min=0.0) * (1 - hebb) +\n","        #        torch.clamp(DAout.view(BATCHSIZE, 1, 1)  * deltahebb, max=0.0) * (hebb + 1) , min=-1.0, max=1.0)\n","        #hebb2 = torch.clamp( hebb +  torch.clamp(self.eta * deltahebb, min=0.0) * (1 - hebb) +  torch.clamp(self.eta * deltahebb, max=0.0) * (hebb + 1) , min=-1.0, max=1.0)\n","        # Purely additive, no clamping. This will almost certainly diverge, don't use it!\n","        #hebb1 = hebb + DAout.view(BATCHSIZE, 1, 1) * deltahebb\n","        #hebb2 = hebb + self.eta * deltahebb\n","\n","        if self.params['fm'] == 1:\n","            hebb = hebb1\n","        elif self.params['fm'] == 0:\n","            # Combine the modulated and non-modulated part\n","            hebb = torch.cat( (hebb1[:, :self.params['hs']//2, :], hebb2[:,  self.params['hs'] // 2:, :]), dim=1) # Maybe along dim=2 instead?...\n","        else:\n","            raise ValueError(\"Must select whether fully modulated or not (params['fm'])\")\n","\n","        hidden = hactiv\n","\n","        return activout, valueout, DAout, hidden, hebb\n","\n","    def initialZeroHebb(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","\n","    def initialZeroState(self):\n","        BATCHSIZE = self.params['bs']\n","        return Variable(torch.zeros(BATCHSIZE, self.params['hs']), requires_grad=False )\n","\n","\n","\n","\n","\n","class RetroModulRNN(nn.Module):\n","    def __init__(self, params):\n","        super(RetroModulRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.alpha =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.eta = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same eta (only for the non-modulated part, if any!)\n","        self.etaet = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same etaet\n","        self.h2DA = torch.nn.Linear(params['hs'], NBDA)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","    def forward(self, inputs, hidden, hebb, et, pw):\n","            NBDA = 1\n","            BATCHSIZE = self.params['bs']\n","            HS = self.params['hs']\n","\n","            hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, pw)),\n","                            hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","            activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed later\n","            valueout = self.h2v(hactiv)\n","\n","            # Now computing the Hebbian updates...\n","\n","            # With batching, DAout is a matrix of size BS x 1 (Really BS x NBDA, but we assume NBDA=1 for now in the deltahebb multiplication below)\n","            if self.params['da'] == 'tanh':\n","                DAout = F.tanh(self.h2DA(hactiv))\n","            elif self.params['da'] == 'sig':\n","                DAout = F.sigmoid(self.h2DA(hactiv))\n","            elif self.params['da'] == 'lin':\n","                DAout =  self.h2DA(hactiv)\n","            else:\n","                raise ValueError(\"Which transformation for DAout ?\")\n","\n","            if self.params['rule'] == 'hebb':\n","                deltahebb =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product...should it be other way round?\n","            elif self.params['rule'] == 'oja':\n","                deltahebb =  torch.mul(hactiv.view(BATCHSIZE, HS, 1), (hidden.view(BATCHSIZE, 1, HS) - torch.mul(self.w.view(1, HS, HS), hactiv.view(BATCHSIZE, HS, 1))))\n","            else:\n","                raise ValueError(\"Must specify learning rule ('hebb' or 'oja')\")\n","\n","            # Hard clamp\n","            deltapw = DAout.view(BATCHSIZE,1,1) * et\n","            pw1 = torch.clamp(pw + deltapw, min=-1.0, max=1.0)\n","\n","            # Should we have a fully neuromodulated network, or only half?\n","            if self.params['fm'] == 1:\n","                pw = pw1\n","            elif self.params['fm']==0:\n","                hebb = torch.clamp(hebb + self.eta * deltahebb, min=-1.0, max=1.0)\n","                pw = torch.cat( (hebb[:, :self.params['hs']//2, :], pw1[:,  self.params['hs'] // 2:, :]), dim=1) # Maybe along dim=2 instead?...\n","            else:\n","                raise ValueError(\"Must select whether fully modulated or not\")\n","\n","            # Updating the eligibility trace - always a simple decay term.\n","            # Note that self.etaet != self.eta (which is used for hebb, i.e. the non-modulated part)\n","            deltaet = deltahebb\n","            et = (1 - self.etaet) * et + self.etaet *  deltaet\n","\n","            hidden = hactiv\n","            return activout, valueout, DAout, hidden, hebb, et, pw\n","\n","\n","\n","\n","    def initialZeroHebb(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","\n","    def initialZeroPlasticWeights(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","    def initialZeroState(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs']), requires_grad=False )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0RCXg3OZCXr","colab_type":"code","colab":{}},"source":["# Stimulus-response task as described in Miconi et al. ICLR 2019.\n","t\n","# Copyright (c) 2018-2019 Uber Technologies, Inc.\n","#\n","# Licensed under the Uber Non-Commercial License (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at the root directory of this project.\n","\n","import argparse\n","import pdb\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np\n","from numpy import random\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.optim import lr_scheduler\n","import random\n","import sys\n","import pickle\n","import time\n","import os\n","import platform\n","#import makemaze\n","\n","import numpy as np\n","#import matplotlib.pyplot as plt\n","import glob\n","\n","\n","\n","\n","\n","np.set_printoptions(precision=4)\n","\n","\n","\n","ADDINPUT = 4 # 1 inputs for the previous reward, 1 inputs for numstep, 1 unused,  1 \"Bias\" inputs\n","\n","\n","def train(paramdict):\n","    #params = dict(click.get_current_context().params)\n","\n","    #params['inputsize'] =  RFSIZE * RFSIZE + ADDINPUT + NBNONRESTACTIONS\n","    print(\"Starting training...\")\n","    params = {}\n","    #params.update(defaultParams)\n","    params.update(paramdict)\n","    print(\"Passed params: \", params)\n","    print(platform.uname())\n","    #params['nbsteps'] = params['nbshots'] * ((params['prestime'] + params['interpresdelay']) * params['nbclasses']) + params['prestimetest']  # Total number of steps per episode\n","    suffix = \"SRB_\"+\"\".join([str(x)+\"_\" if pair[0] != 'pe' and pair[0] != 'nbsteps' and pair[0] != 'rngseed' and pair[0] != 'save_every' and pair[0] != 'test_every' else '' for pair in sorted(zip(params.keys(), params.values()), key=lambda x:x[0] ) for x in pair])[:-1] + \"_rngseed_\" + str(params['rngseed'])   # Turning the parameters into a nice suffix for filenames\n","    print(suffix)\n","\n","    #NBINPUTBITS = params['ni'] + 1\n","    NBINPUTBITS = params['cs'] + 1 # The additional bit is for the response cue (i.e. the \"Go\" cue)\n","    params['outputsize'] =  2  # \"response\" and \"no response\"\n","    params['inputsize'] = NBINPUTBITS +  params['outputsize'] + ADDINPUT  # The total number of input bits is the size of inputs, plus the \"response cue\" input, plus the number of actions, plus the number of additional inputs\n","\n","    # This doesn't work with our version of PyTorch\n","    #params['device'] = 'gpu'\n","    #device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","    BS = params['bs']\n","\n","    # Initialize random seeds (first two redundant?)\n","    print(\"Setting random seeds\")\n","    np.random.seed(params['rngseed']); random.seed(params['rngseed']); torch.manual_seed(params['rngseed'])\n","\n","    print(\"Initializing network\")\n","    if params['type'] == 'modul':\n","        net = RetroModulRNN(params)\n","    elif params['type'] == 'modplast':\n","        net = SimpleModulRNN(params)\n","    elif params['type'] == 'plastic':\n","        net = PlasticRNN(params)\n","    elif params['type'] == 'rnn':\n","        net = NonPlasticRNN(params)\n","    net.to(device)\n","\n","    print (\"Shape of all optimized parameters:\", [x.size() for x in net.parameters()])\n","    allsizes = [torch.numel(x.data.cpu()) for x in net.parameters()]\n","    print (\"Size (numel) of all optimized elements:\", allsizes)\n","    print (\"Total size (numel) of all optimized elements:\", sum(allsizes))\n","\n","    #total_loss = 0.0\n","    print(\"Initializing optimizer\")\n","    #optimizer = torch.optim.SGD(net.parameters(), lr=1.0*params['lr'], weight_decay=params['l2'])\n","    #optimizer = torch.optim.RMSprop(net.parameters(), lr=1.0*params['lr'], weight_decay=params['l2'])\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1.0*params['lr'], eps=params['eps'], weight_decay=params['l2'])\n","    #optimizer = torch.optim.Adam(net.parameters(), lr=1.0*params['lr'], eps=1e-4, weight_decay=params['l2'])\n","    #optimizer = torch.optim.SGD(net.parameters(), lr=1.0*params['lr'])\n","    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=params['gamma'], step_size=params['steplr'])\n","\n","    #LABSIZE = params['lsize']\n","    #lab = np.ones((LABSIZE, LABSIZE))\n","    #CTR = LABSIZE // 2\n","\n","    # Simple cross maze\n","    #lab[CTR, 1:LABSIZE-1] = 0\n","    #lab[1:LABSIZE-1, CTR] = 0\n","\n","\n","    # Double-T maze\n","    #lab[CTR, 1:LABSIZE-1] = 0\n","    #lab[1:LABSIZE-1, 1] = 0\n","    #lab[1:LABSIZE-1, LABSIZE - 2] = 0\n","\n","    # Grid maze\n","    #lab[1:LABSIZE-1, 1:LABSIZE-1].fill(0)\n","    #for row in range(1, LABSIZE - 1):\n","    #    for col in range(1, LABSIZE - 1):\n","    #        if row % 2 == 0 and col % 2 == 0:\n","    #            lab[row, col] = 1\n","    #lab[CTR,CTR] = 0 # Not strictly necessary, but perhaps helps loclization by introducing a detectable irregularity in the center\n","\n","\n","    #LABSIZE = params['msize']\n","    #lab = np.ones((LABSIZE, LABSIZE))\n","    #CTR = LABSIZE // 2\n","\n","\n","    ## Grid maze\n","    #lab[1:LABSIZE-1, 1:LABSIZE-1].fill(0)\n","    #for row in range(1, LABSIZE - 1):\n","    #    for col in range(1, LABSIZE - 1):\n","    #        if row % 2 == 0 and col % 2 == 0:\n","    #            lab[row, col] = 1\n","    #lab[CTR,CTR] = 0 # Not strictly necessary, but perhaps helps loclization by introducing a detectable irregularity in the center\n","\n","    if params['saved_already'] == False:\n","        # create dictionary to keep track of stats over all episodes\n","        all_losses_objective = []\n","        all_total_rewards = []\n","        epoch=0\n","    else:\n","        checkpoint_path = 'saved/Checkpoint' + params['net_type'] + '.pt'\n","        checkpoint = torch.load(checkpoint_path)\n","        print('Loading states 1')\n","        print(net.load_state_dict(checkpoint['state_dict']))\n","        print('Loading states 2')\n","        print(optimizer.load_state_dict(checkpoint['optimizer']))\n","        epoch = checkpoint['iters'] + 1\n","        stats=checkpoint['stats_dict'] \n","        all_total_rewards = stats['total_rewards']\n","        all_losses_objective = stats['losses_objectives']\n","        print(f\"Loaded from '{checkpoint_path}' at episode {epoch}\")\n","\n","    all_losses = []\n","    all_grad_norms = []\n","    all_losses_v = []\n","    lossbetweensaves = 0\n","    nowtime = time.time()\n","    #meanreward = np.zeros((LABSIZE, LABSIZE))\n","    meanreward = np.zeros(params['ni'])\n","    meanrewardT = np.zeros((params['ni'], params['eplen']))\n","\n","    nbtrials = [0]*BS\n","    totalnbtrials = 0\n","    nbtrialswithcc = 0\n","\n","\n","    print(\"Starting episodes!\")\n","\n","    for numepisode in tqdm(range(epoch, params['nbiter'])):\n","\n","        PRINTTRACE = 0\n","        #if (numepisode+1) % (1 + params['pe']) == 0:\n","        if (numepisode+1) % (params['pe']) == 0:\n","            PRINTTRACE = 1\n","\n","        #lab = makemaze.genmaze(size=LABSIZE, nblines=4)\n","        #count = np.zeros((LABSIZE, LABSIZE))\n","\n","        # # Select the reward location for this episode - not on a wall!\n","        # rposr = 0; rposc = 0\n","        # while lab[rposr, rposc] == 1:\n","        #     rposr = np.random.randint(1, LABSIZE - 1)\n","        #     rposc = np.random.randint(1, LABSIZE - 1)\n","\n","        # # We always start the episode from the center (when hitting reward, we may teleport either to center or to a random location depending on params['rsp'])\n","        # posc = CTR\n","        # posr = CTR\n","\n","\n","\n","\n","        optimizer.zero_grad()\n","        loss = 0\n","        lossv = 0\n","        hidden = net.initialZeroState().to(device)\n","        if params['type'] != 'rnn':\n","            hebb = net.initialZeroHebb().to(device)\n","        if params['type'] == 'modul':\n","            et = net.initialZeroHebb().to(device) # Eligibility Trace is identical to Hebbian Trace in shape\n","            pw = net.initialZeroPlasticWeights().to(device)\n","        numactionchosen = 0\n","\n","\n","        # Generate the cues. Make sure they're all different (important when using very small cues for debugging, e.g. cs=2, ni=2)\n","        cuedata=[]\n","        for nb in range(BS):\n","            cuedata.append([])\n","            for ncue in range(params['ni']):\n","                assert len(cuedata[nb]) == ncue\n","                foundsame = 1\n","                cpt = 0\n","                while foundsame > 0 :\n","                    cpt += 1\n","                    if cpt > 10000:\n","                        # This should only occur with very weird parameters, e.g. cs=2, ni>4\n","                        raise ValueError(\"Could not generate a full list of different cues\")\n","                    foundsame = 0\n","                    candidate = np.random.randint(2, size=params['cs']) * 2 - 1\n","                    for backtrace in range(ncue):\n","                        if np.array_equal(cuedata[nb][backtrace], candidate):\n","                            foundsame = 1\n","\n","                cuedata[nb].append(candidate)\n","\n","\n","        reward = np.zeros(BS)\n","        sumreward = np.zeros(BS)\n","        rewards = []\n","        vs = []\n","        logprobs = []\n","        cues=[]\n","        for nb in range(BS):\n","            cues.append([])\n","        dist = 0\n","        numactionschosen = np.zeros(BS, dtype='int32')\n","\n","        #reward = 0.0\n","        #rewards = []\n","        #vs = []\n","        #logprobs = []\n","        #sumreward = 0.0\n","        nbtrials = np.zeros(BS)\n","        nbrewardabletrials = np.zeros(BS)\n","        thistrialhascorrectcue = np.zeros(BS)\n","        triallength = np.zeros(BS, dtype='int32')\n","        correctcue = np.random.randint(params['ni'], size=BS)\n","        trialstep = np.zeros(BS, dtype='int32')\n","\n","        #print(\"EPISODE \", numepisode)\n","        for numstep in range(params['eplen']):\n","\n","            #if params['clamp'] == 0:\n","            inputs = np.zeros((BS, params['inputsize']), dtype='float32')\n","            #else:\n","            #    inputs = np.zeros((1, params['hs']), dtype='float32')\n","\n","            for nb in range(BS):\n","\n","                if trialstep[nb] == 0:\n","                    thistrialhascorrectcue[nb] = 0\n","                    # Trial length is randomly modulated for each trial; first time step always -1 (i.e. no input cue), last time step always response-cue (i.e. NBINPUTBITS-1).\n","                    #triallength = params['ni'] // 2  + 3 + np.random.randint(1 + params['ni'])  # 3 fixed-cue time steps (1st, last and next-to-last) + some random nb of no-cue time steps\n","                    triallength[nb] = params['ni'] // 2  + 3 + np.random.randint(params['ni'])  # 3 fixed-cue time steps (1st, last and next-to-last) + some random nb of no-cue time steps\n","\n","\n","\n","                    # In any trial, we only show half the cues (randomly chosen), once each:\n","                    mycues = [x for x in range(params['ni'])]\n","                    random.shuffle(mycues); mycues = mycues[:len(mycues) // 2]\n","                    # The rest is filled with no-input time steps (i.e. cue = -1), but also with the 3 fixed-cue steps (1st, last, next-to-last)\n","                    for nc in range(triallength[nb] - 3  - len(mycues)):\n","                        mycues.append(-1)\n","                    random.shuffle(mycues)\n","                    mycues.insert(0, -1); mycues.append(params['ni']); mycues.append(-1)  # The first and last time step have no input (cue -1), the next-to-last has the response cue.\n","                    assert(len(mycues) == triallength[nb])\n","                    cues[nb] = mycues\n","\n","\n","                inputs[nb, :NBINPUTBITS] = 0\n","                if cues[nb][trialstep[nb]] > -1 and cues[nb][trialstep[nb]] < params['ni']:\n","                    #inputs[0, cues[trialstep]] = 1.0\n","                    inputs[nb, :NBINPUTBITS-1] = cuedata[nb][cues[nb][trialstep[nb]]][:]\n","                    if cues[nb][trialstep[nb]] == correctcue[nb]:\n","                        thistrialhascorrectcue[nb] = 1\n","                if cues[nb][trialstep[nb]] == params['ni']:\n","                    inputs[nb, NBINPUTBITS-1] = 1  # \"Go\" cue\n","\n","\n","                inputs[nb, NBINPUTBITS + 0] = 1.0 # Bias neuron, probably not necessary\n","                inputs[nb,NBINPUTBITS +  1] = numstep / params['eplen']\n","                inputs[nb, NBINPUTBITS + 2] = 1.0 * reward[nb] # Reward from previous time step\n","                if numstep > 0:\n","                    inputs[nb, NBINPUTBITS + ADDINPUT + numactionschosen[nb]] = 1  # Previously chosen action\n","\n","            inputsC = torch.from_numpy(inputs).to(device)\n","            # Might be better:\n","            #if rposr == posr and rposc = posc:\n","            #    inputs[0][-4] = 100.0\n","            #else:\n","            #    inputs[0][-4] = 0\n","\n","            # Running the network\n","\n","            ## Running the network\n","            if params['type'] == 'modplast':\n","                y, v, DAout, hidden, hebb = net(Variable(inputsC, requires_grad=False).to(device), hidden, hebb)  # y  should output raw scores, not probas\n","            elif params['type'] == 'modul':\n","                y, v, DAout, hidden, hebb, et, pw  = net(Variable(inputsC, requires_grad=False).to(device), hidden, hebb, et, pw)  # y  should output raw scores, not probas\n","            elif params['type'] == 'plastic':\n","                y, v, hidden, hebb = net(Variable(inputsC, requires_grad=False).to(device), hidden, hebb)  # y  should output raw scores, not probas\n","            elif params['type'] == 'rnn':\n","                y, v, hidden = net(Variable(inputsC, requires_grad=False).to(device), hidden)  # y  should output raw scores, not probas\n","            else:\n","                raise ValueError(\"Network type unknown or not yet implemented!\")\n","\n","\n","\n","            y = F.softmax(y, dim=1)\n","            # Must convert y to probas to use this !\n","            distrib = torch.distributions.Categorical(y)\n","            actionschosen = distrib.sample()\n","            logprobs.append(distrib.log_prob(actionschosen))\n","            numactionschosen = actionschosen.data.cpu().numpy()    # Turn to scalar\n","\n","            if PRINTTRACE:\n","                print(\"Step \", numstep, \" Inputs (1st in batch): \", inputs[0,:params['inputsize']], \" - Outputs(0): \", y.data.cpu().numpy()[0,:], \" - action chosen(0): \", numactionschosen[0],\n","                        \"TrialLen(0):\", triallength[0], \"trialstep(0):\", trialstep[0], \"TTHCC(0): \", thistrialhascorrectcue[0], \" -Reward (previous step): \", reward[0], \", cues(0):\", cues[0], \", cc(0):\", correctcue[0])\n","\n","                #print(\"Step \", numstep, \" Inputs: \", inputs[0,:params['inputsize']], \" - Outputs: \", y.data.cpu().numpy(), \" - action chosen: \", numactionchosen,\n","                #        \" - mean abs pw: \", np.mean(np.abs(pw.data.cpu().numpy())), \"TrialLen:\", triallength, \"trialstep:\", trialstep, \"TTHCC: \", thistrialhascorrectcue, \" -Reward (previous step): \", reward, \", cues:\", cues, \", cc:\", correctcue)\n","\n","            reward = np.zeros(BS, dtype='float32')\n","\n","            for nb in range(BS):\n","                if numactionschosen[nb] == 1:\n","                    # Small penalty for any non-rest action taken\n","                    reward[nb]  -= params['wp']\n","\n","\n","            ### DEBUGGING\n","            ## Easiest possible episode-dependent response (i.e. the easiest\n","            ## possible problem that actually require meta-learning, with ni=2)\n","            ## This one works pretty wel... But harder ones don't work well!\n","            #if numactionchosen == correctcue :\n","            #        reward = params['rew']\n","            #else:\n","            #        reward = -params['rew']\n","\n","\n","                trialstep[nb] += 1\n","                if trialstep[nb] == triallength[nb] - 1:\n","                    # This was the next-to-last step of the trial (and we showed the response signal, unless it was the first few steps in episode).\n","                    assert(cues[nb][trialstep[nb] - 1] == params['ni'] or numstep < 2)\n","                    # We must deliver reward (which will be perceived by the agent at the next step), positive or negative, depending on response\n","                    if thistrialhascorrectcue[nb] and numactionschosen[nb] == 1:\n","                        reward[nb] += params['rew']\n","                    elif (not thistrialhascorrectcue[nb]) and numactionschosen[nb] == 0:\n","                        reward[nb] += params['rew']\n","                    else:\n","                        reward[nb] -= params['rew']\n","\n","                    if np.random.rand() < params['pf']:\n","                        reward[nb] = -reward[nb]\n","\n","                if trialstep[nb] == triallength[nb]:\n","                    # This was the last step of the trial (and we showed no input)\n","                    assert(cues[nb][trialstep[nb] - 1] == -1 or numstep < 2)\n","                    nbtrials[nb] += 1\n","                    totalnbtrials += 1\n","                    if thistrialhascorrectcue[nb]:\n","                        nbtrialswithcc += 1\n","                        #nbrewardabletrials += 1\n","                    # Trial is dead, long live trial\n","                    trialstep[nb] = 0\n","\n","                    # We initialize the hidden state between trials!\n","                    #if params['is'] == 1:\n","                    #    hidden = net.initialZeroState()\n","\n","\n","\n","            rewards.append(reward)\n","            vs.append(v)\n","            sumreward += reward\n","\n","\n","\n","            #if params['alg'] in ['A3C' , 'REIE' , 'REIT']:\n","\n","            loss += (params['bent'] * y.pow(2).sum() / BS )   # We want to penalize concentration, i.e. encourage diversity; our version of PyTorch does not have an entropy() function for Distribution, so we use this instead.\n","\n","\n","\n","            ##if PRINTTRACE:\n","            ##    print(\"Probabilities:\", y.data.cpu().numpy(), \"Picked action:\", numactionchosen, \", got reward\", reward)\n","\n","        R = Variable(torch.zeros(BS), requires_grad=False).to(device)\n","        gammaR = params['gr']\n","        for numstepb in reversed(range(params['eplen'])) :\n","            R = gammaR * R + Variable(torch.from_numpy(rewards[numstepb]), requires_grad=False).to(device)\n","            ctrR = R - vs[numstepb][0]\n","            lossv += ctrR.pow(2).sum() / BS\n","            loss -= (logprobs[numstepb] * ctrR.detach()).sum() / BS  # Need to check if detach() is OK\n","            #pdb.set_trace()\n","\n","\n","        # Episode is done, now let's do the actual computations\n","        #gammaR = params['gr']\n","        #if params['alg'] == 'A3C':\n","        #    R = 0\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        R = gammaR * R + rewards[numstepb]\n","        #        lossv += (vs[numstepb][0] - R).pow(2)\n","        #        loss -= logprobs[numstepb] * (R - vs[numstepb].data[0][0])  # Not sure if the \"data\" is needed... put it b/c of worry about weird gradient flows\n","        #    loss += params['bv'] * lossv\n","\n","        #elif params['alg'] in ['REI', 'REIE']:\n","        #    R = sumreward\n","        #    baseline = meanreward[correctcue]\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        loss -= logprobs[numstepb] * (R - baseline)\n","        #elif params['alg'] == 'REIT':\n","        #    R = 0\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        R = gammaR * R + rewards[numstepb]\n","        #        loss -= logprobs[numstepb] * (R - meanrewardT[correctcue, numstepb])\n","        #else:\n","        #    raise ValueError(\"Must select algo type\")\n","        #elif params['alg'] == 'REINOB':\n","        #    R = sumreward\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        loss -= logprobs[numstepb] * R\n","        #elif params['alg'] == 'REITMP':\n","        #    R = 0\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        R = gammaR * R + rewards[numstepb]\n","        #        loss -= logprobs[numstepb] * R\n","\n","        #else:\n","        #    raise ValueError(\"Which algo?\")\n","\n","        #meanreward[correctcue] = (1.0 - params['nu']) * meanreward[correctcue] + params['nu'] * sumreward\n","        ##meanreward[rposr, rposc] = (1.0 - params['nu']) * meanreward[rposr, rposc] + params['nu'] * sumreward\n","        #R = 0\n","        #for numstepb in reversed(range(params['eplen'])) :\n","        #    R = gammaR * R + rewards[numstepb]\n","        #    meanrewardT[correctcue, numstepb] = (1.0 - params['nu']) * meanrewardT[correctcue, numstepb] + params['nu'] * R\n","\n","        loss += params['blossv'] * lossv\n","        loss /= params['eplen']\n","\n","        if PRINTTRACE:\n","            #if params['alg'] == 'A3C':\n","            print(\"lossv: \", float(lossv))\n","            #elif params['alg'] in ['REI', 'REIE', 'REIT']:\n","            #    print(\"meanreward baselines: \", [meanreward[x] for x in range(params['ni'])])\n","            print (\"Total reward for this episode(0):\", sumreward[0], \"Prop. of trials w/ rewarded cue:\", (nbtrialswithcc / totalnbtrials))\n","            print(\"Nb trials for this episode(0):\", nbtrials[0], \"[2]:\",nbtrials[2],\" Total Nb of trials:\", totalnbtrials)\n","\n","        #if params['squash'] == 1:\n","        #    if sumreward < 0:\n","        #        sumreward = -np.sqrt(-sumreward)\n","        #    else:\n","        #        sumreward = np.sqrt(sumreward)\n","        #elif params['squash'] == 0:\n","        #    pass\n","        #else:\n","        #    raise ValueError(\"Incorrect value for squash parameter\")\n","\n","        #loss *= sumreward\n","\n","        #for p in net.parameters():\n","        #    p.grad.data.clamp_(-params['clamp'], params['clamp'])\n","        loss.backward()\n","        all_grad_norms.append(torch.nn.utils.clip_grad_norm(net.parameters(), params['gc']))\n","        #if numepisode > 100:  # Burn-in period for meanreward\n","        optimizer.step()\n","\n","\n","        #print(sumreward)\n","        lossnum = float(loss)\n","        lossbetweensaves += lossnum\n","        all_losses_objective.append(lossnum)\n","        all_total_rewards.append(sumreward.mean())\n","        #all_total_rewards.append(sumreward[0])\n","            #all_losses_v.append(lossv.data[0])\n","        #total_loss  += lossnum\n","\n","\n","        if (numepisode+1) % params['pe'] == 0:\n","\n","            print(numepisode, \"====\")\n","            print(\"Mean loss: \", lossbetweensaves / params['pe'])\n","            lossbetweensaves = 0\n","            print(\"Mean reward: \", np.sum(all_total_rewards[-params['pe']:])/ params['pe'])\n","            previoustime = nowtime\n","            nowtime = time.time()\n","            print(\"Time spent on last\", params['pe'], \"iters: \", nowtime - previoustime)\n","            if params['type'] == 'plastic' or params['type'] == 'lstmplastic':\n","                print(\"ETA: \", float(net.eta), \"alpha[0,1]: \", net.alpha.data.cpu().numpy()[0,1], \"w[0,1]: \", net.w.data.cpu().numpy()[0,1] )\n","            elif params['type'] == 'modul' or params['type'] == 'modul2':\n","                print(\"ETA: \", net.eta.data.cpu().numpy(), \" etaet: \", net.etaet.data.cpu().numpy(), \" mean-abs pw: \", np.mean(np.abs(pw.data.cpu().numpy())))\n","            elif params['type'] == 'rnn':\n","                print(\"w[0,1]: \", net.w.data.cpu().numpy()[0,1] )\n","\n","        if (numepisode+1) % params['save_every'] == 0:\n","            stats = {\n","                'total_rewards':all_total_rewards,\n","                'losses_objectives':all_losses_objective\n","            }\n","            state = {\n","                'iters': numepisode,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'stats_dict': stats\n","            }\n","            savepath='saved/Checkpoint'+params['net_type']+'.pt'\n","            torch.save(state,savepath)\n","            print(f\"Saved to '{savepath}' at episode {numepisode}\")\n","\n","    stats = {\n","        'total_rewards':all_total_rewards,\n","        'losses_objectives':all_losses_objective\n","    }\n","    return {\n","          'iters': params['nbiter'],\n","          'state_dict': net.state_dict(),\n","          'optimizer': optimizer.state_dict(),\n","          'stats_dict': stats\n","    }\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L79-684hZTEQ","colab_type":"code","outputId":"5869a99e-205b-41fa-e128-977a9178f9f7","executionInfo":{"status":"ok","timestamp":1591719348675,"user_tz":-330,"elapsed":11658593,"user":{"displayName":"Rudraksha Kapil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1j4DjyQvmk6MNPd4_CWJ8BrD5-M2vQUJ9n6l5=s64","userId":"17469685325712467361"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8eba4feefc2e4192aad1b8c79b614b34","8745ece04d1e4cf9befc321f6b3eef35","43d95ba3b5064e85b0824d11d12d6c25","e17c8ad828994b31a1d84ba4323188e5","f43527935a0f4da5b39c2c73d5c96e0a","50c67878fa7e4377acdf74641a055b04","0fb4dee7ff09494a8e584f3cb16f0af9","f4089b967d3342f1a5e62897b75ecbe4"]}},"source":["params = {\n","    'net_type': 'RM', # for saving\n","    'type' : 'modul',\n","    'lr': 1e-4,\n","    'eplen': 120,\n","    'hs': 200,\n","    'l2':0,\n","    'pe':10000,\n","    'bv':0.1,\n","    'blossv':0.1,\n","    'bent':0.1,\n","    'rew':1,\n","    'wp':0,\n","    'save_every':1000,\n","    'da':'tanh',\n","    'clamp':0,\n","    'nbiter':60000,\n","    'fm':1,\n","    'ni':4,\n","    'pf':0.0,\n","    'alg':'A3C',\n","    'rule':'hebb',\n","    'cs':20,\n","    'eps':1e-6,\n","    'is':0,\n","    'bs':30,\n","    'gr':0.9,\n","    'gc':2.0,\n","    'rngseed':0,\n","    'saved_already':True\n","}\n","\n","\n","save = train(params)\n","savepath='saved/Final'+params['net_type']+'.pt'\n","torch.save(save,savepath)\n","print(\"Saved!\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Starting training...\n","Passed params:  {'net_type': 'RM', 'type': 'modul', 'lr': 0.0001, 'eplen': 120, 'hs': 200, 'l2': 0, 'pe': 10000, 'bv': 0.1, 'blossv': 0.1, 'bent': 0.1, 'rew': 1, 'wp': 0, 'save_every': 1000, 'da': 'tanh', 'clamp': 0, 'nbiter': 60000, 'fm': 1, 'ni': 4, 'pf': 0.0, 'alg': 'A3C', 'rule': 'hebb', 'cs': 20, 'eps': 1e-06, 'is': 0, 'bs': 30, 'gr': 0.9, 'gc': 2.0, 'rngseed': 0, 'saved_already': True}\n","uname_result(system='Linux', node='7a627b1ff775', release='4.19.104+', version='#1 SMP Wed Feb 19 05:26:34 PST 2020', machine='x86_64', processor='x86_64')\n","SRB_alg_A3C_bent_0.1_blossv_0.1_bs_30_bv_0.1_clamp_0_cs_20_da_tanh_eplen_120_eps_1e-06_fm_1_gc_2.0_gr_0.9_hs_200_is_0_l2_0_lr_0.0001_nbiter_60000_net_type_RM_ni_4_pf_0.0_rew_1_rule_hebb_saved_already_True_type_modul_wp_0_rngseed_0\n","Setting random seeds\n","Initializing network\n","Shape of all optimized parameters: [torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([1]), torch.Size([1]), torch.Size([200, 27]), torch.Size([200]), torch.Size([1, 200]), torch.Size([1]), torch.Size([2, 200]), torch.Size([2]), torch.Size([1, 200]), torch.Size([1])]\n","Size (numel) of all optimized elements: [40000, 40000, 1, 1, 5400, 200, 200, 1, 400, 2, 200, 1]\n","Total size (numel) of all optimized elements: 86406\n","Initializing optimizer\n","Loading states 1\n","<All keys matched successfully>\n","Loading states 2\n","None\n","Loaded from 'saved/CheckpointRM.pt' at episode 32000\n","Starting episodes!\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8eba4feefc2e4192aad1b8c79b614b34","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=28000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:471: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"],"name":"stderr"},{"output_type":"stream","text":["Saved to 'saved/CheckpointRM.pt' at episode 32999\n","Saved to 'saved/CheckpointRM.pt' at episode 33999\n","Saved to 'saved/CheckpointRM.pt' at episode 34999\n","Saved to 'saved/CheckpointRM.pt' at episode 35999\n","Saved to 'saved/CheckpointRM.pt' at episode 36999\n","Saved to 'saved/CheckpointRM.pt' at episode 37999\n","Saved to 'saved/CheckpointRM.pt' at episode 38999\n","Step  0  Inputs (1st in batch):  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0.]  - Outputs(0):  [0.5065 0.4935]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  1  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0083 0.     0.     0.     1.    ]  - Outputs(0):  [0.5063 0.4937]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  2  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0167 0.     0.     0.     1.    ]  - Outputs(0):  [0.5154 0.4846]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  3  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.    -1.     1.     1.    -1.     1.\n","  1.    -1.     1.     1.    -1.    -1.    -1.    -1.    -1.     1.\n","  0.     1.     0.025  0.     0.     1.     0.   ]  - Outputs(0):  [0.5028 0.4972]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  4  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5187 0.4813]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  5  Inputs (1st in batch):  [-1.     -1.     -1.      1.     -1.      1.      1.     -1.     -1.\n"," -1.     -1.      1.     -1.      1.      1.     -1.     -1.      1.\n","  1.     -1.      0.      1.      0.0417  0.      0.      0.      1.    ]  - Outputs(0):  [0.5218 0.4782]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  6  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   1.   1.   0.05 0.   0.   1.   0.  ]  - Outputs(0):  [0.506 0.494]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  7  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0583 1.     0.     1.     0.    ]  - Outputs(0):  [0.5129 0.4871]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  8  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0667 0.     0.     0.     1.    ]  - Outputs(0):  [0.512 0.488]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  9  Inputs (1st in batch):  [-1.    -1.    -1.     1.    -1.     1.     1.    -1.    -1.    -1.\n"," -1.     1.    -1.     1.     1.    -1.    -1.     1.     1.    -1.\n","  0.     1.     0.075  0.     0.     0.     1.   ]  - Outputs(0):  [0.5192 0.4808]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  10  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0833 0.     0.     0.     1.    ]  - Outputs(0):  [0.5098 0.4902]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  11  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5102 0.4898]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  12  Inputs (1st in batch):  [-1.  -1.   1.   1.   1.  -1.  -1.  -1.   1.  -1.  -1.   1.  -1.  -1.\n"," -1.   1.   1.  -1.  -1.  -1.   0.   1.   0.1  0.   0.   1.   0. ]  - Outputs(0):  [0.5037 0.4963]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  13  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1083 0.     0.     0.     1.    ]  - Outputs(0):  [0.5069 0.4931]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  14  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.1167 0.     0.     1.     0.    ]  - Outputs(0):  [0.5051 0.4949]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  15  Inputs (1st in batch):  [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     1.     0.125 -1.     0.     0.     1.   ]  - Outputs(0):  [0.5044 0.4956]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 2, -1, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  16  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1333 0.     0.     1.     0.    ]  - Outputs(0):  [0.5047 0.4953]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  17  Inputs (1st in batch):  [-1.     -1.     -1.      1.     -1.      1.      1.     -1.     -1.\n"," -1.     -1.      1.     -1.      1.      1.     -1.     -1.      1.\n","  1.     -1.      0.      1.      0.1417  0.      0.      0.      1.    ]  - Outputs(0):  [0.5062 0.4938]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  18  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.15 0.   0.   0.   1.  ]  - Outputs(0):  [0.5064 0.4936]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  19  Inputs (1st in batch):  [-1.     -1.      1.      1.      1.     -1.     -1.     -1.      1.\n"," -1.     -1.      1.     -1.     -1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.1583  0.      0.      0.      1.    ]  - Outputs(0):  [0.5023 0.4977]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  20  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.1667 0.     0.     0.     1.    ]  - Outputs(0):  [0.5069 0.4931]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  21  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.175 1.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5081 0.4919]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  22  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1833 0.     0.     1.     0.    ]  - Outputs(0):  [0.5078 0.4922]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  23  Inputs (1st in batch):  [-1.     -1.     -1.      1.     -1.      1.      1.     -1.     -1.\n"," -1.     -1.      1.     -1.      1.      1.     -1.     -1.      1.\n","  1.     -1.      0.      1.      0.1917  0.      0.      0.      1.    ]  - Outputs(0):  [0.5076 0.4924]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  24  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.2 0.  0.  1.  0. ]  - Outputs(0):  [0.5054 0.4946]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  25  Inputs (1st in batch):  [-1.     -1.      1.      1.      1.     -1.     -1.     -1.      1.\n"," -1.     -1.      1.     -1.     -1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.2083  0.      0.      1.      0.    ]  - Outputs(0):  [0.5001 0.4999]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  26  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.2167 0.     0.     0.     1.    ]  - Outputs(0):  [0.5053 0.4947]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  27  Inputs (1st in batch):  [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     1.     0.225 -1.     0.     0.     1.   ]  - Outputs(0):  [0.5053 0.4947]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 2, -1, 3, 4, -1] , cc(0): 1\n","Step  28  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5071 0.4929]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  29  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.      1.      1.     -1.\n","  1.      1.     -1.      1.      1.     -1.     -1.     -1.     -1.\n"," -1.      1.      0.      1.      0.2417  0.      0.      1.      0.    ]  - Outputs(0):  [0.5089 0.4911]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  30  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.25 0.   0.   0.   1.  ]  - Outputs(0):  [0.5078 0.4922]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  31  Inputs (1st in batch):  [-1.     -1.     -1.      1.     -1.      1.      1.     -1.     -1.\n"," -1.     -1.      1.     -1.      1.      1.     -1.     -1.      1.\n","  1.     -1.      0.      1.      0.2583  0.      0.      1.      0.    ]  - Outputs(0):  [0.51 0.49]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  32  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.2667 0.     0.     0.     1.    ]  - Outputs(0):  [0.5078 0.4922]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  33  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.275 1.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5055 0.4945]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 0, -1, 2, 4, -1] , cc(0): 1\n","Step  34  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2833 0.     0.     1.     0.    ]  - Outputs(0):  [0.5046 0.4954]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  35  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2917 0.     0.     0.     1.    ]  - Outputs(0):  [0.504 0.496]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  36  Inputs (1st in batch):  [ 1.  -1.  -1.  -1.   1.  -1.  -1.  -1.   1.  -1.  -1.  -1.  -1.   1.\n","  1.  -1.  -1.  -1.  -1.  -1.   0.   1.   0.3  0.   0.   1.   0. ]  - Outputs(0):  [0.4988 0.5012]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  37  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3083 0.     0.     0.     1.    ]  - Outputs(0):  [0.505 0.495]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  38  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3167 0.     0.     1.     0.    ]  - Outputs(0):  [0.5079 0.4921]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  39  Inputs (1st in batch):  [-1.    -1.     1.     1.     1.    -1.    -1.    -1.     1.    -1.\n"," -1.     1.    -1.    -1.    -1.     1.     1.    -1.    -1.    -1.\n","  0.     1.     0.325  0.     0.     0.     1.   ]  - Outputs(0):  [0.5037 0.4963]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  40  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.3333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5103 0.4897]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  41  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.3417 -1.      0.      1.      0.    ]  - Outputs(0):  [0.51 0.49]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, -1, 1, -1, -1, 3, 4, -1] , cc(0): 1\n","Step  42  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.35 0.   0.   0.   1.  ]  - Outputs(0):  [0.508 0.492]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 1\n","Step  43  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.      1.      1.     -1.\n","  1.      1.     -1.      1.      1.     -1.     -1.     -1.     -1.\n"," -1.      1.      0.      1.      0.3583  0.      0.      0.      1.    ]  - Outputs(0):  [0.5075 0.4925]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 1\n","Step  44  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3667 0.     0.     1.     0.    ]  - Outputs(0):  [0.5046 0.4954]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 1\n","Step  45  Inputs (1st in batch):  [ 1.    -1.    -1.    -1.     1.    -1.    -1.    -1.     1.    -1.\n"," -1.    -1.    -1.     1.     1.    -1.    -1.    -1.    -1.    -1.\n","  0.     1.     0.375  0.     0.     0.     1.   ]  - Outputs(0):  [0.498 0.502]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 1\n","Step  46  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.3833 0.     0.     0.     1.    ]  - Outputs(0):  [0.5031 0.4969]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 1\n","Step  47  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3917 1.     0.     0.     1.    ]  - Outputs(0):  [0.5056 0.4944]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 1\n","Step  48  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.4 0.  0.  1.  0. ]  - Outputs(0):  [0.5072 0.4928]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 2, -1, 4, -1] , cc(0): 1\n","Step  49  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4083 0.     0.     1.     0.    ]  - Outputs(0):  [0.5077 0.4923]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 2, -1, 4, -1] , cc(0): 1\n","Step  50  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.      1.     -1.     -1.     -1.      1.\n"," -1.     -1.     -1.     -1.      1.      1.     -1.     -1.     -1.\n"," -1.     -1.      0.      1.      0.4167  0.      0.      1.      0.    ]  - Outputs(0):  [0.5028 0.4972]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 2, -1, 4, -1] , cc(0): 1\n","Step  51  Inputs (1st in batch):  [-1.    -1.    -1.     1.    -1.     1.     1.    -1.    -1.    -1.\n"," -1.     1.    -1.     1.     1.    -1.    -1.     1.     1.    -1.\n","  0.     1.     0.425  0.     0.     1.     0.   ]  - Outputs(0):  [0.5109 0.4891]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 2, -1, 4, -1] , cc(0): 1\n","Step  52  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5073 0.4927]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 2, -1, 4, -1] , cc(0): 1\n","Step  53  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.4417 0.     0.     1.     0.    ]  - Outputs(0):  [0.5072 0.4928]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 2, -1, 4, -1] , cc(0): 1\n","Step  54  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.45 1.   0.   0.   1.  ]  - Outputs(0):  [0.5063 0.4937]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, 1, 2, -1, 4, -1] , cc(0): 1\n","Step  55  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4583 0.     0.     0.     1.    ]  - Outputs(0):  [0.5056 0.4944]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  56  Inputs (1st in batch):  [-1.     -1.     -1.      1.     -1.      1.      1.     -1.     -1.\n"," -1.     -1.      1.     -1.      1.      1.     -1.     -1.      1.\n","  1.     -1.      0.      1.      0.4667  0.      0.      1.      0.    ]  - Outputs(0):  [0.5084 0.4916]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  57  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.475 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5069 0.4931]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  58  Inputs (1st in batch):  [-1.     -1.      1.      1.      1.     -1.     -1.     -1.      1.\n"," -1.     -1.      1.     -1.     -1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.4833  0.      0.      1.      0.    ]  - Outputs(0):  [0.5019 0.4981]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  59  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5062 0.4938]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  60  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.5 0.  0.  1.  0. ]  - Outputs(0):  [0.5067 0.4933]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  61  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5083 1.     0.     1.     0.    ]  - Outputs(0):  [0.5048 0.4952]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 2, -1, 3, -1, 4, -1] , cc(0): 1\n","Step  62  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5167 0.     0.     1.     0.    ]  - Outputs(0):  [0.5047 0.4953]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, 4, -1] , cc(0): 1\n","Step  63  Inputs (1st in batch):  [-1.    -1.     1.     1.     1.    -1.    -1.    -1.     1.    -1.\n"," -1.     1.    -1.    -1.    -1.     1.     1.    -1.    -1.    -1.\n","  0.     1.     0.525  0.     0.     0.     1.   ]  - Outputs(0):  [0.4998 0.5002]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, 4, -1] , cc(0): 1\n","Step  64  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.      1.     -1.     -1.     -1.      1.\n"," -1.     -1.     -1.     -1.      1.      1.     -1.     -1.     -1.\n"," -1.     -1.      0.      1.      0.5333  0.      0.      0.      1.    ]  - Outputs(0):  [0.5009 0.4991]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, 4, -1] , cc(0): 1\n","Step  65  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.5417 0.     0.     1.     0.    ]  - Outputs(0):  [0.5067 0.4933]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, 4, -1] , cc(0): 1\n","Step  66  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.55 1.   0.   0.   1.  ]  - Outputs(0):  [0.5093 0.4907]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 3, 1, 4, -1] , cc(0): 1\n","Step  67  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5583 0.     0.     1.     0.    ]  - Outputs(0):  [0.5096 0.4904]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 1, -1, 4, -1] , cc(0): 1\n","Step  68  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5667 0.     0.     0.     1.    ]  - Outputs(0):  [0.5086 0.4914]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 1, -1, 4, -1] , cc(0): 1\n","Step  69  Inputs (1st in batch):  [-1.    -1.    -1.     1.    -1.     1.     1.    -1.    -1.    -1.\n"," -1.     1.    -1.     1.     1.    -1.    -1.     1.     1.    -1.\n","  0.     1.     0.575  0.     0.     0.     1.   ]  - Outputs(0):  [0.5095 0.4905]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 1, -1, 4, -1] , cc(0): 1\n","Step  70  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.      1.     -1.     -1.     -1.      1.\n"," -1.     -1.     -1.     -1.      1.      1.     -1.     -1.     -1.\n"," -1.     -1.      0.      1.      0.5833  0.      0.      0.      1.    ]  - Outputs(0):  [0.5008 0.4992]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 1, -1, 4, -1] , cc(0): 1\n","Step  71  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5042 0.4958]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 1, -1, 4, -1] , cc(0): 1\n","Step  72  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.6 0.  0.  1.  0. ]  - Outputs(0):  [0.5053 0.4947]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 1, -1, 4, -1] , cc(0): 1\n","Step  73  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6083 1.     0.     0.     1.    ]  - Outputs(0):  [0.5066 0.4934]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, 2, 1, -1, 4, -1] , cc(0): 1\n","Step  74  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6167 0.     0.     1.     0.    ]  - Outputs(0):  [0.506 0.494]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  75  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.    -1.     1.     1.    -1.     1.\n","  1.    -1.     1.     1.    -1.    -1.    -1.    -1.    -1.     1.\n","  0.     1.     0.625  0.     0.     0.     1.   ]  - Outputs(0):  [0.5087 0.4913]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  76  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5085 0.4915]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  77  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6417 0.     0.     0.     1.    ]  - Outputs(0):  [0.5069 0.4931]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  78  Inputs (1st in batch):  [-1.   -1.   -1.    1.   -1.    1.    1.   -1.   -1.   -1.   -1.    1.\n"," -1.    1.    1.   -1.   -1.    1.    1.   -1.    0.    1.    0.65  0.\n","  0.    0.    1.  ]  - Outputs(0):  [0.508 0.492]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  79  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6583 0.     0.     0.     1.    ]  - Outputs(0):  [0.5059 0.4941]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  80  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.6667 0.     0.     0.     1.    ]  - Outputs(0):  [0.5039 0.4961]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  81  Inputs (1st in batch):  [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     1.     0.675 -1.     0.     0.     1.   ]  - Outputs(0):  [0.5047 0.4953]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  82  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6833 0.     0.     0.     1.    ]  - Outputs(0):  [0.505 0.495]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 1, 4, -1] , cc(0): 1\n","Step  83  Inputs (1st in batch):  [-1.     -1.     -1.      1.     -1.      1.      1.     -1.     -1.\n"," -1.     -1.      1.     -1.      1.      1.     -1.     -1.      1.\n","  1.     -1.      0.      1.      0.6917  0.      0.      1.      0.    ]  - Outputs(0):  [0.5072 0.4928]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 1, 4, -1] , cc(0): 1\n","Step  84  Inputs (1st in batch):  [ 1.  -1.  -1.  -1.   1.  -1.  -1.  -1.   1.  -1.  -1.  -1.  -1.   1.\n","  1.  -1.  -1.  -1.  -1.  -1.   0.   1.   0.7  0.   0.   0.   1. ]  - Outputs(0):  [0.4994 0.5006]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 1, 4, -1] , cc(0): 1\n","Step  85  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.7083 0.     0.     0.     1.    ]  - Outputs(0):  [0.5077 0.4923]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 1, 4, -1] , cc(0): 1\n","Step  86  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7167 1.     0.     0.     1.    ]  - Outputs(0):  [0.5079 0.4921]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 2, 1, 4, -1] , cc(0): 1\n","Step  87  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.725 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5086 0.4914]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 1\n","Step  88  Inputs (1st in batch):  [-1.     -1.     -1.      1.     -1.      1.      1.     -1.     -1.\n"," -1.     -1.      1.     -1.      1.      1.     -1.     -1.      1.\n","  1.     -1.      0.      1.      0.7333  0.      0.      0.      1.    ]  - Outputs(0):  [0.5108 0.4892]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 1\n","Step  89  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.      1.      1.     -1.\n","  1.      1.     -1.      1.      1.     -1.     -1.     -1.     -1.\n"," -1.      1.      0.      1.      0.7417  0.      0.      1.      0.    ]  - Outputs(0):  [0.5094 0.4906]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 1\n","Step  90  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   1.   1.   0.75 0.   0.   0.   1.  ]  - Outputs(0):  [0.5069 0.4931]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 1\n","Step  91  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7583 1.     0.     1.     0.    ]  - Outputs(0):  [0.5056 0.4944]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 1\n","Step  92  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7667 0.     0.     0.     1.    ]  - Outputs(0):  [0.5045 0.4955]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 1\n","Step  93  Inputs (1st in batch):  [-1.    -1.     1.     1.     1.    -1.    -1.    -1.     1.    -1.\n"," -1.     1.    -1.    -1.    -1.     1.     1.    -1.    -1.    -1.\n","  0.     1.     0.775  0.     0.     0.     1.   ]  - Outputs(0):  [0.4994 0.5006]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 1\n","Step  94  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.      1.      1.     -1.\n","  1.      1.     -1.      1.      1.     -1.     -1.     -1.     -1.\n"," -1.      1.      0.      1.      0.7833  0.      0.      0.      1.    ]  - Outputs(0):  [0.5058 0.4942]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 1\n","Step  95  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5058 0.4942]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 1\n","Step  96  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.8 0.  0.  0.  1. ]  - Outputs(0):  [0.5058 0.4942]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 1\n","Step  97  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.8083 -1.      0.      0.      1.    ]  - Outputs(0):  [0.5065 0.4935]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 1\n","Step  98  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8167 0.     0.     1.     0.    ]  - Outputs(0):  [0.5085 0.4915]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  99  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.    -1.     1.     1.    -1.     1.\n","  1.    -1.     1.     1.    -1.    -1.    -1.    -1.    -1.     1.\n","  0.     1.     0.825  0.     0.     1.     0.   ]  - Outputs(0):  [0.507 0.493]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  100  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8333 0.     0.     1.     0.    ]  - Outputs(0):  [0.5063 0.4937]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  101  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8417 0.     0.     1.     0.    ]  - Outputs(0):  [0.5056 0.4944]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  102  Inputs (1st in batch):  [-1.   -1.   -1.    1.   -1.    1.    1.   -1.   -1.   -1.   -1.    1.\n"," -1.    1.    1.   -1.   -1.    1.    1.   -1.    0.    1.    0.85  0.\n","  0.    1.    0.  ]  - Outputs(0):  [0.5072 0.4928]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  103  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8583 0.     0.     0.     1.    ]  - Outputs(0):  [0.5045 0.4955]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  104  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.8667 0.     0.     1.     0.    ]  - Outputs(0):  [0.5075 0.4925]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  105  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.875 1.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5072 0.4928]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 0, -1, -1, 2, -1, 4, -1] , cc(0): 1\n","Step  106  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8833 0.     0.     0.     1.    ]  - Outputs(0):  [0.5074 0.4926]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, -1, 0, 4, -1] , cc(0): 1\n","Step  107  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5074 0.4926]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, -1, 0, 4, -1] , cc(0): 1\n","Step  108  Inputs (1st in batch):  [-1.  -1.   1.   1.   1.  -1.  -1.  -1.   1.  -1.  -1.   1.  -1.  -1.\n"," -1.   1.   1.  -1.  -1.  -1.   0.   1.   0.9  0.   0.   0.   1. ]  - Outputs(0):  [0.5008 0.4992]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, -1, 0, 4, -1] , cc(0): 1\n","Step  109  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9083 0.     0.     0.     1.    ]  - Outputs(0):  [0.505 0.495]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, -1, 0, 4, -1] , cc(0): 1\n","Step  110  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.      1.      1.     -1.\n","  1.      1.     -1.      1.      1.     -1.     -1.     -1.     -1.\n"," -1.      1.      0.      1.      0.9167  0.      0.      1.      0.    ]  - Outputs(0):  [0.5049 0.4951]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, -1, 0, 4, -1] , cc(0): 1\n","Step  111  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    1.    1.    0.925 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.5056 0.4944]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, -1, 0, 4, -1] , cc(0): 1\n","Step  112  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9333 1.     0.     1.     0.    ]  - Outputs(0):  [0.5062 0.4938]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, 3, -1, 0, 4, -1] , cc(0): 1\n","Step  113  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9417 0.     0.     1.     0.    ]  - Outputs(0):  [0.5082 0.4918]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 0, 4, -1] , cc(0): 1\n","Step  114  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.95 0.   0.   1.   0.  ]  - Outputs(0):  [0.5078 0.4922]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 0, 4, -1] , cc(0): 1\n","Step  115  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9583 0.     0.     1.     0.    ]  - Outputs(0):  [0.5064 0.4936]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 0, 4, -1] , cc(0): 1\n","Step  116  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.      1.     -1.     -1.     -1.      1.\n"," -1.     -1.     -1.     -1.      1.      1.     -1.     -1.     -1.\n"," -1.     -1.      0.      1.      0.9667  0.      0.      1.      0.    ]  - Outputs(0):  [0.5014 0.4986]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 0, 4, -1] , cc(0): 1\n","Step  117  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.    -1.     1.     1.    -1.     1.\n","  1.    -1.     1.     1.    -1.    -1.    -1.    -1.    -1.     1.\n","  0.     1.     0.975  0.     0.     1.     0.   ]  - Outputs(0):  [0.5067 0.4933]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 0, 4, -1] , cc(0): 1\n","Step  118  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.9833 0.     0.     1.     0.    ]  - Outputs(0):  [0.5049 0.4951]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 0, 4, -1] , cc(0): 1\n","Step  119  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9917 1.     0.     0.     1.    ]  - Outputs(0):  [0.5047 0.4953]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 1, 0, 4, -1] , cc(0): 1\n","lossv:  90.27611541748047\n","Total reward for this episode(0): 8.0 Prop. of trials w/ rewarded cue: 0.499903173754294\n","Nb trials for this episode(0): 18.0 [2]: 18.0  Total Nb of trials: 4332503\n","39999 ====\n","Mean loss:  0.10240716100046411\n","Mean reward:  0.006996666666666667\n","Time spent on last 10000 iters:  3373.421903848648\n","ETA:  [0.1]  etaet:  [0.3809]  mean-abs pw:  0.92509407\n","Saved to 'saved/CheckpointRM.pt' at episode 39999\n","Saved to 'saved/CheckpointRM.pt' at episode 40999\n","Saved to 'saved/CheckpointRM.pt' at episode 41999\n","Saved to 'saved/CheckpointRM.pt' at episode 42999\n","Saved to 'saved/CheckpointRM.pt' at episode 43999\n","Saved to 'saved/CheckpointRM.pt' at episode 44999\n","Saved to 'saved/CheckpointRM.pt' at episode 45999\n","Saved to 'saved/CheckpointRM.pt' at episode 46999\n","Saved to 'saved/CheckpointRM.pt' at episode 47999\n","Saved to 'saved/CheckpointRM.pt' at episode 48999\n","Step  0  Inputs (1st in batch):  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0.]  - Outputs(0):  [0.4992 0.5008]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 2, -1, -1, 4, -1] , cc(0): 2\n","Step  1  Inputs (1st in batch):  [ 1.     -1.      1.      1.     -1.     -1.      1.     -1.      1.\n"," -1.     -1.     -1.      1.      1.      1.     -1.      1.      1.\n","  1.      1.      0.      1.      0.0083  0.      0.      0.      1.    ]  - Outputs(0):  [0.4903 0.5097]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 2, -1, -1, 4, -1] , cc(0): 2\n","Step  2  Inputs (1st in batch):  [-1.      1.     -1.      1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.     -1.      1.     -1.     -1.      1.      1.     -1.\n"," -1.      1.      0.      1.      0.0167  0.      0.      1.      0.    ]  - Outputs(0):  [0.5145 0.4855]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 2, -1, -1, 4, -1] , cc(0): 2\n","Step  3  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.025 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.4998 0.5002]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 2, -1, -1, 4, -1] , cc(0): 2\n","Step  4  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5061 0.4939]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 2, -1, -1, 4, -1] , cc(0): 2\n","Step  5  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.0417 0.     0.     1.     0.    ]  - Outputs(0):  [0.51 0.49]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 2, -1, -1, 4, -1] , cc(0): 2\n","Step  6  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.05 1.   0.   0.   1.  ]  - Outputs(0):  [0.5216 0.4784]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 1, 2, -1, -1, 4, -1] , cc(0): 2\n","Step  7  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0583 0.     0.     1.     0.    ]  - Outputs(0):  [0.5349 0.4651]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 0, 4, -1] , cc(0): 2\n","Step  8  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0667 0.     0.     0.     1.    ]  - Outputs(0):  [0.5419 0.4581]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 0, 4, -1] , cc(0): 2\n","Step  9  Inputs (1st in batch):  [ 1.    -1.     1.     1.    -1.    -1.     1.    -1.     1.    -1.\n"," -1.    -1.     1.     1.     1.    -1.     1.     1.     1.     1.\n","  0.     1.     0.075  0.     0.     1.     0.   ]  - Outputs(0):  [0.5509 0.4491]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 0, 4, -1] , cc(0): 2\n","Step  10  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.     -1.      1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.0833  0.      0.      0.      1.    ]  - Outputs(0):  [0.5378 0.4622]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 0, 4, -1] , cc(0): 2\n","Step  11  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.0917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5301 0.4699]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 1, 0, 4, -1] , cc(0): 2\n","Step  12  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.1 1.  0.  1.  0. ]  - Outputs(0):  [0.4999 0.5001]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, 1, 0, 4, -1] , cc(0): 2\n","Step  13  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1083 0.     0.     1.     0.    ]  - Outputs(0):  [0.4877 0.5123]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  14  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1167 0.     0.     0.     1.    ]  - Outputs(0):  [0.4925 0.5075]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  15  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.125 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.4903 0.5097]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  16  Inputs (1st in batch):  [ 1.      1.     -1.     -1.     -1.     -1.      1.     -1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.     -1.      1.\n","  1.      1.      0.      1.      0.1333  0.      0.      0.      1.    ]  - Outputs(0):  [0.4916 0.5084]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  17  Inputs (1st in batch):  [ 1.     -1.      1.      1.     -1.     -1.      1.     -1.      1.\n"," -1.     -1.     -1.      1.      1.      1.     -1.      1.      1.\n","  1.      1.      0.      1.      0.1417  0.      0.      0.      1.    ]  - Outputs(0):  [0.5195 0.4805]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  18  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.15 0.   0.   0.   1.  ]  - Outputs(0):  [0.5049 0.4951]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  19  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.1583 0.     0.     0.     1.    ]  - Outputs(0):  [0.5108 0.4892]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  20  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1667 1.     0.     1.     0.    ]  - Outputs(0):  [0.4987 0.5013]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 3, 1, -1, 4, -1] , cc(0): 2\n","Step  21  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.175 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.5055 0.4945]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  22  Inputs (1st in batch):  [-1.      1.     -1.      1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.     -1.      1.     -1.     -1.      1.      1.     -1.\n"," -1.      1.      0.      1.      0.1833  0.      0.      1.      0.    ]  - Outputs(0):  [0.5273 0.4727]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  23  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1917 0.     0.     1.     0.    ]  - Outputs(0):  [0.4854 0.5146]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  24  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.2 0.  0.  1.  0. ]  - Outputs(0):  [0.5113 0.4887]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  25  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2083 0.     0.     1.     0.    ]  - Outputs(0):  [0.5137 0.4863]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  26  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.     -1.      1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.2167  0.      0.      0.      1.    ]  - Outputs(0):  [0.5058 0.4942]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  27  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    1.    1.    0.225 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5026 0.4974]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  28  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.2333 -1.      0.      1.      0.    ]  - Outputs(0):  [0.4811 0.5189]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, 2, -1, -1, -1, 0, 4, -1] , cc(0): 2\n","Step  29  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2417 0.     0.     1.     0.    ]  - Outputs(0):  [0.4958 0.5042]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, 3, 4, -1] , cc(0): 2\n","Step  30  Inputs (1st in batch):  [ 1.    1.    1.   -1.    1.   -1.   -1.    1.   -1.   -1.    1.   -1.\n"," -1.   -1.    1.    1.    1.   -1.   -1.   -1.    0.    1.    0.25  0.\n","  0.    1.    0.  ]  - Outputs(0):  [0.5027 0.4973]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, 3, 4, -1] , cc(0): 2\n","Step  31  Inputs (1st in batch):  [ 1.      1.     -1.     -1.     -1.     -1.      1.     -1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.     -1.      1.\n","  1.      1.      0.      1.      0.2583  0.      0.      1.      0.    ]  - Outputs(0):  [0.5064 0.4936]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, 3, 4, -1] , cc(0): 2\n","Step  32  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.2667 0.     0.     1.     0.    ]  - Outputs(0):  [0.5217 0.4783]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, 3, 4, -1] , cc(0): 2\n","Step  33  Inputs (1st in batch):  [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     1.     0.275 -1.     0.     0.     1.   ]  - Outputs(0):  [0.4917 0.5083]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 0, 3, 4, -1] , cc(0): 2\n","Step  34  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2833 0.     0.     1.     0.    ]  - Outputs(0):  [0.4925 0.5075]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  35  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2917 0.     0.     1.     0.    ]  - Outputs(0):  [0.4965 0.5035]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  36  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.3 0.  0.  0.  1. ]  - Outputs(0):  [0.4925 0.5075]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  37  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3083 0.     0.     1.     0.    ]  - Outputs(0):  [0.4896 0.5104]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  38  Inputs (1st in batch):  [-1.      1.     -1.      1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.     -1.      1.     -1.     -1.      1.      1.     -1.\n"," -1.      1.      0.      1.      0.3167  0.      0.      1.      0.    ]  - Outputs(0):  [0.5091 0.4909]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  39  Inputs (1st in batch):  [ 1.    -1.     1.     1.    -1.    -1.     1.    -1.     1.    -1.\n"," -1.    -1.     1.     1.     1.    -1.     1.     1.     1.     1.\n","  0.     1.     0.325  0.     0.     0.     1.   ]  - Outputs(0):  [0.4676 0.5324]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  40  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.3333 0.     0.     0.     1.    ]  - Outputs(0):  [0.4874 0.5126]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  41  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3417 1.     0.     0.     1.    ]  - Outputs(0):  [0.499 0.501]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  42  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.35 0.   0.   0.   1.  ]  - Outputs(0):  [0.4827 0.5173]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  43  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.     -1.      1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.3583  0.      0.      1.      0.    ]  - Outputs(0):  [0.4982 0.5018]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  44  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3667 0.     0.     1.     0.    ]  - Outputs(0):  [0.4967 0.5033]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  45  Inputs (1st in batch):  [ 1.    -1.     1.     1.    -1.    -1.     1.    -1.     1.    -1.\n"," -1.    -1.     1.     1.     1.    -1.     1.     1.     1.     1.\n","  0.     1.     0.375  0.     0.     1.     0.   ]  - Outputs(0):  [0.4809 0.5191]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  46  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3833 0.     0.     1.     0.    ]  - Outputs(0):  [0.4863 0.5137]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  47  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3917 0.     0.     1.     0.    ]  - Outputs(0):  [0.5056 0.4944]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  48  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.4 0.  0.  0.  1. ]  - Outputs(0):  [0.4905 0.5095]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  49  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.4083 -1.      0.      0.      1.    ]  - Outputs(0):  [0.481 0.519]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 0, -1, 1, -1, -1, 4, -1] , cc(0): 2\n","Step  50  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4167 0.     0.     0.     1.    ]  - Outputs(0):  [0.492 0.508]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  51  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.425 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.4857 0.5143]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  52  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4333 0.     0.     0.     1.    ]  - Outputs(0):  [0.4866 0.5134]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  53  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.     -1.      1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.4417  0.      0.      0.      1.    ]  - Outputs(0):  [0.4907 0.5093]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  54  Inputs (1st in batch):  [ 1.   -1.    1.    1.   -1.   -1.    1.   -1.    1.   -1.   -1.   -1.\n","  1.    1.    1.   -1.    1.    1.    1.    1.    0.    1.    0.45  0.\n","  0.    0.    1.  ]  - Outputs(0):  [0.4971 0.5029]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  55  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4583 0.     0.     0.     1.    ]  - Outputs(0):  [0.4907 0.5093]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  56  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.4667 0.     0.     1.     0.    ]  - Outputs(0):  [0.4976 0.5024]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  57  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.475 1.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.4924 0.5076]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 0, 1, -1, 4, -1] , cc(0): 2\n","Step  58  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4833 0.     0.     1.     0.    ]  - Outputs(0):  [0.5011 0.4989]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 2\n","Step  59  Inputs (1st in batch):  [-1.      1.     -1.      1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.     -1.      1.     -1.     -1.      1.      1.     -1.\n"," -1.      1.      0.      1.      0.4917  0.      0.      1.      0.    ]  - Outputs(0):  [0.5158 0.4842]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 2\n","Step  60  Inputs (1st in batch):  [ 1.   1.   1.  -1.   1.  -1.  -1.   1.  -1.  -1.   1.  -1.  -1.  -1.\n","  1.   1.   1.  -1.  -1.  -1.   0.   1.   0.5  0.   0.   0.   1. ]  - Outputs(0):  [0.4735 0.5265]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 2\n","Step  61  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.5083 0.     0.     0.     1.    ]  - Outputs(0):  [0.5016 0.4984]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 2\n","Step  62  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.5167 -1.      0.      1.      0.    ]  - Outputs(0):  [0.4887 0.5113]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 2\n","Step  63  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.525 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.493 0.507]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 3, 4, -1] , cc(0): 2\n","Step  64  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5333 0.     0.     1.     0.    ]  - Outputs(0):  [0.4938 0.5062]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 3, 4, -1] , cc(0): 2\n","Step  65  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5417 0.     0.     0.     1.    ]  - Outputs(0):  [0.4998 0.5002]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 3, 4, -1] , cc(0): 2\n","Step  66  Inputs (1st in batch):  [-1.    1.   -1.    1.   -1.    1.   -1.   -1.   -1.   -1.   -1.   -1.\n","  1.   -1.   -1.    1.    1.   -1.   -1.    1.    0.    1.    0.55  0.\n","  0.    1.    0.  ]  - Outputs(0):  [0.5057 0.4943]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 3, 4, -1] , cc(0): 2\n","Step  67  Inputs (1st in batch):  [ 1.      1.     -1.     -1.     -1.     -1.      1.     -1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.     -1.      1.\n","  1.      1.      0.      1.      0.5583  0.      0.      1.      0.    ]  - Outputs(0):  [0.479 0.521]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 3, 4, -1] , cc(0): 2\n","Step  68  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.5667 0.     0.     0.     1.    ]  - Outputs(0):  [0.5211 0.4789]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 3, 4, -1] , cc(0): 2\n","Step  69  Inputs (1st in batch):  [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     1.     0.575 -1.     0.     1.     0.   ]  - Outputs(0):  [0.4985 0.5015]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, -1, -1, 2, 3, 4, -1] , cc(0): 2\n","Step  70  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5833 0.     0.     1.     0.    ]  - Outputs(0):  [0.4971 0.5029]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  71  Inputs (1st in batch):  [ 1.      1.     -1.     -1.     -1.     -1.      1.     -1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.     -1.      1.\n","  1.      1.      0.      1.      0.5917  0.      0.      0.      1.    ]  - Outputs(0):  [0.4848 0.5152]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  72  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.6 0.  0.  1.  0. ]  - Outputs(0):  [0.524 0.476]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  73  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6083 0.     0.     1.     0.    ]  - Outputs(0):  [0.5168 0.4832]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  74  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.     -1.      1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.6167  0.      0.      0.      1.    ]  - Outputs(0):  [0.5075 0.4925]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  75  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.625 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5059 0.4941]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  76  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.6333 0.     0.     0.     1.    ]  - Outputs(0):  [0.4999 0.5001]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  77  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.6417 -1.      0.      0.      1.    ]  - Outputs(0):  [0.4719 0.5281]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 3, -1, -1, 0, -1, 4, -1] , cc(0): 2\n","Step  78  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.65 0.   0.   1.   0.  ]  - Outputs(0):  [0.4937 0.5063]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  79  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6583 0.     0.     1.     0.    ]  - Outputs(0):  [0.5016 0.4984]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  80  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6667 0.     0.     1.     0.    ]  - Outputs(0):  [0.5069 0.4931]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  81  Inputs (1st in batch):  [-1.     1.    -1.     1.    -1.     1.    -1.    -1.    -1.    -1.\n"," -1.    -1.     1.    -1.    -1.     1.     1.    -1.    -1.     1.\n","  0.     1.     0.675  0.     0.     1.     0.   ]  - Outputs(0):  [0.5153 0.4847]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  82  Inputs (1st in batch):  [ 1.     -1.      1.      1.     -1.     -1.      1.     -1.      1.\n"," -1.     -1.     -1.      1.      1.      1.     -1.      1.      1.\n","  1.      1.      0.      1.      0.6833  0.      0.      1.      0.    ]  - Outputs(0):  [0.4791 0.5209]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  83  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.6917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5071 0.4929]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  84  Inputs (1st in batch):  [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   1.   0.7 -1.   0.   1.   0. ]  - Outputs(0):  [0.4952 0.5048]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 2\n","Step  85  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7083 0.     0.     0.     1.    ]  - Outputs(0):  [0.4953 0.5047]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 3, 4, -1] , cc(0): 2\n","Step  86  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.     -1.      1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.7167  0.      0.      0.      1.    ]  - Outputs(0):  [0.4946 0.5054]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 3, 4, -1] , cc(0): 2\n","Step  87  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.725 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5032 0.4968]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 3, 4, -1] , cc(0): 2\n","Step  88  Inputs (1st in batch):  [ 1.      1.     -1.     -1.     -1.     -1.      1.     -1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.     -1.      1.\n","  1.      1.      0.      1.      0.7333  0.      0.      0.      1.    ]  - Outputs(0):  [0.4941 0.5059]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 3, 4, -1] , cc(0): 2\n","Step  89  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.7417 0.     0.     1.     0.    ]  - Outputs(0):  [0.5125 0.4875]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 3, 4, -1] , cc(0): 2\n","Step  90  Inputs (1st in batch):  [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n","  0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.75 -1.\n","  0.    0.    1.  ]  - Outputs(0):  [0.4952 0.5048]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 0, -1, 3, 4, -1] , cc(0): 2\n","Step  91  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7583 0.     0.     1.     0.    ]  - Outputs(0):  [0.4958 0.5042]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 0, -1, 4, -1] , cc(0): 2\n","Step  92  Inputs (1st in batch):  [ 1.     -1.      1.      1.     -1.     -1.      1.     -1.      1.\n"," -1.     -1.     -1.      1.      1.      1.     -1.      1.      1.\n","  1.      1.      0.      1.      0.7667  0.      0.      0.      1.    ]  - Outputs(0):  [0.4907 0.5093]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 0, -1, 4, -1] , cc(0): 2\n","Step  93  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.    -1.    -1.     1.    -1.    -1.\n","  1.    -1.    -1.    -1.     1.     1.     1.    -1.    -1.    -1.\n","  0.     1.     0.775  0.     0.     1.     0.   ]  - Outputs(0):  [0.5031 0.4969]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 0, -1, 4, -1] , cc(0): 2\n","Step  94  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7833 0.     0.     0.     1.    ]  - Outputs(0):  [0.5078 0.4922]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 0, -1, 4, -1] , cc(0): 2\n","Step  95  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.7917 0.     0.     1.     0.    ]  - Outputs(0):  [0.4943 0.5057]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 0, -1, 4, -1] , cc(0): 2\n","Step  96  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.8 1.  0.  1.  0. ]  - Outputs(0):  [0.4996 0.5004]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 1, 0, -1, 4, -1] , cc(0): 2\n","Step  97  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8083 0.     0.     0.     1.    ]  - Outputs(0):  [0.5001 0.4999]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  98  Inputs (1st in batch):  [ 1.     -1.      1.      1.     -1.     -1.      1.     -1.      1.\n"," -1.     -1.     -1.      1.      1.      1.     -1.      1.      1.\n","  1.      1.      0.      1.      0.8167  0.      0.      0.      1.    ]  - Outputs(0):  [0.4954 0.5046]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  99  Inputs (1st in batch):  [ 1.     1.    -1.    -1.    -1.    -1.     1.    -1.    -1.    -1.\n","  1.    -1.    -1.    -1.     1.     1.    -1.     1.     1.     1.\n","  0.     1.     0.825  0.     0.     1.     0.   ]  - Outputs(0):  [0.4928 0.5072]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  100  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8333 0.     0.     1.     0.    ]  - Outputs(0):  [0.5263 0.4737]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  101  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8417 0.     0.     0.     1.    ]  - Outputs(0):  [0.5104 0.4896]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  102  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.85 0.   0.   1.   0.  ]  - Outputs(0):  [0.499 0.501]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  103  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.8583 0.     0.     0.     1.    ]  - Outputs(0):  [0.4931 0.5069]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  104  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8667 1.     0.     1.     0.    ]  - Outputs(0):  [0.4916 0.5084]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 1, 3, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  105  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.875 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.5036 0.4964]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 2\n","Step  106  Inputs (1st in batch):  [ 1.      1.     -1.     -1.     -1.     -1.      1.     -1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.     -1.      1.\n","  1.      1.      0.      1.      0.8833  0.      0.      1.      0.    ]  - Outputs(0):  [0.5043 0.4957]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 2\n","Step  107  Inputs (1st in batch):  [-1.      1.     -1.      1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.     -1.      1.     -1.     -1.      1.      1.     -1.\n"," -1.      1.      0.      1.      0.8917  0.      0.      1.      0.    ]  - Outputs(0):  [0.5277 0.4723]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 2\n","Step  108  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.9 0.  0.  0.  1. ]  - Outputs(0):  [0.4785 0.5215]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 2\n","Step  109  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9083 1.     0.     0.     1.    ]  - Outputs(0):  [0.4927 0.5073]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 2\n","Step  110  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9167 0.     0.     0.     1.    ]  - Outputs(0):  [0.4909 0.5091]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  111  Inputs (1st in batch):  [ 1.     1.    -1.    -1.    -1.    -1.     1.    -1.    -1.    -1.\n","  1.    -1.    -1.    -1.     1.     1.    -1.     1.     1.     1.\n","  0.     1.     0.925  0.     0.     0.     1.   ]  - Outputs(0):  [0.4802 0.5198]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  112  Inputs (1st in batch):  [-1.      1.     -1.      1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.     -1.      1.     -1.     -1.      1.      1.     -1.\n"," -1.      1.      0.      1.      0.9333  0.      0.      1.      0.    ]  - Outputs(0):  [0.5285 0.4715]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  113  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9417 0.     0.     0.     1.    ]  - Outputs(0):  [0.4636 0.5364]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  114  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.95 0.   0.   1.   0.  ]  - Outputs(0):  [0.4939 0.5061]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  115  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9583 0.     0.     1.     0.    ]  - Outputs(0):  [0.508 0.492]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  116  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.9667 0.     0.     1.     0.    ]  - Outputs(0):  [0.5023 0.4977]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  117  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.975 1.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.5032 0.4968]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 3, 2, -1, -1, -1, 4, -1] , cc(0): 2\n","Step  118  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9833 0.     0.     1.     0.    ]  - Outputs(0):  [0.5072 0.4928]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 2\n","Step  119  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.     -1.     -1.      1.     -1.\n"," -1.      1.     -1.     -1.     -1.      1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.9917  0.      0.      0.      1.    ]  - Outputs(0):  [0.5131 0.4869]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 0, -1, 1, 4, -1] , cc(0): 2\n","lossv:  90.41349792480469\n","Total reward for this episode(0): 1.0 Prop. of trials w/ rewarded cue: 0.49977309329640457\n","Nb trials for this episode(0): 17.0 [2]: 18.0  Total Nb of trials: 9748500\n","49999 ====\n","Mean loss:  0.1326115635101929\n","Mean reward:  -0.005376666666666667\n","Time spent on last 10000 iters:  4130.382132291794\n","ETA:  [0.1]  etaet:  [0.3765]  mean-abs pw:  0.74211645\n","Saved to 'saved/CheckpointRM.pt' at episode 49999\n","Saved to 'saved/CheckpointRM.pt' at episode 50999\n","Saved to 'saved/CheckpointRM.pt' at episode 51999\n","Saved to 'saved/CheckpointRM.pt' at episode 52999\n","Saved to 'saved/CheckpointRM.pt' at episode 53999\n","Saved to 'saved/CheckpointRM.pt' at episode 54999\n","Saved to 'saved/CheckpointRM.pt' at episode 55999\n","Saved to 'saved/CheckpointRM.pt' at episode 56999\n","Saved to 'saved/CheckpointRM.pt' at episode 57999\n","Saved to 'saved/CheckpointRM.pt' at episode 58999\n","Step  0  Inputs (1st in batch):  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0.]  - Outputs(0):  [0.4944 0.5056]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 0, 4, -1] , cc(0): 0\n","Step  1  Inputs (1st in batch):  [-1.      1.     -1.      1.      1.     -1.     -1.      1.     -1.\n"," -1.     -1.      1.      1.      1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.0083  0.      0.      0.      1.    ]  - Outputs(0):  [0.4989 0.5011]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 0, 4, -1] , cc(0): 0\n","Step  2  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0167 0.     0.     1.     0.    ]  - Outputs(0):  [0.494 0.506]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 0, 4, -1] , cc(0): 0\n","Step  3  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.     1.    -1.    -1.     1.    -1.\n","  1.    -1.    -1.     1.    -1.     1.    -1.    -1.     1.    -1.\n","  0.     1.     0.025  0.     0.     0.     1.   ]  - Outputs(0):  [0.5436 0.4564]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 0, 4, -1] , cc(0): 0\n","Step  4  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.0333 0.     0.     1.     0.    ]  - Outputs(0):  [0.5136 0.4864]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, 0, 4, -1] , cc(0): 0\n","Step  5  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0417 1.     0.     0.     1.    ]  - Outputs(0):  [0.4882 0.5118]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 2, -1, 0, 4, -1] , cc(0): 0\n","Step  6  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.05 0.   0.   0.   1.  ]  - Outputs(0):  [0.4898 0.5102]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 0\n","Step  7  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.0583  0.      0.      1.      0.    ]  - Outputs(0):  [0.4648 0.5352]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 0\n","Step  8  Inputs (1st in batch):  [-1.      1.     -1.      1.      1.     -1.     -1.      1.     -1.\n"," -1.     -1.      1.      1.      1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.0667  0.      0.      1.      0.    ]  - Outputs(0):  [0.4899 0.5101]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 0\n","Step  9  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    1.    1.    0.075 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.4827 0.5173]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 0\n","Step  10  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.0833 -1.      0.      0.      1.    ]  - Outputs(0):  [0.5394 0.4606]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 3, 2, 4, -1] , cc(0): 0\n","Step  11  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.0917 0.     0.     1.     0.    ]  - Outputs(0):  [0.5636 0.4364]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  12  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.1 0.  0.  0.  1. ]  - Outputs(0):  [0.5364 0.4636]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  13  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1083 0.     0.     1.     0.    ]  - Outputs(0):  [0.4828 0.5172]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  14  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.1167  0.      0.      1.      0.    ]  - Outputs(0):  [0.5183 0.4817]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  15  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.     1.    -1.    -1.     1.    -1.\n","  1.    -1.    -1.     1.    -1.     1.    -1.    -1.     1.    -1.\n","  0.     1.     0.125  0.     0.     0.     1.   ]  - Outputs(0):  [0.5335 0.4665]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  16  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5116 0.4884]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  17  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.1417 0.     0.     0.     1.    ]  - Outputs(0):  [0.5143 0.4857]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  18  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.15 1.   0.   0.   1.  ]  - Outputs(0):  [0.4821 0.5179]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  19  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.1583 0.     0.     0.     1.    ]  - Outputs(0):  [0.4731 0.5269]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 0\n","Step  20  Inputs (1st in batch):  [-1.      1.     -1.      1.      1.     -1.     -1.      1.     -1.\n"," -1.     -1.      1.      1.      1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.1667  0.      0.      1.      0.    ]  - Outputs(0):  [0.4924 0.5076]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 0\n","Step  21  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.     1.    -1.    -1.     1.    -1.\n","  1.    -1.    -1.     1.    -1.     1.    -1.    -1.     1.    -1.\n","  0.     1.     0.175  0.     0.     1.     0.   ]  - Outputs(0):  [0.4812 0.5188]  - action chosen(0):  1 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 0\n","Step  22  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.1833 0.     0.     0.     1.    ]  - Outputs(0):  [0.4751 0.5249]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 0\n","Step  23  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.1917 -1.      0.      1.      0.    ]  - Outputs(0):  [0.4397 0.5603]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, 2, 0, 4, -1] , cc(0): 0\n","Step  24  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.2 0.  0.  1.  0. ]  - Outputs(0):  [0.4304 0.5696]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  25  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.2083  0.      0.      0.      1.    ]  - Outputs(0):  [0.5136 0.4864]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  26  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2167 0.     0.     0.     1.    ]  - Outputs(0):  [0.5386 0.4614]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  27  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.225 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.5712 0.4288]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  28  Inputs (1st in batch):  [-1.      1.     -1.      1.      1.     -1.     -1.      1.     -1.\n"," -1.     -1.      1.      1.      1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.2333  0.      0.      1.      0.    ]  - Outputs(0):  [0.5896 0.4104]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  29  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2417 0.     0.     1.     0.    ]  - Outputs(0):  [0.4907 0.5093]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  30  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   1.   1.   0.25 0.   0.   0.   1.  ]  - Outputs(0):  [0.4868 0.5132]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  31  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2583 1.     0.     1.     0.    ]  - Outputs(0):  [0.503 0.497]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 3, -1, -1, 2, -1, 4, -1] , cc(0): 0\n","Step  32  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2667 0.     0.     0.     1.    ]  - Outputs(0):  [0.4712 0.5288]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, -1, 4, -1] , cc(0): 0\n","Step  33  Inputs (1st in batch):  [ 1.    -1.    -1.    -1.    -1.     1.    -1.    -1.    -1.    -1.\n"," -1.     1.     1.     1.     1.    -1.    -1.     1.     1.     1.\n","  0.     1.     0.275  0.     0.     1.     0.   ]  - Outputs(0):  [0.5174 0.4826]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, -1, 4, -1] , cc(0): 0\n","Step  34  Inputs (1st in batch):  [ 1.     -1.      1.      1.      1.     -1.      1.      1.      1.\n"," -1.      1.     -1.      1.     -1.      1.      1.      1.      1.\n","  1.     -1.      0.      1.      0.2833  0.      0.      1.      0.    ]  - Outputs(0):  [0.5364 0.4636]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, -1, 4, -1] , cc(0): 0\n","Step  35  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.2917 0.     0.     1.     0.    ]  - Outputs(0):  [0.5415 0.4585]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, -1, 4, -1] , cc(0): 0\n","Step  36  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.3 0.  0.  1.  0. ]  - Outputs(0):  [0.4929 0.5071]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 1, -1, 4, -1] , cc(0): 0\n","Step  37  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3083 1.     0.     1.     0.    ]  - Outputs(0):  [0.4888 0.5112]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 3, 1, -1, 4, -1] , cc(0): 0\n","Step  38  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3167 0.     0.     0.     1.    ]  - Outputs(0):  [0.4751 0.5249]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 0\n","Step  39  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.325 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.4716 0.5284]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 0\n","Step  40  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3333 0.     0.     0.     1.    ]  - Outputs(0):  [0.4869 0.5131]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 0\n","Step  41  Inputs (1st in batch):  [-1.      1.     -1.      1.      1.     -1.     -1.      1.     -1.\n"," -1.     -1.      1.      1.      1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.3417  0.      0.      0.      1.    ]  - Outputs(0):  [0.5111 0.4889]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 0\n","Step  42  Inputs (1st in batch):  [ 1.   -1.    1.    1.    1.   -1.    1.    1.    1.   -1.    1.   -1.\n","  1.   -1.    1.    1.    1.    1.    1.   -1.    0.    1.    0.35  0.\n","  0.    0.    1.  ]  - Outputs(0):  [0.5346 0.4654]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 0\n","Step  43  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.3583 0.     0.     0.     1.    ]  - Outputs(0):  [0.5354 0.4646]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 0\n","Step  44  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.3667 1.     0.     1.     0.    ]  - Outputs(0):  [0.4908 0.5092]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 2, 1, 4, -1] , cc(0): 0\n","Step  45  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.375 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.4861 0.5139]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, 4, -1] , cc(0): 0\n","Step  46  Inputs (1st in batch):  [ 1.     -1.      1.      1.      1.     -1.      1.      1.      1.\n"," -1.      1.     -1.      1.     -1.      1.      1.      1.      1.\n","  1.     -1.      0.      1.      0.3833  0.      0.      1.      0.    ]  - Outputs(0):  [0.4808 0.5192]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, 4, -1] , cc(0): 0\n","Step  47  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.3917  0.      0.      1.      0.    ]  - Outputs(0):  [0.5013 0.4987]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, 4, -1] , cc(0): 0\n","Step  48  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.4 0.  0.  1.  0. ]  - Outputs(0):  [0.4886 0.5114]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 1, 3, 4, -1] , cc(0): 0\n","Step  49  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4083 1.     0.     1.     0.    ]  - Outputs(0):  [0.5032 0.4968]  - action chosen(0):  0 TrialLen(0): 5 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, 1, 3, 4, -1] , cc(0): 0\n","Step  50  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4167 0.     0.     1.     0.    ]  - Outputs(0):  [0.5155 0.4845]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  51  Inputs (1st in batch):  [ 1.    -1.    -1.    -1.    -1.     1.    -1.    -1.    -1.    -1.\n"," -1.     1.     1.     1.     1.    -1.    -1.     1.     1.     1.\n","  0.     1.     0.425  0.     0.     0.     1.   ]  - Outputs(0):  [0.5164 0.4836]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  52  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.      1.     -1.     -1.      1.\n"," -1.      1.     -1.     -1.      1.     -1.      1.     -1.     -1.\n","  1.     -1.      0.      1.      0.4333  0.      0.      0.      1.    ]  - Outputs(0):  [0.522 0.478]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  53  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4417 0.     0.     0.     1.    ]  - Outputs(0):  [0.5013 0.4987]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  54  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   1.   1.   0.45 0.   0.   0.   1.  ]  - Outputs(0):  [0.4847 0.5153]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  55  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4583 1.     0.     0.     1.    ]  - Outputs(0):  [0.4951 0.5049]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  56  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4667 0.     0.     0.     1.    ]  - Outputs(0):  [0.4941 0.5059]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, 4, -1] , cc(0): 0\n","Step  57  Inputs (1st in batch):  [-1.     1.    -1.     1.     1.    -1.    -1.     1.    -1.    -1.\n"," -1.     1.     1.     1.    -1.     1.     1.    -1.    -1.    -1.\n","  0.     1.     0.475  0.     0.     0.     1.   ]  - Outputs(0):  [0.4921 0.5079]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, 4, -1] , cc(0): 0\n","Step  58  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4833 0.     0.     0.     1.    ]  - Outputs(0):  [0.4874 0.5126]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, 4, -1] , cc(0): 0\n","Step  59  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.4917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5092 0.4908]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, 4, -1] , cc(0): 0\n","Step  60  Inputs (1st in batch):  [ 1.  -1.  -1.  -1.  -1.   1.  -1.  -1.  -1.  -1.  -1.   1.   1.   1.\n","  1.  -1.  -1.   1.   1.   1.   0.   1.   0.5  0.   0.   1.   0. ]  - Outputs(0):  [0.4969 0.5031]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, 4, -1] , cc(0): 0\n","Step  61  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.5083 0.     0.     1.     0.    ]  - Outputs(0):  [0.493 0.507]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 2, -1, -1, 3, 4, -1] , cc(0): 0\n","Step  62  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.5167 -1.      0.      0.      1.    ]  - Outputs(0):  [0.4779 0.5221]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  -1.0 , cues(0): [-1, 2, -1, -1, 3, 4, -1] , cc(0): 0\n","Step  63  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.525 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.4873 0.5127]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  64  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5333 0.     0.     1.     0.    ]  - Outputs(0):  [0.4888 0.5112]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  65  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.5417  0.      0.      1.      0.    ]  - Outputs(0):  [0.5137 0.4863]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  66  Inputs (1st in batch):  [ 1.    1.    1.   -1.    1.    1.   -1.   -1.    1.   -1.    1.   -1.\n"," -1.    1.   -1.    1.   -1.   -1.    1.   -1.    0.    1.    0.55  0.\n","  0.    0.    1.  ]  - Outputs(0):  [0.5262 0.4738]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  67  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5583 0.     0.     1.     0.    ]  - Outputs(0):  [0.5076 0.4924]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  68  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.5667 0.     0.     0.     1.    ]  - Outputs(0):  [0.4917 0.5083]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  69  Inputs (1st in batch):  [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     1.     0.575 -1.     0.     1.     0.   ]  - Outputs(0):  [0.4812 0.5188]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, -1, 3, 0, -1, 4, -1] , cc(0): 0\n","Step  70  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5833 0.     0.     1.     0.    ]  - Outputs(0):  [0.4853 0.5147]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 0, 4, -1] , cc(0): 0\n","Step  71  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.5917 0.     0.     0.     1.    ]  - Outputs(0):  [0.4818 0.5182]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 0, 4, -1] , cc(0): 0\n","Step  72  Inputs (1st in batch):  [-1.   1.  -1.   1.   1.  -1.  -1.   1.  -1.  -1.  -1.   1.   1.   1.\n"," -1.   1.   1.  -1.  -1.  -1.   0.   1.   0.6  0.   0.   1.   0. ]  - Outputs(0):  [0.4876 0.5124]  - action chosen(0):  0 TrialLen(0): 6 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 0, 4, -1] , cc(0): 0\n","Step  73  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.      1.     -1.     -1.      1.\n"," -1.      1.     -1.     -1.      1.     -1.      1.     -1.     -1.\n","  1.     -1.      0.      1.      0.6083  0.      0.      1.      0.    ]  - Outputs(0):  [0.4934 0.5066]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 0, 4, -1] , cc(0): 0\n","Step  74  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.6167 0.     0.     0.     1.    ]  - Outputs(0):  [0.5167 0.4833]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 2, 0, 4, -1] , cc(0): 0\n","Step  75  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.625 1.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.5127 0.4873]  - action chosen(0):  1 TrialLen(0): 6 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, 2, 0, 4, -1] , cc(0): 0\n","Step  76  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6333 0.     0.     0.     1.    ]  - Outputs(0):  [0.5256 0.4744]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 3, 4, -1] , cc(0): 0\n","Step  77  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6417 0.     0.     0.     1.    ]  - Outputs(0):  [0.5048 0.4952]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 3, 4, -1] , cc(0): 0\n","Step  78  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.65 0.   0.   0.   1.  ]  - Outputs(0):  [0.4821 0.5179]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 3, 4, -1] , cc(0): 0\n","Step  79  Inputs (1st in batch):  [ 1.     -1.      1.      1.      1.     -1.      1.      1.      1.\n"," -1.      1.     -1.      1.     -1.      1.      1.      1.      1.\n","  1.     -1.      0.      1.      0.6583  0.      0.      1.      0.    ]  - Outputs(0):  [0.4729 0.5271]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 3, 4, -1] , cc(0): 0\n","Step  80  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.6667  0.      0.      1.      0.    ]  - Outputs(0):  [0.4912 0.5088]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 3, 4, -1] , cc(0): 0\n","Step  81  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    1.    1.    0.675 0.\n"," 0.    0.    1.   ]  - Outputs(0):  [0.4813 0.5187]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 1, 3, 4, -1] , cc(0): 0\n","Step  82  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6833 1.     0.     1.     0.    ]  - Outputs(0):  [0.4794 0.5206]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 1, 3, 4, -1] , cc(0): 0\n","Step  83  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.6917 0.     0.     0.     1.    ]  - Outputs(0):  [0.5033 0.4967]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  84  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.7 0.  0.  0.  1. ]  - Outputs(0):  [0.5216 0.4784]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  85  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7083 0.     0.     1.     0.    ]  - Outputs(0):  [0.531 0.469]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  86  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.7167  0.      0.      1.      0.    ]  - Outputs(0):  [0.5196 0.4804]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  87  Inputs (1st in batch):  [-1.     1.    -1.     1.     1.    -1.    -1.     1.    -1.    -1.\n"," -1.     1.     1.     1.    -1.     1.     1.    -1.    -1.    -1.\n","  0.     1.     0.725  0.     0.     1.     0.   ]  - Outputs(0):  [0.5023 0.4977]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  88  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7333 0.     0.     0.     1.    ]  - Outputs(0):  [0.4623 0.5377]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  89  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.7417 0.     0.     0.     1.    ]  - Outputs(0):  [0.4769 0.5231]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  90  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.75 1.   0.   1.   0.  ]  - Outputs(0):  [0.4709 0.5291]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 3, 2, -1, 4, -1] , cc(0): 0\n","Step  91  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7583 0.     0.     0.     1.    ]  - Outputs(0):  [0.4942 0.5058]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 0, -1, 1, 4, -1] , cc(0): 0\n","Step  92  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7667 0.     0.     0.     1.    ]  - Outputs(0):  [0.4882 0.5118]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 0, -1, 1, 4, -1] , cc(0): 0\n","Step  93  Inputs (1st in batch):  [ 1.     1.     1.    -1.     1.     1.    -1.    -1.     1.    -1.\n","  1.    -1.    -1.     1.    -1.     1.    -1.    -1.     1.    -1.\n","  0.     1.     0.775  0.     0.     0.     1.   ]  - Outputs(0):  [0.5024 0.4976]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 2 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 0, -1, 1, 4, -1] , cc(0): 0\n","Step  94  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.7833 0.     0.     1.     0.    ]  - Outputs(0):  [0.5015 0.4985]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 3 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 0, -1, 1, 4, -1] , cc(0): 0\n","Step  95  Inputs (1st in batch):  [ 1.     -1.      1.      1.      1.     -1.      1.      1.      1.\n"," -1.      1.     -1.      1.     -1.      1.      1.      1.      1.\n","  1.     -1.      0.      1.      0.7917  0.      0.      1.      0.    ]  - Outputs(0):  [0.5017 0.4983]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 0, -1, 1, 4, -1] , cc(0): 0\n","Step  96  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  1.  1.  0.8 0.  0.  1.  0. ]  - Outputs(0):  [0.4823 0.5177]  - action chosen(0):  0 TrialLen(0): 7 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, 0, -1, 1, 4, -1] , cc(0): 0\n","Step  97  Inputs (1st in batch):  [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      0.      0.      0.      0.      0.      0.\n","  0.      0.      0.      1.      0.8083 -1.      0.      1.      0.    ]  - Outputs(0):  [0.463 0.537]  - action chosen(0):  1 TrialLen(0): 7 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  -1.0 , cues(0): [-1, -1, 0, -1, 1, 4, -1] , cc(0): 0\n","Step  98  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8167 0.     0.     0.     1.    ]  - Outputs(0):  [0.4706 0.5294]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  99  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.825 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.4613 0.5387]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  100  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8333 0.     0.     0.     1.    ]  - Outputs(0):  [0.481 0.519]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  101  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8417 0.     0.     0.     1.    ]  - Outputs(0):  [0.4803 0.5197]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  102  Inputs (1st in batch):  [ 1.   -1.   -1.   -1.   -1.    1.   -1.   -1.   -1.   -1.   -1.    1.\n","  1.    1.    1.   -1.   -1.    1.    1.    1.    0.    1.    0.85  0.\n","  0.    1.    0.  ]  - Outputs(0):  [0.5026 0.4974]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  103  Inputs (1st in batch):  [ 1.     -1.      1.      1.      1.     -1.      1.      1.      1.\n"," -1.      1.     -1.      1.     -1.      1.      1.      1.      1.\n","  1.     -1.      0.      1.      0.8583  0.      0.      1.      0.    ]  - Outputs(0):  [0.5199 0.4801]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  104  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.8667 0.     0.     1.     0.    ]  - Outputs(0):  [0.4836 0.5164]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  105  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.875 1.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.4788 0.5212]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  0.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, -1, 3, 1, 4, -1] , cc(0): 0\n","Step  106  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8833 0.     0.     0.     1.    ]  - Outputs(0):  [0.4711 0.5289]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  107  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.8917 0.     0.     0.     1.    ]  - Outputs(0):  [0.4744 0.5256]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  108  Inputs (1st in batch):  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," 0.  0.  0.  1.  0.9 0.  0.  1.  0. ]  - Outputs(0):  [0.4837 0.5163]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  109  Inputs (1st in batch):  [-1.      1.     -1.      1.      1.     -1.     -1.      1.     -1.\n"," -1.     -1.      1.      1.      1.     -1.      1.      1.     -1.\n"," -1.     -1.      0.      1.      0.9083  0.      0.      0.      1.    ]  - Outputs(0):  [0.5005 0.4995]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  110  Inputs (1st in batch):  [ 1.      1.      1.     -1.      1.      1.     -1.     -1.      1.\n"," -1.      1.     -1.     -1.      1.     -1.      1.     -1.     -1.\n","  1.     -1.      0.      1.      0.9167  0.      0.      0.      1.    ]  - Outputs(0):  [0.5113 0.4887]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  111  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.925 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.546 0.454]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  112  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 1.     1.     0.9333 0.     0.     1.     0.    ]  - Outputs(0):  [0.5143 0.4857]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 6 TTHCC(0):  1.0  -Reward (previous step):  0.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  113  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9417 1.     0.     0.     1.    ]  - Outputs(0):  [0.5015 0.4985]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 7 TTHCC(0):  1.0  -Reward (previous step):  1.0 , cues(0): [-1, -1, -1, 2, 0, -1, 4, -1] , cc(0): 0\n","Step  114  Inputs (1st in batch):  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n"," 0.   0.   0.   0.   0.   0.   0.   1.   0.95 0.   0.   0.   1.  ]  - Outputs(0):  [0.4803 0.5197]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 0 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, -1, 1, 4, -1] , cc(0): 0\n","Step  115  Inputs (1st in batch):  [ 1.     -1.     -1.     -1.     -1.      1.     -1.     -1.     -1.\n"," -1.     -1.      1.      1.      1.      1.     -1.     -1.      1.\n","  1.      1.      0.      1.      0.9583  0.      0.      1.      0.    ]  - Outputs(0):  [0.4889 0.5111]  - action chosen(0):  1 TrialLen(0): 8 trialstep(0): 1 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, -1, 1, 4, -1] , cc(0): 0\n","Step  116  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9667 0.     0.     0.     1.    ]  - Outputs(0):  [0.4845 0.5155]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 2 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, -1, 1, 4, -1] , cc(0): 0\n","Step  117  Inputs (1st in batch):  [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n"," 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.975 0.\n"," 0.    1.    0.   ]  - Outputs(0):  [0.4739 0.5261]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 3 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, -1, 1, 4, -1] , cc(0): 0\n","Step  118  Inputs (1st in batch):  [0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n"," 0.     1.     0.9833 0.     0.     1.     0.    ]  - Outputs(0):  [0.4874 0.5126]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 4 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, -1, 1, 4, -1] , cc(0): 0\n","Step  119  Inputs (1st in batch):  [ 1.     -1.      1.      1.      1.     -1.      1.      1.      1.\n"," -1.      1.     -1.      1.     -1.      1.      1.      1.      1.\n","  1.     -1.      0.      1.      0.9917  0.      0.      1.      0.    ]  - Outputs(0):  [0.5157 0.4843]  - action chosen(0):  0 TrialLen(0): 8 trialstep(0): 5 TTHCC(0):  0.0  -Reward (previous step):  0.0 , cues(0): [-1, 3, -1, -1, -1, 1, 4, -1] , cc(0): 0\n","lossv:  93.84724426269531\n","Total reward for this episode(0): 7.0 Prop. of trials w/ rewarded cue: 0.4997998528077321\n","Nb trials for this episode(0): 17.0 [2]: 17.0  Total Nb of trials: 15163840\n","59999 ====\n","Mean loss:  0.1308843811475381\n","Mean reward:  -0.0013999999999999954\n","Time spent on last 10000 iters:  4151.9349138736725\n","ETA:  [0.1]  etaet:  [0.3694]  mean-abs pw:  0.6285209\n","Saved to 'saved/CheckpointRM.pt' at episode 59999\n","\n","Saved!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SfyHw04nb1M-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}