{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SM_theirs.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"13f9c49584c14c969dc780463ccb1a33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7d967c5e657740cc8d5df30f8a181ff2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7647ece2099f4f4db4ffeb16d14a020d","IPY_MODEL_3517cca42ed348bbbd17185ecd246bb7"]}},"7d967c5e657740cc8d5df30f8a181ff2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7647ece2099f4f4db4ffeb16d14a020d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3b6197063cbb477390f08f0d816f7058","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b86bd45939ab43f4838208a4da41a99b"}},"3517cca42ed348bbbd17185ecd246bb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1b7d17d1e6094b17b7f3d7949492ebd9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/0 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_525ce038b2134c809e46994369a83424"}},"3b6197063cbb477390f08f0d816f7058":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b86bd45939ab43f4838208a4da41a99b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b7d17d1e6094b17b7f3d7949492ebd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"525ce038b2134c809e46994369a83424":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LqiEYIQuMxMo"},"source":["# Cue Reward Association \n","\n","In this simple meta-learning task, one of four input cues is arbitrarily chosen as a *target cue*. The agent is repeatedly shown two random cues in succession, and then a *response cue* during which the agent must respond with a 1 if the target was part of the pair, or 0 otherwise. \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2EP3PdRlM2v9"},"source":["**Google Drive Set-up:**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GH-TD0QVWvsY","outputId":"9fef535a-2173-4870-adf7-32aebb8552a8","executionInfo":{"status":"ok","timestamp":1591692044871,"user_tz":-330,"elapsed":30680,"user":{"displayName":"Rudraksh Kapil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoFYYIzudjawgt9sydxKemiYUjrtPu0UKhIh9F=s64","userId":"06594730916062239985"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# project folder, e.g. 'project submissions/A09_A10_A54'\n","# $$ for us - 'DSc Term Project/'\n","FOLDERNAME = 'DSc Term Project/Task 1'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# test - this notebooks name should show up:\n","# is oserror - restart runtime\n","%cd /content/drive/My\\ Drive/$FOLDERNAME\n","%ls "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1khPuOHY9jnXOARox2UYYUJ3TRirri2cS/DSc Term Project/Task 1\n","'Cue Reward Association.ipynb'  'Plot Test.ipynb'   SM.ipynb            \u001b[0m\u001b[01;34mutils\u001b[0m/\n"," NM.ipynb                        RM.ipynb           SM_rudraksh.ipynb\n"," NP.ipynb                        \u001b[01;34msaved\u001b[0m/             SM_theirs.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QW-zioHpNhqF"},"source":["**Import Statements:**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lYMYbf6BNuJo","colab":{}},"source":["import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch import optim\n","from torch.optim import lr_scheduler\n","\n","import numpy as np\n","import time\n","import pickle\n","import random\n","from tqdm.autonotebook import tqdm\n","#from graphics import *\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (20.0, 16.0) # set default size of plots\n","\n","\n","\n","np.set_printoptions(precision=4)\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZXE1D9PkN-E_"},"source":["**GPU Set-up:**\n","\n","You have an option to use GPU by setting the flag to `True` below.\n","\n","The global variables `dtype` and `device` will control the data types throughout this notebook. \n","\n","You need to manually switch to a GPU device. You can do this by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. Note that you have to rerun the cells from the top since the kernel gets restarted upon switching runtimes."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CH6wFaQaOB__","outputId":"47cf575b-1beb-4b70-ab2b-a63911432407","executionInfo":{"status":"ok","timestamp":1591692064447,"user_tz":-330,"elapsed":3941,"user":{"displayName":"Rudraksh Kapil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoFYYIzudjawgt9sydxKemiYUjrtPu0UKhIh9F=s64","userId":"06594730916062239985"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# flag\n","USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    dtype = torch.float32\n","else:\n","    device = torch.device('cpu')\n","    dtype = torch.float32\n","\n","print('Using Device:', device)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using Device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2pwUH5WHZKhd","colab_type":"code","colab":{}},"source":["import pdb\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np\n","import torch.nn.functional as F\n","\n","\n","\n","\n","##ttype = torch.FloatTensor;\n","#ttype = torch.cuda.FloatTensor;\n","\n","#ttype = torch.FloatTensor;\n","#ttype = torch.cuda.FloatTensor;\n","\n","\n","\n","class NonPlasticRNN(nn.Module):\n","    def __init__(self, params):\n","        super(NonPlasticRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","\n","    def forward(self, inputs, hidden): #, hebb):\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","\n","        # Here, the *rows* of w and hebb are the inputs weights to a single neuron\n","        # hidden = x, hactiv = y\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul(self.w,\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        #hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, hebb)),\n","        #                hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","\n","        hidden = hactiv\n","\n","        return activout, valueout, hidden #, hebb\n","\n","\n","    def initialZeroState(self):\n","        BATCHSIZE = self.params['bs']\n","        return Variable(torch.zeros(BATCHSIZE, self.params['hs']), requires_grad=False )\n","\n","\n","\n","\n","\n","class PlasticRNN(nn.Module):\n","    def __init__(self, params):\n","        super(PlasticRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.alpha =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.eta = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same eta\n","        #self.h2DA = torch.nn.Linear(params['hs'], NBDA)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","    def forward(self, inputs, hidden, hebb):\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","\n","        # Here, the *rows* of w and hebb are the inputs weights to a single neuron\n","        # hidden = x, hactiv = y\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, hebb)),\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","\n","        # Now computing the Hebbian updates...\n","\n","        # deltahebb has shape BS x HS x HS\n","        # Each row of hebb contain the input weights to a neuron\n","        deltahebb =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product...should it be other way round?\n","        hebb = torch.clamp(hebb + self.eta * deltahebb, min=-1.0, max=1.0)\n","\n","        hidden = hactiv\n","\n","        return activout, valueout, hidden, hebb\n","\n","    def initialZeroHebb(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","\n","    def initialZeroState(self):\n","        BATCHSIZE = self.params['bs']\n","        return Variable(torch.zeros(BATCHSIZE, self.params['hs']), requires_grad=False )\n","\n","\n","\n","\n","class SimpleModulRNN(nn.Module):\n","    def __init__(self, params):\n","        super(SimpleModulRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.alpha =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.eta = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same eta (only for the non-modulated part, if any!)\n","        self.h2DA = torch.nn.Linear(params['hs'], NBDA)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","    def forward_test(self, inputs, hidden, hebb):\n","        NBDA = 1\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul(self.w,\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","        return activout, valueout, 0, hidden, hebb\n","\n","    def forward(self, inputs, hidden, hebb):\n","        NBDA = 1\n","        BATCHSIZE = self.params['bs']\n","        HS = self.params['hs']\n","\n","        # Here, the *rows* of w and hebb are the inputs weights to a single neuron\n","        # hidden = x, hactiv = y\n","        hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, hebb)),\n","                        hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","        activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed by the calling program\n","        valueout = self.h2v(hactiv)\n","\n","        # Now computing the Hebbian updates...\n","\n","        # With batching, DAout is a matrix of size BS x 1 (Really BS x NBDA, but we assume NBDA=1 for now in the deltahebb multiplication below)\n","        if self.params['da'] == 'tanh':\n","            DAout = F.tanh(self.h2DA(hactiv))\n","        elif self.params['da'] == 'sig':\n","            DAout = F.sigmoid(self.h2DA(hactiv))\n","        elif self.params['da'] == 'lin':\n","            DAout =  self.h2DA(hactiv)\n","        else:\n","            raise ValueError(\"Which transformation for DAout ?\")\n","\n","        # deltahebb has shape BS x HS x HS\n","        # Each row of hebb contain the input weights to a neuron\n","        deltahebb =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product...should it be other way round?\n","\n","\n","        hebb1 = torch.clamp(hebb + DAout.view(BATCHSIZE, 1, 1) * deltahebb, min=-1.0, max=1.0)\n","        if self.params['fm'] == 0:\n","            # Non-modulated part\n","            hebb2 = torch.clamp(hebb + self.eta * deltahebb, min=-1.0, max=1.0)\n","        # Soft Clamp (note that it's different from just putting a tanh on top of a freely varying value):\n","        #hebb1 = torch.clamp( hebb +  torch.clamp(DAout.view(BATCHSIZE, 1, 1) * deltahebb, min=0.0) * (1 - hebb) +\n","        #        torch.clamp(DAout.view(BATCHSIZE, 1, 1)  * deltahebb, max=0.0) * (hebb + 1) , min=-1.0, max=1.0)\n","        #hebb2 = torch.clamp( hebb +  torch.clamp(self.eta * deltahebb, min=0.0) * (1 - hebb) +  torch.clamp(self.eta * deltahebb, max=0.0) * (hebb + 1) , min=-1.0, max=1.0)\n","        # Purely additive, no clamping. This will almost certainly diverge, don't use it!\n","        #hebb1 = hebb + DAout.view(BATCHSIZE, 1, 1) * deltahebb\n","        #hebb2 = hebb + self.eta * deltahebb\n","\n","        if self.params['fm'] == 1:\n","            hebb = hebb1\n","        elif self.params['fm'] == 0:\n","            # Combine the modulated and non-modulated part\n","            hebb = torch.cat( (hebb1[:, :self.params['hs']//2, :], hebb2[:,  self.params['hs'] // 2:, :]), dim=1) # Maybe along dim=2 instead?...\n","        else:\n","            raise ValueError(\"Must select whether fully modulated or not (params['fm'])\")\n","\n","        hidden = hactiv\n","\n","        return activout, valueout, DAout, hidden, hebb\n","\n","    def initialZeroHebb(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","\n","    def initialZeroState(self):\n","        BATCHSIZE = self.params['bs']\n","        return Variable(torch.zeros(BATCHSIZE, self.params['hs']), requires_grad=False )\n","\n","\n","\n","\n","\n","class RetroModulRNN(nn.Module):\n","    def __init__(self, params):\n","        super(RetroModulRNN, self).__init__()\n","        # NOTE: 'outputsize' excludes the value and neuromodulator outputs!\n","        for paramname in ['outputsize', 'inputsize', 'hs', 'bs', 'fm']:\n","            if paramname not in params.keys():\n","                raise KeyError(\"Must provide missing key in argument 'params': \"+paramname)\n","        NBDA = 1 # For now we limit the number of neuromodulatory-output neurons to 1\n","        # Doesn't work with our version of PyTorch:\n","        #self.device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","        self.params = params\n","        self.activ = F.tanh\n","        self.i2h = torch.nn.Linear(self.params['inputsize'], params['hs'])\n","        self.w =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.alpha =  torch.nn.Parameter((.01 * torch.t(torch.rand(params['hs'], params['hs']))), requires_grad=True)\n","        self.eta = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same eta (only for the non-modulated part, if any!)\n","        self.etaet = torch.nn.Parameter((.1 * torch.ones(1)), requires_grad=True)  # Everyone has the same etaet\n","        self.h2DA = torch.nn.Linear(params['hs'], NBDA)\n","        self.h2o = torch.nn.Linear(params['hs'], self.params['outputsize'])\n","        self.h2v = torch.nn.Linear(params['hs'], 1)\n","\n","    def forward(self, inputs, hidden, hebb, et, pw):\n","            NBDA = 1\n","            BATCHSIZE = self.params['bs']\n","            HS = self.params['hs']\n","\n","            hactiv = self.activ(self.i2h(inputs).view(BATCHSIZE, HS, 1) + torch.matmul((self.w + torch.mul(self.alpha, pw)),\n","                            hidden.view(BATCHSIZE, HS, 1))).view(BATCHSIZE, HS)\n","            activout = self.h2o(hactiv)  # Pure linear, raw scores - will be softmaxed later\n","            valueout = self.h2v(hactiv)\n","\n","            # Now computing the Hebbian updates...\n","\n","            # With batching, DAout is a matrix of size BS x 1 (Really BS x NBDA, but we assume NBDA=1 for now in the deltahebb multiplication below)\n","            if self.params['da'] == 'tanh':\n","                DAout = F.tanh(self.h2DA(hactiv))\n","            elif self.params['da'] == 'sig':\n","                DAout = F.sigmoid(self.h2DA(hactiv))\n","            elif self.params['da'] == 'lin':\n","                DAout =  self.h2DA(hactiv)\n","            else:\n","                raise ValueError(\"Which transformation for DAout ?\")\n","\n","            if self.params['rule'] == 'hebb':\n","                deltahebb =  torch.bmm(hactiv.view(BATCHSIZE, HS, 1), hidden.view(BATCHSIZE, 1, HS)) # batched outer product...should it be other way round?\n","            elif self.params['rule'] == 'oja':\n","                deltahebb =  torch.mul(hactiv.view(BATCHSIZE, HS, 1), (hidden.view(BATCHSIZE, 1, HS) - torch.mul(self.w.view(1, HS, HS), hactiv.view(BATCHSIZE, HS, 1))))\n","            else:\n","                raise ValueError(\"Must specify learning rule ('hebb' or 'oja')\")\n","\n","            # Hard clamp\n","            deltapw = DAout.view(BATCHSIZE,1,1) * et\n","            pw1 = torch.clamp(pw + deltapw, min=-1.0, max=1.0)\n","\n","            # Should we have a fully neuromodulated network, or only half?\n","            if self.params['fm'] == 1:\n","                pw = pw1\n","            elif self.params['fm']==0:\n","                hebb = torch.clamp(hebb + self.eta * deltahebb, min=-1.0, max=1.0)\n","                pw = torch.cat( (hebb[:, :self.params['hs']//2, :], pw1[:,  self.params['hs'] // 2:, :]), dim=1) # Maybe along dim=2 instead?...\n","            else:\n","                raise ValueError(\"Must select whether fully modulated or not\")\n","\n","            # Updating the eligibility trace - always a simple decay term.\n","            # Note that self.etaet != self.eta (which is used for hebb, i.e. the non-modulated part)\n","            deltaet = deltahebb\n","            et = (1 - self.etaet) * et + self.etaet *  deltaet\n","\n","            hidden = hactiv\n","            return activout, valueout, DAout, hidden, hebb, et, pw\n","\n","\n","\n","\n","    def initialZeroHebb(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","\n","    def initialZeroPlasticWeights(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs'], self.params['hs']) , requires_grad=False)\n","    def initialZeroState(self):\n","        return Variable(torch.zeros(self.params['bs'], self.params['hs']), requires_grad=False )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0RCXg3OZCXr","colab_type":"code","colab":{}},"source":["# Stimulus-response task as described in Miconi et al. ICLR 2019.\n","\n","# Copyright (c) 2018-2019 Uber Technologies, Inc.\n","#\n","# Licensed under the Uber Non-Commercial License (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at the root directory of this project.\n","\n","import argparse\n","import pdb\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np\n","from numpy import random\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.optim import lr_scheduler\n","import random\n","import sys\n","import pickle\n","import time\n","import os\n","import platform\n","#import makemaze\n","\n","import numpy as np\n","#import matplotlib.pyplot as plt\n","import glob\n","\n","\n","\n","\n","\n","np.set_printoptions(precision=4)\n","\n","\n","\n","ADDINPUT = 4 # 1 inputs for the previous reward, 1 inputs for numstep, 1 unused,  1 \"Bias\" inputs\n","\n","\n","def train(paramdict):\n","    #params = dict(click.get_current_context().params)\n","\n","    #params['inputsize'] =  RFSIZE * RFSIZE + ADDINPUT + NBNONRESTACTIONS\n","    print(\"Starting training...\")\n","    params = {}\n","    #params.update(defaultParams)\n","    params.update(paramdict)\n","    print(\"Passed params: \", params)\n","    print(platform.uname())\n","    #params['nbsteps'] = params['nbshots'] * ((params['prestime'] + params['interpresdelay']) * params['nbclasses']) + params['prestimetest']  # Total number of steps per episode\n","    suffix = \"SRB_\"+\"\".join([str(x)+\"_\" if pair[0] != 'pe' and pair[0] != 'nbsteps' and pair[0] != 'rngseed' and pair[0] != 'save_every' and pair[0] != 'test_every' else '' for pair in sorted(zip(params.keys(), params.values()), key=lambda x:x[0] ) for x in pair])[:-1] + \"_rngseed_\" + str(params['rngseed'])   # Turning the parameters into a nice suffix for filenames\n","    print(suffix)\n","\n","    #NBINPUTBITS = params['ni'] + 1\n","    NBINPUTBITS = params['cs'] + 1 # The additional bit is for the response cue (i.e. the \"Go\" cue)\n","    params['outputsize'] =  2  # \"response\" and \"no response\"\n","    params['inputsize'] = NBINPUTBITS +  params['outputsize'] + ADDINPUT  # The total number of input bits is the size of inputs, plus the \"response cue\" input, plus the number of actions, plus the number of additional inputs\n","\n","    # This doesn't work with our version of PyTorch\n","    #params['device'] = 'gpu'\n","    #device = torch.device(\"cuda:0\" if self.params['device'] == 'gpu' else \"cpu\")\n","    BS = params['bs']\n","\n","    # Initialize random seeds (first two redundant?)\n","    print(\"Setting random seeds\")\n","    np.random.seed(params['rngseed']); random.seed(params['rngseed']); torch.manual_seed(params['rngseed'])\n","\n","    print(\"Initializing network\")\n","    if params['type'] == 'modul':\n","        net = RetroModulRNN(params)\n","    elif params['type'] == 'modplast':\n","        net = SimpleModulRNN(params)\n","    elif params['type'] == 'plastic':\n","        net = PlasticRNN(params)\n","    elif params['type'] == 'rnn':\n","        net = NonPlasticRNN(params)\n","    net.to(device)\n","\n","    print (\"Shape of all optimized parameters:\", [x.size() for x in net.parameters()])\n","    allsizes = [torch.numel(x.data.cpu()) for x in net.parameters()]\n","    print (\"Size (numel) of all optimized elements:\", allsizes)\n","    print (\"Total size (numel) of all optimized elements:\", sum(allsizes))\n","\n","    #total_loss = 0.0\n","    print(\"Initializing optimizer\")\n","    #optimizer = torch.optim.SGD(net.parameters(), lr=1.0*params['lr'], weight_decay=params['l2'])\n","    #optimizer = torch.optim.RMSprop(net.parameters(), lr=1.0*params['lr'], weight_decay=params['l2'])\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1.0*params['lr'], eps=params['eps'], weight_decay=params['l2'])\n","    #optimizer = torch.optim.Adam(net.parameters(), lr=1.0*params['lr'], eps=1e-4, weight_decay=params['l2'])\n","    #optimizer = torch.optim.SGD(net.parameters(), lr=1.0*params['lr'])\n","    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=params['gamma'], step_size=params['steplr'])\n","\n","    #LABSIZE = params['lsize']\n","    #lab = np.ones((LABSIZE, LABSIZE))\n","    #CTR = LABSIZE // 2\n","\n","    # Simple cross maze\n","    #lab[CTR, 1:LABSIZE-1] = 0\n","    #lab[1:LABSIZE-1, CTR] = 0\n","\n","\n","    # Double-T maze\n","    #lab[CTR, 1:LABSIZE-1] = 0\n","    #lab[1:LABSIZE-1, 1] = 0\n","    #lab[1:LABSIZE-1, LABSIZE - 2] = 0\n","\n","    # Grid maze\n","    #lab[1:LABSIZE-1, 1:LABSIZE-1].fill(0)\n","    #for row in range(1, LABSIZE - 1):\n","    #    for col in range(1, LABSIZE - 1):\n","    #        if row % 2 == 0 and col % 2 == 0:\n","    #            lab[row, col] = 1\n","    #lab[CTR,CTR] = 0 # Not strictly necessary, but perhaps helps loclization by introducing a detectable irregularity in the center\n","\n","\n","    #LABSIZE = params['msize']\n","    #lab = np.ones((LABSIZE, LABSIZE))\n","    #CTR = LABSIZE // 2\n","\n","\n","    ## Grid maze\n","    #lab[1:LABSIZE-1, 1:LABSIZE-1].fill(0)\n","    #for row in range(1, LABSIZE - 1):\n","    #    for col in range(1, LABSIZE - 1):\n","    #        if row % 2 == 0 and col % 2 == 0:\n","    #            lab[row, col] = 1\n","    #lab[CTR,CTR] = 0 # Not strictly necessary, but perhaps helps loclization by introducing a detectable irregularity in the center\n","\n","    if params['saved_already'] == False:\n","        # create dictionary to keep track of stats over all episodes\n","        all_losses_objective = []\n","        all_total_rewards = []\n","        epoch=0\n","    else:\n","        checkpoint_path = 'saved/Checkpoint' + params['net_type'] + '.pt'\n","        checkpoint = torch.load(checkpoint_path)\n","        print('Loading states 1')\n","        print(net.load_state_dict(checkpoint['state_dict']))\n","        print('Loading states 2')\n","        print(optimizer.load_state_dict(checkpoint['optimizer']))\n","        epoch = checkpoint['iters'] + 1\n","        stats=checkpoint['stats_dict'] \n","        all_total_rewards = stats['total_rewards']\n","        all_losses_objective = stats['losses_objectives']\n","        print(f\"Loaded from '{checkpoint_path}' at episode {epoch}\")\n","\n","    all_losses = []\n","    all_grad_norms = []\n","    all_losses_v = []\n","    lossbetweensaves = 0\n","    nowtime = time.time()\n","    #meanreward = np.zeros((LABSIZE, LABSIZE))\n","    meanreward = np.zeros(params['ni'])\n","    meanrewardT = np.zeros((params['ni'], params['eplen']))\n","\n","    nbtrials = [0]*BS\n","    totalnbtrials = 0\n","    nbtrialswithcc = 0\n","\n","\n","    print(\"Starting episodes!\")\n","\n","    for numepisode in tqdm(range(epoch, params['nbiter'])):\n","\n","        PRINTTRACE = 0\n","        #if (numepisode+1) % (1 + params['pe']) == 0:\n","        if (numepisode+1) % (params['pe']) == 0:\n","            PRINTTRACE = 1\n","\n","        #lab = makemaze.genmaze(size=LABSIZE, nblines=4)\n","        #count = np.zeros((LABSIZE, LABSIZE))\n","\n","        # # Select the reward location for this episode - not on a wall!\n","        # rposr = 0; rposc = 0\n","        # while lab[rposr, rposc] == 1:\n","        #     rposr = np.random.randint(1, LABSIZE - 1)\n","        #     rposc = np.random.randint(1, LABSIZE - 1)\n","\n","        # # We always start the episode from the center (when hitting reward, we may teleport either to center or to a random location depending on params['rsp'])\n","        # posc = CTR\n","        # posr = CTR\n","\n","\n","\n","\n","        optimizer.zero_grad()\n","        loss = 0\n","        lossv = 0\n","        hidden = net.initialZeroState().to(device)\n","        if params['type'] != 'rnn':\n","            hebb = net.initialZeroHebb().to(device)\n","        if params['type'] == 'modul':\n","            et = net.initialZeroHebb().to(device) # Eligibility Trace is identical to Hebbian Trace in shape\n","            pw = net.initialZeroPlasticWeights().to(device)\n","        numactionchosen = 0\n","\n","\n","        # Generate the cues. Make sure they're all different (important when using very small cues for debugging, e.g. cs=2, ni=2)\n","        cuedata=[]\n","        for nb in range(BS):\n","            cuedata.append([])\n","            for ncue in range(params['ni']):\n","                assert len(cuedata[nb]) == ncue\n","                foundsame = 1\n","                cpt = 0\n","                while foundsame > 0 :\n","                    cpt += 1\n","                    if cpt > 10000:\n","                        # This should only occur with very weird parameters, e.g. cs=2, ni>4\n","                        raise ValueError(\"Could not generate a full list of different cues\")\n","                    foundsame = 0\n","                    candidate = np.random.randint(2, size=params['cs']) * 2 - 1\n","                    for backtrace in range(ncue):\n","                        if np.array_equal(cuedata[nb][backtrace], candidate):\n","                            foundsame = 1\n","\n","                cuedata[nb].append(candidate)\n","\n","\n","        reward = np.zeros(BS)\n","        sumreward = np.zeros(BS)\n","        rewards = []\n","        vs = []\n","        logprobs = []\n","        cues=[]\n","        for nb in range(BS):\n","            cues.append([])\n","        dist = 0\n","        numactionschosen = np.zeros(BS, dtype='int32')\n","\n","        #reward = 0.0\n","        #rewards = []\n","        #vs = []\n","        #logprobs = []\n","        #sumreward = 0.0\n","        nbtrials = np.zeros(BS)\n","        nbrewardabletrials = np.zeros(BS)\n","        thistrialhascorrectcue = np.zeros(BS)\n","        triallength = np.zeros(BS, dtype='int32')\n","        correctcue = np.random.randint(params['ni'], size=BS)\n","        trialstep = np.zeros(BS, dtype='int32')\n","\n","        #print(\"EPISODE \", numepisode)\n","        for numstep in range(params['eplen']):\n","\n","            #if params['clamp'] == 0:\n","            inputs = np.zeros((BS, params['inputsize']), dtype='float32')\n","            #else:\n","            #    inputs = np.zeros((1, params['hs']), dtype='float32')\n","\n","            for nb in range(BS):\n","\n","                if trialstep[nb] == 0:\n","                    thistrialhascorrectcue[nb] = 0\n","                    # Trial length is randomly modulated for each trial; first time step always -1 (i.e. no input cue), last time step always response-cue (i.e. NBINPUTBITS-1).\n","                    #triallength = params['ni'] // 2  + 3 + np.random.randint(1 + params['ni'])  # 3 fixed-cue time steps (1st, last and next-to-last) + some random nb of no-cue time steps\n","                    triallength[nb] = params['ni'] // 2  + 3 + np.random.randint(params['ni'])  # 3 fixed-cue time steps (1st, last and next-to-last) + some random nb of no-cue time steps\n","\n","\n","\n","                    # In any trial, we only show half the cues (randomly chosen), once each:\n","                    mycues = [x for x in range(params['ni'])]\n","                    random.shuffle(mycues); mycues = mycues[:len(mycues) // 2]\n","                    # The rest is filled with no-input time steps (i.e. cue = -1), but also with the 3 fixed-cue steps (1st, last, next-to-last)\n","                    for nc in range(triallength[nb] - 3  - len(mycues)):\n","                        mycues.append(-1)\n","                    random.shuffle(mycues)\n","                    mycues.insert(0, -1); mycues.append(params['ni']); mycues.append(-1)  # The first and last time step have no input (cue -1), the next-to-last has the response cue.\n","                    assert(len(mycues) == triallength[nb])\n","                    cues[nb] = mycues\n","\n","\n","                inputs[nb, :NBINPUTBITS] = 0\n","                if cues[nb][trialstep[nb]] > -1 and cues[nb][trialstep[nb]] < params['ni']:\n","                    #inputs[0, cues[trialstep]] = 1.0\n","                    inputs[nb, :NBINPUTBITS-1] = cuedata[nb][cues[nb][trialstep[nb]]][:]\n","                    if cues[nb][trialstep[nb]] == correctcue[nb]:\n","                        thistrialhascorrectcue[nb] = 1\n","                if cues[nb][trialstep[nb]] == params['ni']:\n","                    inputs[nb, NBINPUTBITS-1] = 1  # \"Go\" cue\n","\n","\n","                inputs[nb, NBINPUTBITS + 0] = 1.0 # Bias neuron, probably not necessary\n","                inputs[nb,NBINPUTBITS +  1] = numstep / params['eplen']\n","                inputs[nb, NBINPUTBITS + 2] = 1.0 * reward[nb] # Reward from previous time step\n","                if numstep > 0:\n","                    inputs[nb, NBINPUTBITS + ADDINPUT + numactionschosen[nb]] = 1  # Previously chosen action\n","\n","            inputsC = torch.from_numpy(inputs).to(device)\n","            # Might be better:\n","            #if rposr == posr and rposc = posc:\n","            #    inputs[0][-4] = 100.0\n","            #else:\n","            #    inputs[0][-4] = 0\n","\n","            # Running the network\n","\n","            ## Running the network\n","            if params['type'] == 'modplast':\n","                y, v, DAout, hidden, hebb = net(Variable(inputsC, requires_grad=False).to(device), hidden, hebb)  # y  should output raw scores, not probas\n","            elif params['type'] == 'modul':\n","                y, v, DAout, hidden, hebb, et, pw  = net(Variable(inputsC, requires_grad=False).to(device), hidden, hebb, et, pw)  # y  should output raw scores, not probas\n","            elif params['type'] == 'plastic':\n","                y, v, hidden, hebb = net(Variable(inputsC, requires_grad=False).to(device), hidden, hebb)  # y  should output raw scores, not probas\n","            elif params['type'] == 'rnn':\n","                y, v, hidden = net(Variable(inputsC, requires_grad=False).to(device), hidden)  # y  should output raw scores, not probas\n","            else:\n","                raise ValueError(\"Network type unknown or not yet implemented!\")\n","\n","\n","\n","            y = F.softmax(y, dim=1)\n","            # Must convert y to probas to use this !\n","            distrib = torch.distributions.Categorical(y)\n","            actionschosen = distrib.sample()\n","            logprobs.append(distrib.log_prob(actionschosen))\n","            numactionschosen = actionschosen.data.cpu().numpy()    # Turn to scalar\n","\n","            if PRINTTRACE:\n","                print(\"Step \", numstep, \" Inputs (1st in batch): \", inputs[0,:params['inputsize']], \" - Outputs(0): \", y.data.cpu().numpy()[0,:], \" - action chosen(0): \", numactionschosen[0],\n","                        \"TrialLen(0):\", triallength[0], \"trialstep(0):\", trialstep[0], \"TTHCC(0): \", thistrialhascorrectcue[0], \" -Reward (previous step): \", reward[0], \", cues(0):\", cues[0], \", cc(0):\", correctcue[0])\n","\n","                #print(\"Step \", numstep, \" Inputs: \", inputs[0,:params['inputsize']], \" - Outputs: \", y.data.cpu().numpy(), \" - action chosen: \", numactionchosen,\n","                #        \" - mean abs pw: \", np.mean(np.abs(pw.data.cpu().numpy())), \"TrialLen:\", triallength, \"trialstep:\", trialstep, \"TTHCC: \", thistrialhascorrectcue, \" -Reward (previous step): \", reward, \", cues:\", cues, \", cc:\", correctcue)\n","\n","            reward = np.zeros(BS, dtype='float32')\n","\n","            for nb in range(BS):\n","                if numactionschosen[nb] == 1:\n","                    # Small penalty for any non-rest action taken\n","                    reward[nb]  -= params['wp']\n","\n","\n","            ### DEBUGGING\n","            ## Easiest possible episode-dependent response (i.e. the easiest\n","            ## possible problem that actually require meta-learning, with ni=2)\n","            ## This one works pretty wel... But harder ones don't work well!\n","            #if numactionchosen == correctcue :\n","            #        reward = params['rew']\n","            #else:\n","            #        reward = -params['rew']\n","\n","\n","                trialstep[nb] += 1\n","                if trialstep[nb] == triallength[nb] - 1:\n","                    # This was the next-to-last step of the trial (and we showed the response signal, unless it was the first few steps in episode).\n","                    assert(cues[nb][trialstep[nb] - 1] == params['ni'] or numstep < 2)\n","                    # We must deliver reward (which will be perceived by the agent at the next step), positive or negative, depending on response\n","                    if thistrialhascorrectcue[nb] and numactionschosen[nb] == 1:\n","                        reward[nb] += params['rew']\n","                    elif (not thistrialhascorrectcue[nb]) and numactionschosen[nb] == 0:\n","                        reward[nb] += params['rew']\n","                    else:\n","                        reward[nb] -= params['rew']\n","\n","                    if np.random.rand() < params['pf']:\n","                        reward[nb] = -reward[nb]\n","\n","                if trialstep[nb] == triallength[nb]:\n","                    # This was the last step of the trial (and we showed no input)\n","                    assert(cues[nb][trialstep[nb] - 1] == -1 or numstep < 2)\n","                    nbtrials[nb] += 1\n","                    totalnbtrials += 1\n","                    if thistrialhascorrectcue[nb]:\n","                        nbtrialswithcc += 1\n","                        #nbrewardabletrials += 1\n","                    # Trial is dead, long live trial\n","                    trialstep[nb] = 0\n","\n","                    # We initialize the hidden state between trials!\n","                    #if params['is'] == 1:\n","                    #    hidden = net.initialZeroState()\n","\n","\n","\n","            rewards.append(reward)\n","            vs.append(v)\n","            sumreward += reward\n","\n","\n","\n","            #if params['alg'] in ['A3C' , 'REIE' , 'REIT']:\n","\n","            loss += (params['bent'] * y.pow(2).sum() / BS )   # We want to penalize concentration, i.e. encourage diversity; our version of PyTorch does not have an entropy() function for Distribution, so we use this instead.\n","\n","\n","\n","            ##if PRINTTRACE:\n","            ##    print(\"Probabilities:\", y.data.cpu().numpy(), \"Picked action:\", numactionchosen, \", got reward\", reward)\n","\n","        R = Variable(torch.zeros(BS), requires_grad=False).to(device)\n","        gammaR = params['gr']\n","        for numstepb in reversed(range(params['eplen'])) :\n","            R = gammaR * R + Variable(torch.from_numpy(rewards[numstepb]), requires_grad=False).to(device)\n","            ctrR = R - vs[numstepb][0]\n","            lossv += ctrR.pow(2).sum() / BS\n","            loss -= (logprobs[numstepb] * ctrR.detach()).sum() / BS  # Need to check if detach() is OK\n","            #pdb.set_trace()\n","\n","\n","        # Episode is done, now let's do the actual computations\n","        #gammaR = params['gr']\n","        #if params['alg'] == 'A3C':\n","        #    R = 0\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        R = gammaR * R + rewards[numstepb]\n","        #        lossv += (vs[numstepb][0] - R).pow(2)\n","        #        loss -= logprobs[numstepb] * (R - vs[numstepb].data[0][0])  # Not sure if the \"data\" is needed... put it b/c of worry about weird gradient flows\n","        #    loss += params['bv'] * lossv\n","\n","        #elif params['alg'] in ['REI', 'REIE']:\n","        #    R = sumreward\n","        #    baseline = meanreward[correctcue]\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        loss -= logprobs[numstepb] * (R - baseline)\n","        #elif params['alg'] == 'REIT':\n","        #    R = 0\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        R = gammaR * R + rewards[numstepb]\n","        #        loss -= logprobs[numstepb] * (R - meanrewardT[correctcue, numstepb])\n","        #else:\n","        #    raise ValueError(\"Must select algo type\")\n","        #elif params['alg'] == 'REINOB':\n","        #    R = sumreward\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        loss -= logprobs[numstepb] * R\n","        #elif params['alg'] == 'REITMP':\n","        #    R = 0\n","        #    for numstepb in reversed(range(params['eplen'])) :\n","        #        R = gammaR * R + rewards[numstepb]\n","        #        loss -= logprobs[numstepb] * R\n","\n","        #else:\n","        #    raise ValueError(\"Which algo?\")\n","\n","        #meanreward[correctcue] = (1.0 - params['nu']) * meanreward[correctcue] + params['nu'] * sumreward\n","        ##meanreward[rposr, rposc] = (1.0 - params['nu']) * meanreward[rposr, rposc] + params['nu'] * sumreward\n","        #R = 0\n","        #for numstepb in reversed(range(params['eplen'])) :\n","        #    R = gammaR * R + rewards[numstepb]\n","        #    meanrewardT[correctcue, numstepb] = (1.0 - params['nu']) * meanrewardT[correctcue, numstepb] + params['nu'] * R\n","\n","        loss += params['blossv'] * lossv\n","        loss /= params['eplen']\n","\n","        if PRINTTRACE:\n","            #if params['alg'] == 'A3C':\n","            print(\"lossv: \", float(lossv))\n","            #elif params['alg'] in ['REI', 'REIE', 'REIT']:\n","            #    print(\"meanreward baselines: \", [meanreward[x] for x in range(params['ni'])])\n","            print (\"Total reward for this episode(0):\", sumreward[0], \"Prop. of trials w/ rewarded cue:\", (nbtrialswithcc / totalnbtrials))\n","            print(\"Nb trials for this episode(0):\", nbtrials[0], \"[2]:\",nbtrials[2],\" Total Nb of trials:\", totalnbtrials)\n","\n","        #if params['squash'] == 1:\n","        #    if sumreward < 0:\n","        #        sumreward = -np.sqrt(-sumreward)\n","        #    else:\n","        #        sumreward = np.sqrt(sumreward)\n","        #elif params['squash'] == 0:\n","        #    pass\n","        #else:\n","        #    raise ValueError(\"Incorrect value for squash parameter\")\n","\n","        #loss *= sumreward\n","\n","        #for p in net.parameters():\n","        #    p.grad.data.clamp_(-params['clamp'], params['clamp'])\n","        loss.backward()\n","        all_grad_norms.append(torch.nn.utils.clip_grad_norm(net.parameters(), params['gc']))\n","        #if numepisode > 100:  # Burn-in period for meanreward\n","        optimizer.step()\n","\n","\n","        #print(sumreward)\n","        lossnum = float(loss)\n","        lossbetweensaves += lossnum\n","        all_losses_objective.append(lossnum)\n","        all_total_rewards.append(sumreward.mean())\n","        #all_total_rewards.append(sumreward[0])\n","            #all_losses_v.append(lossv.data[0])\n","        #total_loss  += lossnum\n","\n","\n","        if (numepisode+1) % params['pe'] == 0:\n","\n","            print(numepisode, \"====\")\n","            print(\"Mean loss: \", lossbetweensaves / params['pe'])\n","            lossbetweensaves = 0\n","            print(\"Mean reward: \", np.sum(all_total_rewards[-params['pe']:])/ params['pe'])\n","            previoustime = nowtime\n","            nowtime = time.time()\n","            print(\"Time spent on last\", params['pe'], \"iters: \", nowtime - previoustime)\n","            if params['type'] == 'plastic' or params['type'] == 'lstmplastic':\n","                print(\"ETA: \", float(net.eta), \"alpha[0,1]: \", net.alpha.data.cpu().numpy()[0,1], \"w[0,1]: \", net.w.data.cpu().numpy()[0,1] )\n","            elif params['type'] == 'modul' or params['type'] == 'modul2':\n","                print(\"ETA: \", net.eta.data.cpu().numpy(), \" etaet: \", net.etaet.data.cpu().numpy(), \" mean-abs pw: \", np.mean(np.abs(pw.data.cpu().numpy())))\n","            elif params['type'] == 'rnn':\n","                print(\"w[0,1]: \", net.w.data.cpu().numpy()[0,1] )\n","\n","        if (numepisode+1) % params['save_every'] == 0:\n","            stats = {\n","                'total_rewards':all_total_rewards,\n","                'losses_objectives':all_losses_objective\n","            }\n","            state = {\n","                'iters': numepisode,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'stats_dict': stats\n","            }\n","            savepath='saved/Checkpoint'+params['net_type']+'.pt'\n","            torch.save(state,savepath)\n","            print(f\"Saved to '{savepath}' at episode {numepisode}\")\n","\n","    stats = {\n","        'total_rewards':all_total_rewards,\n","        'losses_objectives':all_losses_objective\n","    }\n","    return {\n","          'iters': params['nbiter'],\n","          'state_dict': net.state_dict(),\n","          'optimizer': optimizer.state_dict(),\n","          'stats_dict': stats\n","    }\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L79-684hZTEQ","colab_type":"code","outputId":"0a4eb154-76d3-45bf-cfed-42f2961391ac","executionInfo":{"status":"ok","timestamp":1591692076953,"user_tz":-330,"elapsed":16032,"user":{"displayName":"Rudraksh Kapil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoFYYIzudjawgt9sydxKemiYUjrtPu0UKhIh9F=s64","userId":"06594730916062239985"}},"colab":{"base_uri":"https://localhost:8080/","height":358,"referenced_widgets":["13f9c49584c14c969dc780463ccb1a33","7d967c5e657740cc8d5df30f8a181ff2","7647ece2099f4f4db4ffeb16d14a020d","3517cca42ed348bbbd17185ecd246bb7","3b6197063cbb477390f08f0d816f7058","b86bd45939ab43f4838208a4da41a99b","1b7d17d1e6094b17b7f3d7949492ebd9","525ce038b2134c809e46994369a83424"]}},"source":["params = {\n","    'net_type': 'SM', # for saving\n","    'type' : 'modplast',\n","    'lr': 1e-4,\n","    'eplen': 120,\n","    'hs': 200,\n","    'l2':0,\n","    'pe':10000,\n","    'bv':0.1,\n","    'blossv':0.1,\n","    'bent':0.1,\n","    'rew':1,\n","    'wp':0,\n","    'save_every':1000,\n","    'da':'tanh',\n","    'clamp':0,\n","    'nbiter':60000,\n","    'fm':1,\n","    'ni':4,\n","    'pf':0.0,\n","    'alg':'A3C',\n","    'cs':20,\n","    'eps':1e-6,\n","    'is':0,\n","    'bs':30,\n","    'gr':0.9,\n","    'gc':2.0,\n","    'rngseed':0,\n","    'saved_already':True\n","}\n","\n","\n","save = train(params)\n","savepath='saved/Final'+params['net_type']+'.pt'\n","torch.save(save,savepath)\n","print(\"Saved!\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Starting training...\n","Passed params:  {'net_type': 'SM', 'type': 'modplast', 'lr': 0.0001, 'eplen': 120, 'hs': 200, 'l2': 0, 'pe': 10000, 'bv': 0.1, 'blossv': 0.1, 'bent': 0.1, 'rew': 1, 'wp': 0, 'save_every': 1000, 'da': 'tanh', 'clamp': 0, 'nbiter': 60000, 'fm': 1, 'ni': 4, 'pf': 0.0, 'alg': 'A3C', 'cs': 20, 'eps': 1e-06, 'is': 0, 'bs': 30, 'gr': 0.9, 'gc': 2.0, 'rngseed': 0, 'saved_already': True}\n","uname_result(system='Linux', node='84eacb585e09', release='4.19.104+', version='#1 SMP Wed Feb 19 05:26:34 PST 2020', machine='x86_64', processor='x86_64')\n","SRB_alg_A3C_bent_0.1_blossv_0.1_bs_30_bv_0.1_clamp_0_cs_20_da_tanh_eplen_120_eps_1e-06_fm_1_gc_2.0_gr_0.9_hs_200_is_0_l2_0_lr_0.0001_nbiter_60000_net_type_SM_ni_4_pf_0.0_rew_1_saved_already_True_type_modplast_wp_0_rngseed_0\n","Setting random seeds\n","Initializing network\n","Shape of all optimized parameters: [torch.Size([200, 200]), torch.Size([200, 200]), torch.Size([1]), torch.Size([200, 27]), torch.Size([200]), torch.Size([1, 200]), torch.Size([1]), torch.Size([2, 200]), torch.Size([2]), torch.Size([1, 200]), torch.Size([1])]\n","Size (numel) of all optimized elements: [40000, 40000, 1, 5400, 200, 200, 1, 400, 2, 200, 1]\n","Total size (numel) of all optimized elements: 86405\n","Initializing optimizer\n","Loading states 1\n","<All keys matched successfully>\n","Loading states 2\n","None\n","Loaded from 'saved/CheckpointSM.pt' at episode 60000\n","Starting episodes!\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13f9c49584c14c969dc780463ccb1a33","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SfyHw04nb1M-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}